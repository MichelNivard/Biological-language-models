---
title: "Weaving together models"
format: html
---

::: callout-tip
## Abstract

In Chapter 5, we will put these two models — Vanilla BERT and GPN-MSA-BERT — to the test. We will evaluate their performance on:

Predicting masked bases (MLM accuracy). Identifying regulatory elements. Predicting the functional impact of mutations.

This head-to-head comparison will highlight the strengths and weaknesses of each approach and show the value of embedding evolutionary context directly into genomic language models. Finally we'll build a hybrid model, in which the DN sequence is moeld bih in evolutionary context and in its sequence context and the results are blended, a model architecture that is a little like alphafold, the Nobel winning protein language model devleoped at googl deepmind.

All scripts for this chapter are found here: <https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/Chapter_4>
:::

Preview of Chapter 5