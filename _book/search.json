[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Biological language models",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Chapter1.html",
    "href": "Chapter1.html",
    "title": "1  Collecting and Preparing DNA Sequences for a Language Model",
    "section": "",
    "text": "1.1 Introduction\nHigh-quality data is essential for training effective machine learning models. In natural language processing (NLP), researchers invest significant effort in curating clean datasets from raw internet text. For example a dataset like ‘fineweb-edu’ contains english text that is of very high quality. models trained on fineweb-edu (and similar high quality datasets) will improve MUCH faster.\nThose with experience with genetics will know most of what I am about to explain when it comes to the sources of genetic data and the file formats these come in. Those with an ML background will be very familiar with Huggingface and the amazing data/model/training integration their libraries offer. Assembing a useful genomics fatal for language modeling requires familiarity with both.\nWhen working with DNA, RNA, and protein sequences, the authoritative sources of data are specialized databases and infrastructures rather than data scraped from the internet. If you want to learn to train DNA/RNA/Protein-based language model, we must learn how, and where, to retrieve data and convert it into a structured format.\nIn this tutorial, we will begin by collecting coding DNA sequences (CDS) from the human genome using the Ensembl database via BioMart. The goal is to store these sequences in a format suitable for training a DNA-based language model. We will then upload the processed data to Hugging Face, an efficient platform for hosting datasets for machine learning tasks.\nThis chapter will introduce key biological and computational concepts, ensuring that both biology newcomers and those unfamiliar with language modeling tools can follow along.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Collecting and Preparing DNA Sequences for a Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#introduction",
    "href": "Chapter1.html#introduction",
    "title": "1  Collecting and Preparing DNA Sequences for a Language Model",
    "section": "",
    "text": "Relative training efficiency using a high quality dataset like fineweb-edu and other massive datasets of English language texts. Image obtained from: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Collecting and Preparing DNA Sequences for a Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#understanding-ensembl-and-biomart",
    "href": "Chapter1.html#understanding-ensembl-and-biomart",
    "title": "1  Collecting and Preparing DNA Sequences for a Language Model",
    "section": "1.2 Understanding Ensembl and BioMart",
    "text": "1.2 Understanding Ensembl and BioMart\nEnsembl is a genomic database that provides extensive annotations for various species, including humans. It offers access to gene sequences, transcript annotations, and protein-coding information. One of its most powerful tools is BioMart, a flexible data retrieval system that allows users to download specific genomic datasets easily.\nBefore working with the data, we need to store it in a format that is accessible and efficient for machine learning. Unlike text-based NLP datasets, genomic data is structured differently and must be properly formatted before use in a model.\n\n1.2.1 What Are FASTA Files?\nA FASTA file is a simple text-based format used for storing biological sequences. Each entry in a FASTA file consists of: 1. A header line (starting with &gt;), which contains metadata such as gene IDs and chromosome locations. 2. A sequence line, which contains the nucleotide or protein sequence.\nThere is a very comprehensive Wikipedia entry on the FASTA format.\n“Sequences may be protein sequences or nucleic acid sequences, and they can contain gaps or alignment characters (see sequence alignment). Sequences are expected to be represented in the standard IUB/IUPAC amino acid and nucleic acid codes, with these exceptions: lower-case letters are accepted and are mapped into upper-case; a single hyphen or dash can be used to represent a gap character; and in amino acid sequences, U and * are acceptable letters (see below). Numerical digits are not allowed but are used in some databases to indicate the position in the sequence.” ((source: https://en.wikipedia.org/wiki/FASTA_format))\n\n\n\nNucleic Acid Code\nMeaning\nMnemonic\n\n\n\n\nA\nA\nAdenine\n\n\nC\nC\nCytosine\n\n\nG\nG\nGuanine\n\n\nT\nT\nThymine\n\n\nU\nU\nUracil\n\n\n(i)\ni\ninosine (non-standard)\n\n\nR\nA or G (I)\npuRine\n\n\nY\nC, T or U\npYrimidines\n\n\nK\nG, T or U\nbases which are Ketones\n\n\nM\nA or C\nbases with aMino groups\n\n\nS\nC or G\nStrong interaction\n\n\nW\nA, T or U\nWeak interaction\n\n\nB\nnot A (i.e. C, G, T or U)\nB comes after A\n\n\nD\nnot C (i.e. A, G, T or U)\nD comes after C\n\n\nH\nnot G (i.e., A, C, T or U)\nH comes after G\n\n\nV\nneither T nor U (i.e. A, C or G)\nV comes after U\n\n\nN\nA C G T U\nNucleic acid\n\n\n-\ngap of indeterminate length\n\n\n\n\nThe amino acid codes supported (22 amino acids and 3 special codes) are:\n\n\n\nAmino Acid Code\nMeaning\n\n\n\n\nA\nAlanine\n\n\nB\nAspartic acid (D) or Asparagine (N)\n\n\nC\nCysteine\n\n\nD\nAspartic acid\n\n\nE\nGlutamic acid\n\n\nF\nPhenylalanine\n\n\nG\nGlycine\n\n\nH\nHistidine\n\n\nI\nIsoleucine\n\n\nJ\nLeucine (L) or Isoleucine (I)\n\n\nK\nLysine\n\n\nL\nLeucine\n\n\nM\nMethionine/Start codon\n\n\nN\nAsparagine\n\n\nO\nPyrrolysine (rare)\n\n\nP\nProline\n\n\nQ\nGlutamine\n\n\nR\nArginine\n\n\nS\nSerine\n\n\nT\nThreonine\n\n\nU\nSelenocysteine (rare)\n\n\nV\nValine\n\n\nW\nTryptophan\n\n\nY\nTyrosine\n\n\nZ\nGlutamic acid (E) or Glutamine (Q)\n\n\nX\nany\n\n\n*\ntranslation stop\n\n\n-\ngap of indeterminate length\n\n\n\nYou’ll notice the FASTA format has a well defined structure, and it could be leveraged to build a complete tokenizer, for now though our 4 character (+6 special characters) tokenizer will have to do.\n\n\n1.2.2 Why Focus on Coding DNA Sequences (CDS)?\nIn our case, we will retrieve the human coding DNA sequences (CDS), which represent the DNA sequence of protein-coding regions of genes.\nWhile our ultimate goal is to model the entire human genome—and potentially multiple genomes across species or individuals—such tasks require significant computational resources. The tutorials in this book are designed to be accessible, running efficiently on a MacBook or Google Colab. Therefore, we focus on CDS, which are highly structured DNA sequences within genes, that directly transcribed into RNA which is in turn translate into proteins. the Table below contains the direct translation from 3 letter DNA sequence to amino-acid (which are the building blocks of proteins).\n\n\n\nThe Genetic code to translate codins (3 leter DNA sequences) to amino-acids that are in turn the building blocks of proteins (source: https://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/_images/P7_image1.png)\n\n\nIn contrast, much of the rest of the genome consists of regulatory regions, which are more complex and less structured. CDS sequences provide a strong foundation for a small DNA-based language model because they contain well-defined biological structure, making them a logical starting point before expanding to more complex genomic elements.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Collecting and Preparing DNA Sequences for a Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#why-upload-dna-data-to-hugging-face",
    "href": "Chapter1.html#why-upload-dna-data-to-hugging-face",
    "title": "1  Collecting and Preparing DNA Sequences for a Language Model",
    "section": "1.3 Why Upload DNA Data to Hugging Face?",
    "text": "1.3 Why Upload DNA Data to Hugging Face?\nHugging Face provides a robust ecosystem for hosting and sharing datasets, particularly for machine learning applications. Some key advantages include: - Easy accessibility: Researchers and models can easily retrieve datasets. - Standardized format: Datasets are structured for seamless integration with deep learning frameworks. - Direct integration with Hugging Face tools: The data on the Hugging Face Hub integrates seamlessly with their Transformers and Trainer Python libraries, making it easy to load datasets and train models. - Version control and updates: Data can be refined and expanded over time.\nBy storing our dataset on Hugging Face, we enable efficient training and collaboration for DNA language modeling.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Collecting and Preparing DNA Sequences for a Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#the-script-downloading-and-formatting-human-cds-data",
    "href": "Chapter1.html#the-script-downloading-and-formatting-human-cds-data",
    "title": "1  Collecting and Preparing DNA Sequences for a Language Model",
    "section": "1.4 The Script: Downloading and Formatting Human CDS Data",
    "text": "1.4 The Script: Downloading and Formatting Human CDS Data\nBelow is the R script that downloads human CDS from Ensembl using BioMart, extracts metadata, and saves the processed data into a CSV file. the package we use, biomartr isnt the official R package but its a great option! it has very extensive documentation, so if you want to download other sequences in the future make sure to start here: https://docs.ropensci.org/biomartr/\n# Install necessary packages\ninstall.packages(\"biomartr\", dependencies = TRUE)\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n  install.packages(\"BiocManager\")\n}\n\n# Load required libraries\nlibrary(Biostrings)\nlibrary(biomartr)\n\n# Download the human CDS dataset from Ensembl\nHS.cds.ensembl &lt;- getCDS(db = \"ensembl\", organism = \"Homo sapiens\", path = file.path(\"_ncbi_downloads\",\"CDS\"))\n\n# Read CDS data as a Biostrings object\nHuman_CDS &lt;- read_cds(file = HS.cds.ensembl, obj.type = \"Biostrings\")\n\n# Extract headers and sequences\nheaders &lt;- names(Human_CDS)\nsequences &lt;- as.character(Human_CDS)\n\n# Function to extract metadata from headers\nextract_metadata &lt;- function(header) {\n  transcript_id &lt;- sub(\"^&gt;([^ ]+).*\", \"\\\\1\", header)\n  chromosome &lt;- sub(\".*chromosome:([^ ]+).*\", \"\\\\1\", header)\n  start &lt;- sub(\".*chromosome:[^:]+:([^:]+).*\", \"\\\\1\", header)\n  end &lt;- sub(\".*chromosome:[^:]+:[^:]+:([^:]+).*\", \"\\\\1\", header)\n  strand &lt;- sub(\".*chromosome:[^:]+:[^:]+:[^:]+:([^ ]+).*\", \"\\\\1\", header)\n  gene_id &lt;- sub(\".*gene:([^ ]+).*\", \"\\\\1\", header)\n  gene_biotype &lt;- sub(\".*gene_biotype:([^ ]+).*\", \"\\\\1\", header)\n  transcript_biotype &lt;- sub(\".*transcript_biotype:([^ ]+).*\", \"\\\\1\", header)\n  gene_symbol &lt;- sub(\".*gene_symbol:([^ ]+).*\", \"\\\\1\", header)\n  description &lt;- sub(\".*description:(.*)\", \"\\\\1\", header)\n\n  list(\n    transcript_id = transcript_id,\n    chromosome = chromosome,\n    start = start,\n    end = end,\n    strand = strand,\n    gene_id = gene_id,\n    gene_biotype = gene_biotype,\n    transcript_biotype = transcript_biotype,\n    gene_symbol = gene_symbol,\n    description = description\n  )\n}\n\n# Extract metadata from headers\nmetadata_list &lt;- lapply(headers, extract_metadata)\nmetadata_df &lt;- do.call(rbind, lapply(metadata_list, as.data.frame))\nmetadata_df$sequence &lt;- sequences\n\n# Save data to CSV\nwrite.csv(metadata_df, \"genome_sequences.csv\", row.names = FALSE, quote = TRUE)\n\n# Print sample data\nhead(metadata_df)\nYou can run the script yourself, but I have also gone ahead and uploaded it to huggingface: https://huggingface.co/datasets/MichelNivard/Human-genome-CDS-GRCh38",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Collecting and Preparing DNA Sequences for a Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#summary",
    "href": "Chapter1.html#summary",
    "title": "1  Collecting and Preparing DNA Sequences for a Language Model",
    "section": "1.5 Summary",
    "text": "1.5 Summary\nIn this chapter, we: - Introduced Ensembl and BioMart as tools for retrieving genomic data. - Explained FASTA files and human CDS, which form the core of our dataset. - Discussed the advantages of uploading datasets to Hugging Face, emphasizing its integration with Transformers and Trainer libraries. - Provided an R script to download, process, and store human CDS in a structured format.\nIn the next chapter, we will explore preprocessing techniques and strategies for encoding DNA sequences into a format suitable for training a deep learning language model, and use Huggingface to train our first little DNA language model!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Collecting and Preparing DNA Sequences for a Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2.html",
    "href": "Chapter2.html",
    "title": "2  Training our first DNA Language Model: BERT",
    "section": "",
    "text": "2.1 Introduction\nNow that we have collected and formatted a DNA dataset in Chapter 1, we can proceed to train a language model using BERT. In this chapter, we will walk through the process of tokenizing DNA sequences, configuring a BERT model, and training it using the Masked Language Modeling (MLM) objective.\nWe will cover the key concepts behind tokenization, BERT, and MLM before diving into the Python script.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model: BERT</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#introduction",
    "href": "Chapter2.html#introduction",
    "title": "2  Chapter 2: Training a DNA Language Model with BERT",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nNow that we have collected and formatted a DNA dataset in Chapter 1, we can proceed to train a language model using BERT. In this chapter, we will walk through the process of tokenizing DNA sequences, configuring a BERT model, and training it using the Masked Language Modeling (MLM) objective.\nWe will cover the key concepts behind tokenization, BERT, and MLM before diving into the Python script.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Training a DNA Language Model with BERT</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#understanding-tokenization",
    "href": "Chapter2.html#understanding-tokenization",
    "title": "2  Training our first DNA Language Model: BERT",
    "section": "2.2 Understanding Tokenization",
    "text": "2.2 Understanding Tokenization\n\n2.2.1 What is a Tokenizer?\nA tokenizer is a fundamental component of any language model. Language models are essentially large multinomial models that predict the next token in a sequence based on previous tokens, or predict a masked token using the surrounding tokens. Since machine learning models operate on numerical representations, tokenized text must be converted into integers, which serve as indices for lookup in a vocabulary.\nThese integers, however, have no inherent numeric value—they simply act as categorical labels that the model learns to associate with semantic meaning. For example, if we tokenize the sentence:\n\n“The quick brown fox jumps over the lazy dog”\n\nat the word level, we might obtain a numerical sequence like:\n\n[4, 123, 678, 89, 245, 983, 56, 4564]\n\nwhere each number corresponds to a word based on a pre-defined tokenization dictionary, such as:\n{\"the\": 4, \"quick\": 123, \"brown\": 678, \"fox\": 89, \"jumps\": 245, \"over\": 983, \"lazy\": 56, \"dog\": 4564}\nSimilarly, for DNA sequences, each nucleotide (A, T, C, G) is assigned a unique integer ID.\n\n\n2.2.2 Our DNA Tokenizer\nOur tokenizer uses a character-level approach, where each nucleotide is assigned a unique integer ID. Special tokens are also included for various purposes:\n\n[UNK] (unknown token)\n[PAD] (padding token for equal-length sequences)\n[CLS] (classification token, useful for downstream tasks)\n[SEP] (separator token, used in tasks like sequence-pair classification)\n[MASK] (used for masked language modeling training)\n\nPython Code:\nimport torch\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.pre_tokenizers import Split\nfrom transformers import PreTrainedTokenizerFast\n\n# --------------------------------\n# 1. DNA Tokenizer with Full FASTA Nucleic Acid Code\n# --------------------------------\n\n# Define vocabulary to include all FASTA nucleotides and symbols\ndna_vocab = {\n    \"A\": 0, \"T\": 1, \"C\": 2, \"G\": 3, \"N\": 4, \"U\": 5, \"i\": 6,  # Standard bases + Inosine\n    \"R\": 7, \"Y\": 8, \"K\": 9, \"M\": 10, \"S\": 11, \"W\": 12,  # Ambiguous bases\n    \"B\": 13, \"D\": 14, \"H\": 15, \"V\": 16,  # More ambiguity codes\n    \"-\": 17,  # Gap character\n    \"[UNK]\": 18, \"[PAD]\": 19, \"[CLS]\": 20, \"[SEP]\": 21, \"[MASK]\": 22\n}\n\n# Create tokenizer\ntokenizer = Tokenizer(WordLevel(vocab=dna_vocab, unk_token=\"[UNK]\"))\ntokenizer.pre_tokenizer = Split(\"\", \"isolated\")  # Character-level splitting\n\n# Convert to Hugging Face-compatible tokenizer\nhf_tokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\"\n)\n\n\n2.2.3 Other Tokenization Strategies for DNA, RNA, and Proteins\nWhile character-level tokenization is effective, other tokenization approaches can offer different tradeoffs:\n\n2.2.3.1 Byte Pair Encoding (BPE)\nBPE is widely used in NLP and allows frequently occurring patterns to be merged into subword units. While BPE could be useful for repetitive genome sequences, it may not capture biologically meaningful units.\n\n\n2.2.3.2 K-mer Tokenization\nK-mer tokenization groups nucleotides into fixed-length substrings (e.g., 3-mers like “ATG”). This approach retains local sequence structure but can lead to a large vocabulary size.\n\n\n2.2.3.3 Tiktoken and Similar Models\nSome modern tokenization methods, such as Tiktoken, optimize speed and efficiency by precomputing merges. These are often optimized for large-scale NLP tasks but could be adapted for biological sequences.\nChoosing the best tokenizer depends on the specific modeling task. For example, RNA secondary structures or protein folding models might benefit from more complex tokenization strategies.\nSource: RPubs Tokenization Review",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model: BERT</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#loading-and-tokenizing-the-dna-dataset",
    "href": "Chapter2.html#loading-and-tokenizing-the-dna-dataset",
    "title": "2  Training our first DNA Language Model: BERT",
    "section": "2.3 Loading and Tokenizing the DNA Dataset",
    "text": "2.3 Loading and Tokenizing the DNA Dataset\n\n2.3.1 Understanding the Dataset\nWe will use a pre-existing dataset, Human-genome-CDS-GRCh38, which contains coding sequences from the human genome.\n\n\n2.3.2 Tokenizing the Dataset\nTo prepare the dataset for training, we must apply the tokenizer to each sequence while ensuring:\n\nSequences are truncated or padded to a fixed length (512 tokens)\nUnwanted columns are removed\n\nPython Code:\nfrom datasets import load_dataset\n\ndataset_name = \"MichelNivard/Human-genome-CDS-GRCh38\"\ndataset = load_dataset(dataset_name)\n\ncolumn_name = \"sequence\"\n\ndef tokenize_function(examples):\n    return hf_tokenizer(examples[column_name], truncation=True, padding=\"max_length\", max_length=512)\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[column_name])",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model: BERT</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#saving-and-preparing-the-dataset-for-training",
    "href": "Chapter2.html#saving-and-preparing-the-dataset-for-training",
    "title": "2  Training our first DNA Language Model: BERT",
    "section": "2.4 Saving and Preparing the Dataset for Training",
    "text": "2.4 Saving and Preparing the Dataset for Training\nOnce tokenized, we save the dataset for efficient access during training.\nPython Code:\ntokenized_dataset.save_to_disk(\"tokenized_dna_dataset\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model: BERT</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#understanding-bert-and-masked-language-modeling-mlm",
    "href": "Chapter2.html#understanding-bert-and-masked-language-modeling-mlm",
    "title": "2  Training our first DNA Language Model: BERT",
    "section": "2.5 Understanding BERT and Masked Language Modeling (MLM)",
    "text": "2.5 Understanding BERT and Masked Language Modeling (MLM)\n\n2.5.1 What is BERT?\nBERT (Bidirectional Encoder Representations from Transformers) is a powerful transformer-based language model. Unlike traditional left-to-right models, BERT learns bidirectional context, allowing it to understand sequences more effectively.\nReturning to our earlier example sentence:\n\n“The quick brown fox jumps over the lazy dog”\n\nBERT does not process words one at a time but instead considers the entire sequence simultaneously. This bidirectional approach allows BERT to infer missing words based on context from both directions.\n\n\n2.5.2 What is Masked Language Modeling (MLM)?\nMLM is a self-supervised learning objective where the model learns by predicting missing tokens in a sequence. During training:\n\nSome tokens are randomly replaced with [MASK]\nThe model must predict the original token based on surrounding context\n\nFor example, if we mask the word “fox” in our sentence:\n\n“The quick brown [MASK] jumps over the lazy dog”\n\nBERT will analyze the remaining words and attempt to predict “fox.”\nThis technique enables BERT to learn useful representations without requiring labeled data.\n\n\n2.5.3 Understanding Transformer Layers, Attention Heads, and Hidden Size\nA transformer layer consists of self-attention and feed-forward layers that help the model learn relationships between tokens. The number of transformer layers determines how deep the model is.\nAn attention head is a component of the self-attention mechanism that learns different types of relationships within the data. Having multiple attention heads allows the model to capture various dependencies between tokens.\nReturning to our example:\n\nOne attention head might focus on subject-verb relationships, recognizing that “fox” is the subject of “jumps.”\nAnother head might capture adjective-noun relationships, linking “brown” to “fox.”\n\nThe hidden size defines the dimensionality of the model’s internal representations. A larger hidden size enables the model to capture more complex patterns, but also increases computational cost.\nBy stacking multiple transformer layers and attention heads, BERT gains a deep understanding of sentence structures.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model: BERT</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#defining-the-bert-model-for-dna-sequences",
    "href": "Chapter2.html#defining-the-bert-model-for-dna-sequences",
    "title": "2  Training our first DNA Language Model: BERT",
    "section": "2.6 Defining the BERT Model for DNA Sequences",
    "text": "2.6 Defining the BERT Model for DNA Sequences\nWhile the “quick brown fox” example helps us understand how BERT processes natural language, our goal is to apply the same principles to DNA sequences. Instead of predicting missing words in a sentence, we want our model to learn biological patterns and genomic structure by predicting masked nucleotides within DNA sequences.\nIn DNA modeling, understanding sequence context is just as critical as in language modeling. Just as BERT learns that “fox” fits within a given sentence structure, our model should learn that specific nucleotide sequences appear in biologically meaningful patterns. This could involve recognizing gene coding regions, regulatory motifs, or conserved sequence elements across different genomes.\nTo accomplish this, we define a custom BERT model designed specifically for processing DNA sequences. Unlike traditional text-based models, our DNA BERT model uses a character-level vocabulary of nucleotides (A, T, C, G) and special tokens to represent sequence structure. By leveraging masked language modeling (MLM), the model will learn to predict missing nucleotides based on surrounding context, allowing it to capture meaningful genomic features.\nWith this in mind, let’s move forward and define our BERT architecture for DNA sequences.\nPython Code:\nfrom transformers import ModernBertConfig, ModernBertForMaskedLM\n\nconfig = ModernBertConfig(\n    vocab_size=len(dna_vocab),\n    hidden_size=256,\n    num_hidden_layers=4,\n    num_attention_heads=8,\n    intermediate_size=512,\n    max_position_embeddings=512,\n    type_vocab_size=1,\n)\nconfig.pad_token_id = dna_vocab[\"[PAD]\"]\nmodel = ModernBertForMaskedLM(config)\n\n2.6.1 Configuring Training for DNA BERT\nNow that we have defined our BERT model for DNA sequences, we need to set up the training process. This involves specifying various training hyperparameters, handling masked language modeling (MLM) data, and preparing for efficient learning.\nUnlike traditional NLP tasks where models are trained on massive text corpora, training on DNA sequences presents unique challenges. The structure of genomic data, sequence length, and biological patterns all influence how the model learns. Our configuration ensures that the training process is efficient while maintaining biological relevance.\n\n\n\n2.6.2 Setting Training Parameters\nTo train our DNA BERT model, we use the Hugging Face TrainingArguments class, which allows us to define key training settings. These include:\n\nBatch size: We set a batch size of 16 for both training and evaluation. This determines how many sequences are processed at once.\nLogging & Saving: We log loss every 50 steps and save model checkpoints every 100 steps to monitor training progress.\nLearning Rate: We use a learning rate of 5e-5, a common choice for transformer models that balances learning speed and stability.\nWeight Decay: A value of 0.01 is used to prevent overfitting by applying L2 regularization to model weights.\nTraining Steps: The model is trained for 4000 steps. This ensures sufficient learning without excessive computation.\nModel Saving: The model checkpoints are stored in ./bert-dna, allowing us to resume training if needed.\n\nPython Code:\ntraining_args = TrainingArguments(\n    output_dir=\"./bert-dna\",\n    overwrite_output_dir=True,\n    logging_steps=50,  # Log loss every step\n    save_steps=100,\n    save_total_limit=2,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    max_steps=4000,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    push_to_hub=False,\n    report_to=\"none\",  # Disables wandb logging\n)\n\n\n\n2.6.3 Preparing for Masked Language Modeling (MLM)\nSince we are training our DNA BERT model using masked language modeling (MLM), we need to handle masked tokens properly. This is done using the DataCollatorForLanguageModeling, which:\n\nRandomly masks nucleotides in the training sequences.\nCreates labels automatically, meaning the model learns by trying to predict these masked tokens.\nUses a masking probability of 5%, ensuring that a small but meaningful portion of the sequence is masked during training.\n\nBy applying MLM, we allow the model to generalize nucleotide relationships and capture sequence dependencies, just like how BERT learns relationships between words in text.\nPython Code:\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=hf_tokenizer,\n    mlm=True,\n    mlm_probability=0.05\n)\n\n\n\n2.6.4 Training the DNA BERT Model\nWith our configuration and data collator in place, we now train the model. We use the Hugging Face Trainer API, which simplifies the training process by handling:\n\nDataset iteration: Automatically loads and batches training sequences.\nGradient updates: Adjusts model weights based on training loss.\nLogging & saving: Tracks training progress and stores checkpoints.\n\nOnce training begins, the model will gradually learn nucleotide dependencies and improve its ability to predict missing DNA bases.\nPython Code:\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=hf_tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()\nyou set up free wandb logging (go to https://wandb.ai/site for more info) you can track your training runs online, wherever they are running. You then get a dashboard full of pretty loss vs progress plots like the one below which I screencapped about ± 30 minutes into training on my macbook.\n\n\n\n2.6.5 Saving the Trained Model\nAfter training completes, we save both the model and tokenizer so they can be used for future predictions or fine-tuning.\n\nThe model weights are stored in ./bert-dna, allowing us to reload the trained model.\nThe tokenizer is also saved, ensuring that input sequences can be processed the same way during inference.\n\nFinally, a success message is printed, confirming that the training process has been completed.\nPython Code:\n# Save the final model and tokenizer\ntrainer.save_model(\"./bert-dna\")\nhf_tokenizer.save_pretrained(\"./bert-dna\")\n\nprint(\"🎉 Training complete! Model saved to ./bert-dna\")\n\n\n2.6.6 Summary\nIn this section, we:\n\nDefined training hyperparameters such as batch size, learning rate, and training steps.\nUsed masked language modeling (MLM) to train the model on DNA sequences.\nLeveraged the Hugging Face Trainer API to automate model training.\nSaved the final trained model and tokenizer for future use.\n\nWith this trained model, we can now fine-tune or apply it to various genomic tasks, such as predicting genetic variations or classifying functional DNA sequences. In the next chapter, we will explore how to fine-tune our DNA BERT model for specific applications.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model: BERT</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#section",
    "href": "Chapter2.html#section",
    "title": "2  Chapter 2: Training a DNA Language Model with BERT",
    "section": "2.7 ",
    "text": "2.7",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Training a DNA Language Model with BERT</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]