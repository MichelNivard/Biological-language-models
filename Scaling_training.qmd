# Scale up Training {.unnumbered}

This chapter isnt a part of the "DNA" section of the book, because the lessons are really quite general, but it coems afer because we needed a little bit of experience with language model training before even considering training a serious model. This is also a somewhat akward chapter for me to write, especially for the part of the readership that has a background in ML. See I am a psychologist by training (though I have worked in genetic epidemiology for years and years), and while a lot of my academic work is fairly computational, I am no expext in language model scaling by any means! Remember, the pre-ambble to the book explains this book is an account of me learning about Biological language models and taking others along for the ride, not an authoritative text!

## Don't try to win the compute race

Among the DNA models I could find on huggingface are 7B parameter model like <https://huggingface.co/genbio-ai/AIDO.DNA-7B>. AIDO i trained on "256 H100 GPUs" in "8 days". The training data consisted of 10.6 Billion bases. Thats not even a particularly large model in the grant scheme of things, but if you consider a cost of ±\$2 per hour per H100 you are going to spend \$100k. Obviously there are academic compute resources you can get access to by appointment or based on fair use at your institute, university or trough collaborative national infrastructure but even those are finite.

TOday (MArch 2025) the Dutch national computer cluster for research (snellius at surfsara) has 88 nodes with 4 H100 GPUs and 72 nodes with 4 A100 GPUs. TAC the University of Texas at Austin compute provider has ±80 A100 nodes (each with 3 GPUs) THose are robust HPC providers for academic work. In my experience you could get time reserved for your research at steep discounts, and these systems are likely large enough to train that 7B model I linked to. Peraps even at larger national research clusters, like [Isambard-ai](https://www.bristol.ac.uk/news/2023/november/supercomputer-announcement.html) (being build in Bristol right now, a motivation for me to write this) which ha 5000 H200 GPUs. In general it is likely you are going to be compute constrained. Don't be discouraged though, most breakthroughs are not going to be compute based and there are immense efficiency gains to be made that will level the playing field.

## Smart architectures

In Chapter 4 we studied smarter, DNA specific model architectures. The GPN model inspired by @Benegas2024 we intrudiced can blow away a standard Bert in a hour of training on my 2022 macbook air (the Bert we trained and compared to our GPN bert trained for ±8 hous on a strong GPU). THe massive efficiency gain may mean you can beat the 7B bert like model we took as an example of compute costs with a fraction of the compute! Other reseachers have designed alternatives for the transformer module at the core of these models with DNA in mind, and expanded its context window up to 1 million bases with far less compute [@nguyen2023]

## Optimize optimize optimize

## Paralllel training