# Scale up Training {.unnumbered}

This chapter isn't a part of the "DNA" section of the book, because the lessons are really quite general, but it comes after because we needed a little bit of experience with language model training before even considering training a serious model. This is also a somewhat awkward chapter for me to write, especially for the part of the readership that has a background in ML. See, I am a psychologist by training (though I have worked in genetic epidemiology for years and years), and while a lot of my academic work is fairly computational, I am no expert in language model scaling by any means! Remember, the preamble to the book explains this book is an account of me learning about biological language models and taking others along for the ride, not an authoritative text!

## Don't Try to Win the Compute Race

Among the DNA models I could find on Hugging Face is a 7B parameter model like <https://huggingface.co/genbio-ai/AIDO.DNA-7B>. AIDO is trained on "256 H100 GPUs" in "8 days". The training data consisted of 10.6 billion bases. That's not even a particularly large model in the grand scheme of things, but if you consider a cost of ±$2 per hour per H100, you are going to spend $100k. Obviously, there are academic compute resources you can get access to by appointment or based on fair use at your institute, university, or through collaborative national infrastructure, but even those are finite.

You have to consider feasibility. Today (March 2025), the Dutch national computer cluster for research (Snellius at SURF Sara) has 88 nodes with 4 H100 GPUs and 72 nodes with 4 A100 GPUs. TACC, the University of Texas at Austin compute provider, has ±80 A100 nodes (each with 3 GPUs). Those are two examples of reasonably well-funded HPC providers in academia. In my experience, you could get time reserved for your research at your local academic HPC provider at steep discounts, and these systems are likely large enough to train that 7B model I linked to. However, note how on either TACC or Snellius, 256 GPUs for 8 days would block the entire system for over a week. Perhaps you could apply for access to larger national research clusters, like [Isambard-AI](https://www.bristol.ac.uk/news/2023/november/supercomputer-announcement.html) in the UK (being built in Bristol right now, a motivation for me to write this) which has 5,000 H200 GPUs. However, in general, it is likely you are going to be relatively compute constrained. Don't be discouraged though—most breakthroughs are not going to be compute-based, and there are immense efficiency gains to be made that will level the playing field.

## Smart Architectures

In Chapter 4, we studied smarter, DNA-specific model architectures. The GPN model inspired by @Benegas2024 we introduced can blow away a standard BERT in an hour of training on my 2022 MacBook Air (the BERT we trained and compared to our GPN-BERT trained for ±8 hours on a strong GPU). The massive efficiency gain may mean you can beat the 7B BERT-like model we took as an example of compute costs with a fraction of the compute! As briefly remarked on in Chapter 6 reseachers have designed alternatives for the transformer module in order to expand its context window up to 1 million bases, with far less compute reuirement than the transformer [@nguyen2023]. If you are to design and run you own model, it may pay off to consider implementing some of these optimisations.

## Focus on a specific question

Ist is tempting to train a model on all DNA known to man, but honestly, there is actully more of that then peopel an even beginn to train on. The models disucssed so far ofeten train on the reference seqeucnes, a sort of modal genome, but indiviual people's genomes are different from that references. YOu culd consider thousands of species, or (tens/hundreds of) thousands of individual genomes. That would require a lot of bioinformatcs. You'd have to phase the genomes to untangle the maternal and paternal strnd, youd hae to decided whether you want to get rid of the reference entirely and build a specific reference/genome for eah individual, you migh require some reference, or a graph genome? It's also worth considering whether your task really requires the whole genome. Are you performing gene centric tasks (mutation consequence prediction, gene expresssion prediction, alternative splice modeling)? If your speciic tasks dont requre the whole genome, why not consider traning on coding sequences only or genes and a few thousand bases around them? 

## Optimize, Optimize, Optimize

## Parallel Training