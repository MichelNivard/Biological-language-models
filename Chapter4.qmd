---
title: "Chapter 4 - Evolution-Aware BERT: Designing Encoders for Genomics"
---

::: callout-tip
## Abstract

I this chapter we'll evaluate the architecture of natural language models, which we have up to this point uncritically adopted for DNA modeling from our NLP/corporate brethern. We'll discuss how some reseachers have bugun to move on from applying known model architectures to DNA and started to (re)designing model architectures specifically with DNA in mind. These models lean into our very extensive knowledge of the evolutionary history of the Genome.

All scripts for this chapter are found here: <https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/Chapter_4>
:::


## Introduction

In previous chapters, we introduced the basic principles of **BERT for DNA sequences**. We took inspiration from natural language processing (NLP), treating DNA as a **language**, where sequences of nucleotides (A, T, C, G, -) could be processed using transformers. This approach, while powerful, carries over several assumptions from natural language that do not perfectly align with biological sequences. In this chapter, we will re-examine how we encode genomic data and introduce **a new design paradigm — evolutionary-aware encoding — inspired by the recently proposed GPN (Genomic Pre-trained Network).**

---

## 1. Tokenization and Embedding in Language Models

Modern language models, whether **BERT**, **GPT**, or similar architectures, rely heavily on **how input sequences are tokenized and encoded before they ever reach the attention layers**. This initial step — often overlooked — plays a profound role in shaping how the model learns.

### Tokenization in Natural Language

In human languages like English or French, the vocabulary is **large**, often comprising tens of thousands of tokens. These tokens could be:

- Whole words ("cat", "sat").
- Subwords ("cat" might break into "c", "at").
- Even characters (in rare cases).

Since the number of tokens is so large, **each token is assigned a unique vector embedding — a dense, learnable representation of its "meaning"**. These embeddings are gradually refined during training as the model learns how tokens behave in different contexts.

### The Embedding Process (NLP BERT)

```
Input Sentence:  "The cat sat on the mat"

Step 1 - Tokenization:
    ["The", "cat", "sat", "on", "the", "mat"]

Step 2 - Lookup:
    Each token gets a fixed vector from an embedding table.

    "The" -> [0.25, 0.13, -0.11, ..., 0.04]
    "cat" -> [0.88, -0.23, 0.45, ..., -0.67]

Step 3 - Transformer Layers:
    These embeddings are updated based on surrounding words (context).
```

### Language Evolution is Decayed

The design of these token embeddings reflects a key fact about human languages: **the evolutionary history of words is largely irrelevant to understanding their meaning today**. While linguistic etymology exists, the meaning of "cat" today does not rely on whether the word originated from Latin or Proto-Indo-European. **Context (the words around "cat") matters far more than distant etymology.**

---

## 2. Biological Sequences are Fundamentally Different

In genomics, this assumption breaks down. Each nucleotide (A, T, G, C, -) has a **fixed biochemical meaning**, but its biological relevance **depends profoundly on its evolutionary history across species**.

### Key Differences Between Language and DNA

| Aspect | Natural Language | Genomics |
|---|---|---|
| Number of Tokens | Tens of thousands | ~5 (A, T, G, C, -) |
| Meaning | Flexible, evolves over time | Biochemically fixed |
| Evolutionary Context | Mostly irrelevant to meaning | Often crucial (conservation, divergence) |
| Token Embedding | Fully learned | Can be partly predefined (one-hot) |
| Neighboring Context | Defines meaning | Defines local motifs, but evolutionary context adds extra layer |

### Evolution is a Rich Context Layer

The **evolutionary history of a genomic position — how conserved it is, how it varies across species — directly influences its biological function**. A nucleotide in a highly conserved enhancer region means something different from a nucleotide in a rapidly evolving spacer.

---

## 3. Encoding Genomes with Evolution in Mind

To capture this **cross-species evolutionary context**, we need an **embedding strategy that combines:**

1. **The identity of the nucleotide itself (A, T, G, C, -)**.
2. **The state of this position in aligned species (what bases appear at the same position in other species).**

This evolutionary-aware encoding is at the heart of the **Genomic Pre-trained Network (GPN)** architecture.

---

## 4. Introducing GPN-BERT

GPN-BERT (inspired by Benegas et al., 2024) adapts BERT-style masked language modeling (MLM) to DNA sequences, but **incorporates multispecies alignment (MSA) data directly into the model's input**.

### Key Idea: Dynamic Position Embeddings

For each position in the human genome, the model receives:

- The **human base (A, T, G, C, -)** — this is the usual input.
- The **aligned bases from other species** — these are additional features.
- These aligned bases are **one-hot encoded** and concatenated to the human base's embedding.

This turns a **simple nucleotide (A)** into a **dynamic, position-specific vector** that depends on its **evolutionary context across species**.

### Visualization

```
Human Position:     A
Aligned Species:    A  G  A  (species 1, species 2, species 3)

Embedding:
    [ OneHot_A | OneHot_A | OneHot_G | OneHot_A ]
```

This combined vector captures:

- What the human base is.
- How conserved the site is.
- Which substitutions are tolerated across species.

---

### Practical Implementation - Replacing the BERT Encoder

To implement this in practice, we can directly modify a Hugging Face model class (like `ModernBertForMaskedLM`) to use our custom **GPNEmbedding** layer in place of the standard token embedding layer.

This requires:

- Defining a **GPNEmbedding** class that concatenates the one-hot human base with species features.
- Subclassing `ModernBertForMaskedLM` to replace the embedding layer.
- Ensuring the `forward` method accepts both `input_ids` and `aux_features`, which are passed into the embedding layer.

```python
class GPNEmbedding(nn.Module):
    def __init__(self, config, n_species):
        super().__init__()
        self.config = config
        self.n_species = n_species
        self.vocab_size = 5
        self.species_feature_size = self.n_species * self.vocab_size

    def forward(self, input_ids, aux_features):
        one_hot = F.one_hot(input_ids, num_classes=self.config.vocab_size).float()
        combined = torch.cat([one_hot[..., :self.vocab_size], aux_features], dim=-1)
        if combined.shape[-1] < self.config.hidden_size:
            combined = F.pad(combined, (0, self.config.hidden_size - combined.shape[-1]))
        return combined

class ModernBertForMaskedLMWithGPN(ModernBertForMaskedLM):
    def __init__(self, config, n_species):
        super().__init__(config)
        self.gpn_embedding = GPNEmbedding(config, n_species)

    def forward(self, input_ids=None, aux_features=None, **kwargs):
        embeddings = self.gpn_embedding(input_ids, aux_features)
        outputs = self.bert.encoder(inputs_embeds=embeddings, **kwargs)
        logits = self.cls(outputs[0])
        return {'logits': logits}
```

This illustrates how **pre-trained transformers can be adapted for genomics** while preserving compatibility with Hugging Face's `Trainer` ecosystem.

---


## 5. Recap of Our Approach

In **Chapter 2**, we trained a vanilla **BERT** on DNA sequences alone — treating DNA as just another language. That model only had access to the **human sequence**, with no evolutionary context.

In **this chapter**, we’ve reimagined that process. Instead of treating A, T, G, C, - as abstract symbols, we inject **evolutionary history directly into the input encoding**. This allows our model to:

- Use the aligned species data as **a rich evolutionary prior**.
- Still leverage transformers for learning **sequence motifs**.
- Predict masked human bases using both **local sequence** and **cross-species evolutionary patterns**.

---

## 6. Preview of Chapter 5

In **Chapter 5**, we will put these two models — **Vanilla BERT** and **GPN-BERT** — to the test. We will evaluate their performance on:

- Predicting masked bases (MLM accuracy).
- Identifying regulatory elements.
- Predicting the functional impact of mutations.

This head-to-head comparison will highlight the **strengths and weaknesses** of each approach and show the value of embedding **evolutionary context directly into genomic language models**.

---

## References

- Benegas et al., *Nature Biotechnology*, 2024.  
  [A DNA language model based on multispecies alignment predicts the effects of genome-wide variants](https://doi.org/10.1038/s41587-024-02511-w)

