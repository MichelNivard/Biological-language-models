---
title: "Training our first DNA Language Model: BERT"
format: html
---


## Introduction

Now that we have collected and formatted a DNA dataset in Chapter 1, we can proceed to train a language model using BERT. In this chapter, we will walk through the process of tokenizing DNA sequences, configuring a BERT model, and training it using the **Masked Language Modeling (MLM)** objective.

We will cover the key concepts behind tokenization, BERT, and MLM before diving into the Python script.

## Understanding Tokenization

### What is a Tokenizer?

A **tokenizer** is a fundamental component of any language model. Language models are essentially large multinomial models that predict the next token in a sequence based on previous tokens, or predict a masked token using the surrounding tokens. Since machine learning models operate on numerical representations, tokenized text must be converted into integers, which serve as indices for lookup in a vocabulary.

These integers, however, **have no inherent numeric value**—they simply act as categorical labels that the model learns to associate with semantic meaning. For example, if we tokenize the sentence:

> "The quick brown fox jumps over the lazy dog"

at the **word level**, we might obtain a numerical sequence like:

> `[4, 123, 678, 89, 245, 983, 56, 4564]`

where each number corresponds to a word based on a pre-defined tokenization dictionary, such as:

```         
{"the": 4, "quick": 123, "brown": 678, "fox": 89, "jumps": 245, "over": 983, "lazy": 56, "dog": 4564}
```

Similarly, for DNA sequences, each nucleotide (A, T, C, G) is assigned a unique integer ID.

### Our DNA Tokenizer

Our tokenizer uses a **character-level** approach, where each nucleotide is assigned a unique integer ID. Special tokens are also included for various purposes:

-   `[UNK]` (unknown token)
-   `[PAD]` (padding token for equal-length sequences)
-   `[CLS]` (classification token, useful for downstream tasks)
-   `[SEP]` (separator token, used in tasks like sequence-pair classification)
-   `[MASK]` (used for masked language modeling training)

**Python Code:**

``` python
import torch
from tokenizers import Tokenizer
from tokenizers.models import WordLevel
from tokenizers.pre_tokenizers import Split
from transformers import PreTrainedTokenizerFast

# --------------------------------
# 1. DNA Tokenizer with Full FASTA Nucleic Acid Code
# --------------------------------

# Define vocabulary to include all FASTA nucleotides and symbols
dna_vocab = {
    "A": 0, "T": 1, "C": 2, "G": 3, "N": 4, "U": 5, "i": 6,  # Standard bases + Inosine
    "R": 7, "Y": 8, "K": 9, "M": 10, "S": 11, "W": 12,  # Ambiguous bases
    "B": 13, "D": 14, "H": 15, "V": 16,  # More ambiguity codes
    "-": 17,  # Gap character
    "[UNK]": 18, "[PAD]": 19, "[CLS]": 20, "[SEP]": 21, "[MASK]": 22
}

# Create tokenizer
tokenizer = Tokenizer(WordLevel(vocab=dna_vocab, unk_token="[UNK]"))
tokenizer.pre_tokenizer = Split("", "isolated")  # Character-level splitting

# Convert to Hugging Face-compatible tokenizer
hf_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]"
)
```

### Other Tokenization Strategies for DNA, RNA, and Proteins

While character-level tokenization is effective, other tokenization approaches can offer different tradeoffs:

#### Byte Pair Encoding (BPE)

BPE is widely used in NLP and allows frequently occurring patterns to be merged into subword units. While BPE could be useful for repetitive genome sequences, it may not capture biologically meaningful units.

#### K-mer Tokenization

K-mer tokenization groups nucleotides into fixed-length substrings (e.g., 3-mers like "ATG"). This approach retains local sequence structure but can lead to a large vocabulary size.

#### Tiktoken and Similar Models

Some modern tokenization methods, such as Tiktoken, optimize speed and efficiency by precomputing merges. These are often optimized for large-scale NLP tasks but could be adapted for biological sequences.

Choosing the best tokenizer depends on the specific modeling task. For example, RNA secondary structures or protein folding models might benefit from more complex tokenization strategies.

Source: [RPubs Tokenization Review](https://rpubs.com/yuchenz585/1161578)

## Loading and Tokenizing the DNA Dataset

### Understanding the Dataset

We will use a pre-existing dataset, **Human-genome-CDS-GRCh38**, which contains coding sequences from the human genome.

### Tokenizing the Dataset

To prepare the dataset for training, we must **apply the tokenizer to each sequence** while ensuring:

-   Sequences are truncated or padded to a fixed length (512 tokens)
-   Unwanted columns are removed

**Python Code:**

``` python
from datasets import load_dataset

dataset_name = "MichelNivard/Human-genome-CDS-GRCh38"
dataset = load_dataset(dataset_name)

column_name = "sequence"

def tokenize_function(examples):
    return hf_tokenizer(examples[column_name], truncation=True, padding="max_length", max_length=512)

# Tokenize dataset
tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[column_name])
```

## Saving and Preparing the Dataset for Training

Once tokenized, we save the dataset for efficient access during training.

**Python Code:**

``` python
tokenized_dataset.save_to_disk("tokenized_dna_dataset")
```

## Understanding BERT and Masked Language Modeling (MLM)

### What is BERT?

BERT (**Bidirectional Encoder Representations from Transformers**) is a powerful transformer-based language model. Unlike traditional left-to-right models, BERT **learns bidirectional context**, allowing it to understand sequences more effectively.

Returning to our earlier example sentence:

> "The quick brown fox jumps over the lazy dog"

BERT does not process words one at a time but instead considers the entire sequence simultaneously. This bidirectional approach allows BERT to infer missing words based on context from both directions.

### What is Masked Language Modeling (MLM)?

MLM is a self-supervised learning objective where the model learns by predicting missing tokens in a sequence. During training:

-   **Some tokens are randomly replaced with `[MASK]`**
-   The model must **predict the original token** based on surrounding context

For example, if we mask the word "fox" in our sentence:

> "The quick brown \[MASK\] jumps over the lazy dog"

BERT will analyze the remaining words and attempt to predict "fox."

This technique enables BERT to **learn useful representations** without requiring labeled data.

### Understanding Transformer Layers, Attention Heads, and Hidden Size

A **transformer layer** consists of self-attention and feed-forward layers that help the model learn relationships between tokens. The number of **transformer layers** determines how deep the model is.

An **attention head** is a component of the self-attention mechanism that learns different types of relationships within the data. Having **multiple attention heads** allows the model to capture various dependencies between tokens.

Returning to our example:

-   One attention head might focus on subject-verb relationships, recognizing that "fox" is the subject of "jumps."
-   Another head might capture adjective-noun relationships, linking "brown" to "fox."

The **hidden size** defines the dimensionality of the model’s internal representations. A larger hidden size enables the model to capture more complex patterns, but also increases computational cost.

By stacking multiple transformer layers and attention heads, BERT gains a deep understanding of sentence structures.

## Defining the BERT Model for DNA Sequences

While the "quick brown fox" example helps us understand how BERT processes natural language, our goal is to apply the same principles to **DNA sequences**. Instead of predicting missing words in a sentence, we want our model to learn **biological patterns** and **genomic structure** by predicting masked nucleotides within DNA sequences.

In **DNA modeling**, understanding sequence context is just as critical as in language modeling. Just as BERT learns that "fox" fits within a given sentence structure, our model should learn that **specific nucleotide sequences appear in biologically meaningful patterns**. This could involve recognizing **gene coding regions, regulatory motifs, or conserved sequence elements** across different genomes.

To accomplish this, we define a **custom BERT model** designed specifically for processing DNA sequences. Unlike traditional text-based models, our DNA BERT model uses a **character-level vocabulary** of nucleotides (A, T, C, G) and special tokens to represent sequence structure. By leveraging **masked language modeling (MLM)**, the model will learn to predict missing nucleotides based on surrounding context, allowing it to capture meaningful genomic features.

With this in mind, let's move forward and define our BERT architecture for DNA sequences.

**Python Code:**

``` python
from transformers import ModernBertConfig, ModernBertForMaskedLM

config = ModernBertConfig(
    vocab_size=len(dna_vocab),
    hidden_size=256,
    num_hidden_layers=4,
    num_attention_heads=8,
    intermediate_size=512,
    max_position_embeddings=512,
    type_vocab_size=1,
)
config.pad_token_id = dna_vocab["[PAD]"]
model = ModernBertForMaskedLM(config)
```

### Configuring Training for DNA BERT

Now that we have defined our BERT model for DNA sequences, we need to set up the **training process**. This involves specifying various **training hyperparameters**, handling **masked language modeling (MLM)** data, and preparing for efficient learning.

Unlike traditional NLP tasks where models are trained on massive text corpora, training on DNA sequences presents unique challenges. The structure of genomic data, sequence length, and biological patterns all influence how the model learns. Our configuration ensures that the training process is efficient while maintaining biological relevance.

------------------------------------------------------------------------

### Setting Training Parameters

To train our DNA BERT model, we use the **Hugging Face `TrainingArguments` class**, which allows us to define key training settings. These include:

-   **Batch size:** We set a batch size of `16` for both training and evaluation. This determines how many sequences are processed at once.
-   **Logging & Saving:** We log loss every `50` steps and save model checkpoints every `100` steps to monitor training progress.
-   **Learning Rate:** We use a learning rate of `5e-5`, a common choice for transformer models that balances learning speed and stability.
-   **Weight Decay:** A value of `0.01` is used to prevent overfitting by applying **L2 regularization** to model weights.
-   **Training Steps:** The model is trained for `4000` steps. This ensures sufficient learning without excessive computation.
-   **Model Saving:** The model checkpoints are stored in `./bert-dna`, allowing us to resume training if needed.

**Python Code:**

``` python
training_args = TrainingArguments(
    output_dir="./bert-dna",
    overwrite_output_dir=True,
    logging_steps=50,  # Log loss every step
    save_steps=100,
    save_total_limit=2,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    max_steps=4000,
    learning_rate=5e-5,
    weight_decay=0.01,
    push_to_hub=False,
    report_to="none",  # Disables wandb logging
)
```

------------------------------------------------------------------------

### Preparing for Masked Language Modeling (MLM)

Since we are training our DNA BERT model using **masked language modeling (MLM)**, we need to handle **masked tokens** properly. This is done using the **`DataCollatorForLanguageModeling`**, which:

-   **Randomly masks nucleotides** in the training sequences.
-   **Creates `labels` automatically**, meaning the model learns by trying to predict these masked tokens.
-   **Uses a masking probability of 5%**, ensuring that a small but meaningful portion of the sequence is masked during training.

By applying MLM, we allow the model to **generalize nucleotide relationships** and capture **sequence dependencies**, just like how BERT learns relationships between words in text.

**Python Code:**

``` python
data_collator = DataCollatorForLanguageModeling(
    tokenizer=hf_tokenizer,
    mlm=True,
    mlm_probability=0.05
)
```

------------------------------------------------------------------------

### Training the DNA BERT Model

With our configuration and data collator in place, we now **train the model**. We use the **Hugging Face `Trainer` API**, which simplifies the training process by handling:

-   **Dataset iteration:** Automatically loads and batches training sequences.
-   **Gradient updates:** Adjusts model weights based on training loss.
-   **Logging & saving:** Tracks training progress and stores checkpoints.

Once training begins, the model will gradually **learn nucleotide dependencies** and improve its ability to predict missing DNA bases.

**Python Code:**

``` python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=hf_tokenizer,
    data_collator=data_collator,
)

trainer.train()
```

you set up free wandb logging (go to <https://wandb.ai/site> for more info) you can track your training runs online, wherever they are running. You then get a dashboard full of pretty loss vs progress plots like the one below which I screencapped about ± 30 minutes into training on my macbook.

![](images/paste-4.png)

### Saving the Trained Model

After training completes, we save both the **model** and **tokenizer** so they can be used for future predictions or fine-tuning.

-   The **model weights** are stored in `./bert-dna`, allowing us to reload the trained model.
-   The **tokenizer** is also saved, ensuring that input sequences can be processed the same way during inference.

Finally, a success message is printed, confirming that the training process has been completed.

**Python Code:**

``` python
# Save the final model and tokenizer
trainer.save_model("./bert-dna")
hf_tokenizer.save_pretrained("./bert-dna")

print("🎉 Training complete! Model saved to ./bert-dna")
```

### Summary

In this section, we:

-   Defined **training hyperparameters** such as batch size, learning rate, and training steps.
-   Used **masked language modeling (MLM)** to train the model on DNA sequences.
-   Leveraged the **Hugging Face `Trainer` API** to automate model training.
-   Saved the **final trained model and tokenizer** for future use.

With this trained model, we can now **fine-tune or apply it to various genomic tasks**, such as predicting genetic variations or classifying functional DNA sequences. In the next chapter, we will explore **how to fine-tune our DNA BERT model for specific applications**.

