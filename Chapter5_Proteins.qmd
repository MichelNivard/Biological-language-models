# Integrated protein diffusion language models

::: callout-tip
## Abstract

In this chapter, we'll discuss AlphaFold3 and its joint sequence (language model like) & diffusion (Image generation model like) architecture. To do so we'll need to covewr three core deeplearning concepts, 1. denoising diffusion 2. guided diffusion and 3. cross attention (the actual link between sequecnes and 3D molecules). The code used for this chapter is found here: <https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/Proteins/Chapter11>
:::

## Alphafold3

Alphafold3[@abramson2024b] has what initially looks like a very complex architecture (See **Figure 1**), at least it did to me for the longest time.

![**Figure 1:** (source: Figure 1d Alphafold paper[@abramson2024b]) which abstracts the model architecture used in Alphafold3. We'll talk trough hings step by step below.](images/paste-33.png)

What's new in this architecture is the "Diffusion module" on the bottom right. It has 2 inputs, the results form the other model elements feed into it (blue paths) and a weird little point cloud. Because I am not interested in digging very deeply into the language model like inputs right now, we are going to replace the inputs with a generic protein language model ( `EvolutionaryScale/esmc-300m-2024-12` to be precise, ESM2's successor) and attach a diffusion model to it. Now since I don't have the compute that google deepmind has (and neither do you...) I am not training a 3D diffusion model that generates 3D molecules (or even whole protein complexes), instead we'll train a 2D diffusion model that, guided by the protein language model, generates protein contact maps. As You can imagine training a model to model 128\*128 distance/contact values is easier on my GPU/wallet then modeling a 128\*128\*128 3D molecule. Fortunately for us if we would want to go from a 2D to a 3D model we'd simply replace a few 2D operators with 3D operators and so we'll actually be able to cover all concepts involved without training very heavy models.

## Diffusion models

Gaussian Denoising DIffusion models are a very flexible, and ingenious, class of deep learning models for images. Architectures derived form diffusion models the conceptual basis for all kinds of famous image generation models, like the one integrated into ChatGPT or other image generation AI models.

Diffusion modles have two parts, a forward process, which is defined, or fixed, and a reverse process which is learned. The forward process takes images (in our case of protein distance maps) and adds seqeucnetially more noise (See Figure 2).

The relation between the image (x) at time t, and t-1 is:

$$
X_{t-1} =  b_0 * X_{t} + b_1 * \mathcal{N}(\mu,\sigma)
$$

Where $b_0$ and $b_1$ are parameterized in a clever way, such that they 1. depend on t alone and 2. the variance of the total image stays the same, or is controlled. This means that we dont have to store/create all \$X\_{t-1} \$ images bu can reconstruct them from t and the input image $X_0$. For a specific protein contact map ( $X_0$ )the "noising" sequence might look a little like **figure 2**.

```{r}

```

![**Figure 2**: Denoising diffusion forward process](images/paste-35.png)

A diffusion model uses the, and the value of $t$ to learn to "denoise" the image. It doesnt denoise in one step though it trains to denoise from t=500 to t=499, t=150 to t=149 etc etc. They way this happens s that the images are embedded with their specific timestep $t$ such that one model (generally a U-Net architecture) can learn weights that are able to optimally estimate (and the subtract) the noise for. given image at time step t.

### Diffusion model architecture

Each "down block" in a diffusion U-Net starts with a normalization layer, which helps keep the model stable during training by making sure the numbers flowing through the network stay within a reasonable range. Then comes an activation function, like Swish or GELU, which adds flexibility to the model and helps it learn more complex patterns. The core part of the block is a **Conv2D layer**, which looks at small squares of pixels (like 3×3 patches) and learns to summarize what's in them—kind of like learning to detect edges, textures, or other useful features. A special trick used in diffusion models is the **time embedding**, which tells the model what step of the denoising process it's on. This time information is turned into numbers and added to the features in the block so the model can behave differently at each step.

After the main part of the down block, there's a **downsampling layer** that reduces the size of the image (usually by half) so the next layer can focus on a broader view of the picture. This is often done with a strided convolution, which skips over pixels to shrink the height and width while keeping the most important features. Skip connections pass the feature maps from each downsampling stage directly to the matching upsampling stage on the other side of the U. This helps the model keep important details that might be lost during compression, allowing the decoder to reconstruct sharper and more accurate outputs.

In the **middle of the U-Net**, after the deepest downsampling layer, there's often a **self-attention block**. This layer helps the model understand **global relationships** in the image — for example, connecting far-apart pixels that should be related (like opposite ends of a stripe or outline). Since it operates at the most compressed resolution, it’s efficient but powerful, and it benefits from the time embedding just like the ResBlocks.

```{r}
library(DiagrammeR)

grViz("
digraph diffusion_unet {
  graph [layout = dot, rankdir = LR, fontsize = 30]

  // Node styles
  node [shape=box, style=filled, fontname=Helvetica, fontsize=30]

  // Input and encoder
  input     [label='Input Image\\n(1×128×128)', fillcolor=lightgray]
  init_conv [label='Init Conv\\n(1 → 64)', fillcolor=lightgray]
  down1     [label='Down Block 1\\nResBlock ×2\\n(64)', fillcolor=lightblue]
  down2     [label='Down Block 2\\nResBlock ×2\\n(128)', fillcolor=lightblue]
  down3     [label='Down Block 3\\nResBlock ×2\\n(256)', fillcolor=lightblue]
  down4     [label='Down Block 4\\nResBlock ×2\\n(512)', fillcolor=lightblue]

  // Bottleneck
  mid1      [label='Mid Block 1\\nResBlock\\n(512)', fillcolor=gold]
  mid_attn  [label='Mid Attention\\nSelf-Attn\\n(512)', shape=ellipse, fillcolor=orange]
  mid2      [label='Mid Block 2\\nResBlock\\n(512)', fillcolor=gold]

  // Decoder (upsampling path)
  up4       [label='Up Block 4\\nResBlock ×2\\n(512 → 256)', fillcolor=lightblue]
  up3       [label='Up Block 3\\nResBlock ×2\\n(256 → 128)', fillcolor=lightblue]
  up2       [label='Up Block 2\\nResBlock ×2\\n(128 → 64)', fillcolor=lightblue]
  up1       [label='Up Block 1\\nResBlock ×2\\n(64)', fillcolor=lightblue]
  final_res [label='Final ResBlock\\n(64)', fillcolor=lightblue]
  final_conv[label='Final Conv\\n(64 → 1)', fillcolor=lightblue]
  output    [label='Output Image\\n(1×128×128)', fillcolor=lightgray]

  // Time embedding
  time_mlp  [label='Time MLP\\nSinusoidal + MLP\\n(→ 64/128/256/512)', shape=hexagon, fillcolor=pink]

  // Flow (encoder → bottleneck → decoder)
  input     -> init_conv -> down1 -> down2 -> down3 -> down4
  down4     -> mid1 -> mid_attn -> mid2 -> up4 -> up3 -> up2 -> up1 -> final_res -> final_conv -> output

  // Time embedding connections to all ResBlocks
  time_mlp -> down1
  time_mlp -> down2
  time_mlp -> down3
  time_mlp -> down4
  time_mlp -> mid1
  time_mlp -> mid2
  time_mlp -> up4
  time_mlp -> up3
  time_mlp -> up2
  time_mlp -> up1
  time_mlp -> final_res

  // Skip connections across the U
  down4 -> up4 [style=dashed, color=darkgray]
  down3 -> up3 [style=dashed, color=darkgray]
  down2 -> up2 [style=dashed, color=darkgray]
  down1 -> up1 [style=dashed, color=darkgray]
}
")


```

**Up blocks** in a diffusion U-Net are the mirror of the down blocks—they take the compressed features and gradually rebuild the image to its original size. Each up block typically starts by **upsampling** the feature map, usually with a transpose convolution or nearest-neighbor interpolation followed by a Conv2D, which increases the height and width (often doubling them). After upsampling, the block combines the upsampled features with the skip connection from the matching down block, so it has both high-level context and fine details. It then passes this combined input through one or more ResBlocks, just like in the encoder, using normalization, activation, convolutions, and time embedding again to refine the reconstruction.

At the start, the scalar timestep `t` is turned into a **high-dimensional vector** using a sinusoidal position embedding, and then passed through a small MLP (multi-layer perceptron) to create a learned time embedding. This embedding is then **injected into nearly every ResBlock** in the model — both in the down blocks, up blocks, and middle blocks. Inside each ResBlock, the time embedding is added (broadcasted) to the feature map after a linear layer transforms it to match the number of channels. This allows every part of the network to be conditioned on how much noise it should expect and how aggressively to denoise.

### Diffusion model for contact maps

As a trainignset I build 7000 contact maps, based on experimentally validated proteins in the CASP12 set. I do not build strictly binary contact maps, but build images that reflect 9 distances thresholds (see **Figure 3** for an example entry form the training set).

![**Figure 3**: A distance map based on experimental CASP12 data.](images/1KZQ_d1kzqb1.jpg){fig-align="center" width="444"}

#### **Detailed Parameter Breakdown by Module for our model**

| Module | Parameters | Description |
|----|----|----|
| `ups` | 19,832,768 | **Upsampling path (decoder):** progressively reconstructs the denoised image from compressed features. Includes ResBlocks, upsampling (e.g. transposed convolutions), and skip connections from encoder layers. |
| `downs` | 5,402,816 | **Downsampling path (encoder):** extracts hierarchical features from the noisy input image using stacked ResBlocks and downsampling layers. |
| `mid_block1` | 4,983,808 | **First bottleneck ResBlock:** processes the most compressed latent representation of the input, directly before/after the attention block. |
| `mid_block2` | 4,983,808 | **Second bottleneck ResBlock:** further transforms latent features after attention at the bottleneck. Acts as a transition before decoding. |
| `mid_attn` | 264,192 | **Self-attention at bottleneck:** captures global spatial dependencies in the most compressed feature map, enabling long-range interactions. |
| `final_res_block` | 152,000 | **Final ResBlock before output:** fuses decoder output and prepares it for the final convolution. Often used to refine the final image prediction. |
| `time_mlp` | 82,432 | **Timestep embedding network:** converts scalar timestep into a vector that conditions all ResBlocks, allowing the model to denoise appropriately for each diffusion step. |
| `init_conv` | 3,200 | **Initial input convolution:** expands the input image from 1 channel to base feature dimension (`dim=64`), preparing it for downstream processing. |
| `final_conv` | 65 | **Final output convolution:** projects the final hidden features back to 1 channel to match the original image shape. Predicts either noise (`ε_t`) or clean image (`x₀`). |

![](images/paste-36.png)