# "How to read this book" {.unnumbered}


## Practicalities

The book is accompanied by scripts in both the R and Python programming languages. I had to make some choices, some of the biological data repositories have great integrated perl and R packages, I wouldn't want to force people into perl (especially not myself!), I am more comfortable wrangling the initial data in R then in Python so here we are.

If you want to code along, rest assured you can run most of this on a macbook. Maybe you'll need to run a training run overnight a few times. If you want a bit more performance, or not have your MAckbook turn into a space heater for 24 hours you can use google colab for access to A100 GPUs. Training the DNABert model we outline in Chapter 2 on 500k coding sequences from 13 species took ±6 hours on an A100 on Colab, which means that cost me ±4\$ in colab credit.

The Gihub repo that hosts the book will be populated with all the scripts I discuss and use. The data used to train the models, and some of the models themselves, will be hosted on Huggingface (a repository of ML training data). I will try to make jupyter notebooks available though given my R background I usually run Puython in a REPL because that is what R people do...

If you come at this from an R background, and Python isn't your native language, I can highly recommend using [positron](https://positron.posit.co) as an IDE when following along with this book, its a vscode derivative developed by Posit, the company that developed RStudio. Positron has tooling integrated for data science in both python and R and I can switch between python and R sessions instantly!

## Structure

The book is divided up into sections that deal with a specific biological modality or data type. the last chapter in each section is a review of current models. Its more common to begin a chapter reviewing whats available out there, but given the novelty of these models it makes sense to learn how they work before reading a review of what's out there. There is risk of the reader attributing insights to me, simply because I describe it to you first. I'll always cite my sources as if this is a peer reviewed article, and you should assume most models we build together are directly, or indirectly, influenced by the literature. I also ask you do not cite this book other then for novel content, or to refer to it as teaching material, please for models, architectures, insights cite the underlying empirical literature.

### DNA language models

**Chapter 1 covers** downloading, and pocessing (DNA) sequences data from ensembl and uploading it to Huggingface. **Chapter 2 covers** training a first small DNA sequence language mode, the model is a bog standard language model, meant for natural languages simply applied to DNA. in **Chapter 3** we explore how you'd evaluate whether a DNA model is any good, is our model learning anything at all? Then in **Chapter 4**

### Protein language models

### Multi-modal models (DNA meets proteins)