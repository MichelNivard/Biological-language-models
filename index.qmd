# Preface {.unnumbered}

These are my study notes on training DNA/RNA/Protein and other biomedical language models. The text/book is intended for people who want to casually explore the field before they on-ramp to actually training large DNA/Biological language models, or for their PIs—anxious aging millennials or wise Gen-X'ers who want to be able to understand the next generation of computational genomics that is about to wash over us all.

At all times, I'll try to add minimal biological context (though I am no biologist!) for people who have an ML background but no college bio experience, and I'll try to add context on ML concepts for those with a bio background but limited experience with language models.

## The Times We Live In

More than natural language models, biological/sequence language models rely heavily on NIH-funded databases, datasets, resources, and scientists. The data we train on was bought and paid for by taxpayers all over the globe. The Human Genome Project was to a great extent funded, directed, and conceived under the auspices of the US federal government, under both Democratic and Republican presidents. Had they not, pharmaceutical companies might have done it, and while those can be highly innovative, there would have been no space for startups, no space for Google DeepMind to come in and iterate, revolutionize, or grow biological modeling. There is no uproar over training data in biology because under the firm guidance of US federal policy, all the data sequencers generate is generally in the public domain, or accessible for those willing and able to meet ethical standards. All scientists reading this know this—should you find yourself as someone from Silicon Valley, from a well-funded startup even, take a beat and think through whether you'd stand a snowball's chance in hell to compete if the next wave of sequence data isn't public but generated inside Google/Microsoft/pharma. Then adjust your politics accordingly.

## Acknowledgements

These notes are written by me, Michel Nivard, a professor of Epidemiology at the University of Bristol, and as this book is not a core output for my job, I rely on LLMs to help me with drafting, spelling, and formatting.

These study notes are influenced by discussions with Robbee Wedow and Seyedeh Zahra Paylakhi, with whom I work on related projects.