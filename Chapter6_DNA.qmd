# A Review of Current DNA Language Models {#sec-DNAreview}

This is not an academic review; those are obviously far more comprehensive. See @benegas2025 for a review I really liked.

## Modeling Paradigms

As we learned in the first 5 chapters, gLMs (genomic Language Models) borrow from natural language processing by treating DNA sequences as “text” composed of four characters (A, C, G, T). Early models used k-mer tokenization (e.g., DNABERT), but newer approaches experiment with both nucleotide‐level and subword tokenizations (such as 3-mer or 6-mer) to better capture biological semantics.

## Architectural Innovations

There are two specific architectural innovations in DNA language models worth discussing. The first is GPA-MSA, the core idea behind a model we discussed in Chapter 4. In this model, the trainable embedding layer is replaced with a biologically informed deterministic embedding that reflects the evolutionary history of the genome at a given base.

A second innovation was necessitated by the need to model long-range dependence in DNA (changes to DNA can have effects over thousands of bases "downstream"). While transformer-based models initially dominated the field, their quadratic scaling with sequence length has prompted the development of more efficient architectures. Models such as HyenaDNA extend context lengths up to 1 million tokens at single-nucleotide resolution, and hybrid architectures like HybriDNA combine transformers with selective state-space models (Mamba2) to process sequences up to 131 kilobases. Omni-DNA and GENERator further illustrate the trend toward unified, cross-modal genomic foundation models capable of multitask learning.

Below is a summary table of several prominent DNA language models with innovative architectures, along with links to their corresponding paper and GitHub (or related resource) repositories:

| Model | Description | Paper Link | GitHub / Resource Link |
|-----------------|--------------------|-----------------|-----------------|
| **DNABERT** | A transformer-based model that learns bidirectional representations from DNA sequences using k-mer tokenization. | [DNABERT Paper](https://doi.org/10.1093/bioinformatics/btab083) | [GitHub](https://github.com/jerryji1993/DNABERT) |
| **Nucleotide Transformer (NT‑v2)** | A large transformer pretrained on massive human genomic data to learn robust DNA representations for various downstream tasks. | [Nucleotide Transformer v2](https://doi.org/10.1038/s41592-024-02523-z) | [GitHub](https://github.com/instadeepai/nucleotide-transformer)/[HuggingFace](https://huggingface.co/collections/InstaDeepAI/nucleotide-transformer-65099cdde13ff96230f2e592) |
| **GPN** | The Genomic Pre-trained Network that leverages unsupervised DNA language modeling to predict genome-wide variant effects. | [GPN Paper (PNAS 2023)](https://doi.org/10.1073/pnas.2311219120) | [GitHub](https://github.com/songlab-cal/gpn) |
| **GPN-MSA** | The Genomic Pre-trained Network that leverage multiple sequence alignment across species to develop specialized evolution-aware token embedding. | [GPN-MSA preprint](https://pubmed.ncbi.nlm.nih.gov/37873118/) | [GitHub](https://github.com/songlab-cal/gpn) |
| **HyenaDNA** | A long-range genomic language model operating at single-nucleotide resolution using Hyena’s implicit convolutional approach to overcome transformer scaling issues. | [HyenaDNA (arXiv)](https://arxiv.org/abs/2306.15794) | [GitHub](https://github.com/HazyResearch/hyena-dna) |
| **HybriDNA** | A hybrid model combining Transformer and Mamba2 (state-space) architectures for efficient long-range DNA modeling. | [HybriDNA (arXiv)](https://arxiv.org/abs/2502.10807) |  |
| **Omni‑DNA** | A unified genomic foundation model that supports cross-modal and multi-task learning across a wide range of genomic applications. | [Omni‑DNA (arXiv)](https://arxiv.org/abs/2502.03499) | [Hugging Face Collection](https://huggingface.co/collections/zehui127) |
| **GENERator** | A long-context generative genomic foundation model designed for sequence generation and optimization tasks with a context length of up to 98k bp. | [GENERator (arXiv)](https://arxiv.org/abs/2502.07272) | [GitHub](https://github.com/GenerTeam/GENERator) |

This table highlights each model’s core features and provides direct access to the publication and code repository (or resource page) where available.

## Applications

These models have demonstrated state-of-the-art performance across multiple downstream tasks including: - **Variant Effect Prediction:** Unsupervised approaches can predict deleterious mutations by modeling the “grammar” of the genome. - **Regulatory Element Identification:** By learning long-range interactions, gLMs help detect promoters, enhancers, and other regulatory motifs. - **Sequence Generation and Protein Tasks:** Some models generate synthetic regulatory sequences or transform coding DNA into meaningful protein representations, bridging genomics and proteomics.

## Challenges and Future Directions

There are some interesting immediate challenges that become apparent form the literature. One very obvious one is a better grasp of the relation between training data (multi-species sequences vs intra human variation) and model performance. The nucleotide transformer paper [@dalla-torre2024] highlights how as things stand training on human sequences does not improve the model over multi-species training. Its obviously true that most bases are identical for most people, and so the human training data has low variability, which might adversely impact the model. It could also be that for the specific validation tests evolutionary constraint convey's more information than the limited human variation in 1000-genomes.

one specific avenue for exploration could therefore be to deeply consider the order in which training data is presented (first train across species, then train within humans) and the effect of learning rate on specific segments of training. YOu could imagine training at a high learning rate using the high variance intra-species data and then train at a lower learning rate with the low variance sequences, only modestly updating the model in that phase. Alternatively you could consider "low rank training", where the model is first trained on multi-species data and then ony fine-tuned on human data, restricting the degrees of freedom in that second phase trough low rank matrix approximation (LoRa)[@hu2021] which learns less, burt also forgets less from previous training epochs[@biderman2024].