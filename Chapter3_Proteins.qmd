# Training our first Protein Language Model

::: callout-tip
## Abstract

In this chapter, we'll train a protein language model. To run a full training session, you will need a powerful GPU setup or access to cloud computing services, the script below trains on GPCR proteins only to ensure we can study protein language models on a MacBook or moderately powerfull workstation. A Google Colab notebook to train the model is available [here](#).

If you lack the necessary computing resources, you can download a pre-trained version of GPCR-BERT [here](https://huggingface.co/MichelNivard/GPCRBert-v0.1)

All scripts for this chapter are found here: <https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/Proteins/Chapter_3>
:::

## Introduction

Now that we have prepared a dataset of protein sequences, we can proceed to train a protein language model using the (Modern)BERT model architecture. Protein sequences, like DNA, follow structured patterns that can be learned by deep learning models. This chapter introduces the training of GPCR-BERT, a transformer-based masked language model (MLM) designed to understand and generate protein sequences.

Because the fundementals of trainign a Protein language model arent that different then trainign a DNA language model this chapter is somewhat abbriviated, see @sec-DNALM for more context, most of which directly translates to protein language models.

## Dataset & Preprocessing

Unlike large-scale protein language models trained on massive datasets from UniProt or AlphaFold, our approach focuses on a more specialized subset: G-protein coupled receptor (GPCR) genes, sourced from RefProt 90. This choice is driven by computational constraints and the goal of achieving high-quality representations within a biologically meaningful domain. By narrowing our scope, we ensure that the model learns from a well-defined set of sequences, optimizing for accuracy and relevance in GPCR-related research rather than sheer scale.

Protein sequences are represented using the FASTA format, which encodes amino acids with standard single-letter codes. For this model, we define a vocabulary that includes:

- The 20 standard amino acids (A, R, N, D, C, Q, E, G, H, I, L, K, M, F, P, S, T, W, Y, V)
- Ambiguity codes: B (N/D), Z (Q/E), X (any)
- A gap character: '-'

A custom tokenizer is developed to handle these sequences efficiently. The tokenizer splits sequences using the defined vocabulary and applies a `WordLevel` tokenization approach for better generalization.

## Model Architecture & Training

We use the `ModernBertForMaskedLM` model from the `transformers` library. The key components of the training setup are:

- **Tokenizer**: A `PreTrainedTokenizerFast` instance is initialized with our protein vocabulary.
- **Model Configuration**: `ModernBertConfig` defines the modelâ€™s parameters, including the number of transformer layers and attention heads.
- **Data Collation**: `DataCollatorForLanguageModeling` is used to prepare masked language modeling inputs.
- **Training Arguments**: Configured with learning rate scheduling, batch size, and gradient accumulation settings.
- **Trainer**: The `Trainer` class orchestrates training, validation, and logging using `wandb` (Weights & Biases).

Training is conducted on protein sequence datasets, with GPCR (G-protein coupled receptor) sequences serving as a primary example.

## Evaluation & Results

The trained model is evaluated using perplexity and masked token prediction accuracy. We compare its performance to other protein models and analyze its ability to generate coherent amino acid sequences.

## Conclusion

This chapter introduced GPCR-BERT, a transformer-based language model for protein sequences. In subsequent chapters, we will explore fine-tuning strategies and downstream applications, such as functional annotation and protein structure prediction.

