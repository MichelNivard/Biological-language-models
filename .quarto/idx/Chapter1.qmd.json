{"title":"Preparing DNA data for training","markdown":{"yaml":{"title":"Preparing DNA data for training","format":"html"},"headingText":"Abstract","containsRefs":false,"markdown":"\n\n::: callout-tip\n\nIn this chapter we'll learn how to create a high fidelity DNA datasets. Two datasets I have created are available on Huggingface: [the human CDS](https://huggingface.co/datasets/MichelNivard/Human-genome-CDS-GRCh38) and [the CDS of 13 vertebrae species](https://huggingface.co/datasets/MichelNivard/Coding_Sequences_13_Species).\n:::\n\n## Garbage in garbage out\n\nHigh-quality data is essential for training effective machine learning models. In natural language processing (NLP), researchers invest significant effort in curating clean datasets from raw internet text. For example a dataset like 'fineweb-edu' contains English text that is of very high quality. Models trained on fineweb-edu (and similar high quality datasets) will improve MUCH faster then the equivalent model trained on other less carefully processed and evaluated datasets.\n\n![Relative training efficiency using a high quality dataset like fineweb-edu and other massive datasets of English language texts. Image obtained from: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu](https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/QqXOM8h_ZjjhuCv71xmV7.png){fig-align=\"center\"}\n\nThose with experience with **genetics** will know most of what I am about to explain when it comes to the sources of genetic data and the file formats these come in. Those with an **ML** background will be very familiar with Huggingface and the amazing data/model/training integration their libraries offer. Assembing high quality genomics datasets for language modeling requires familiarity with both. When working with DNA, RNA, and protein sequences, the authoritative sources of data are specialized databases and infrastructures rather than data scraped from the internet. If you want to learn to train DNA/RNA/Protein-based language model, we must learn how, and where, to retrieve data and convert it into a structured format.\n\nIn this tutorial, we will begin by collecting coding DNA sequences (CDS) from the human genome using the Ensembl database via BioMart. The goal is to store these sequences in a format suitable for training a DNA-based language model. We will then upload the processed data to [Huggingface](https://huggingface.co), a platform for hosting datasets and models for machine learning tasks.\n\nThis chapter will introduce key biological and computational concepts, ensuring that both biology newcomers and those unfamiliar with language modeling tools can follow along.\n\n## Understanding Ensembl and BioMart\n\nFortunately for us, there is decades of work cleaning up genomic data and we can just go and get it from US government funded websites, where it is deposited by the global scientific community. **Ensembl** is a genomic database that provides extensive annotations for various species, including humans. It offers access to gene sequences, transcript annotations, and protein-coding information. One of its most powerful tools is **BioMart**, a flexible data retrieval system that allows users to download specific genomic datasets easily.\n\nIf we want t work with the data in a language model its efficient to store it in a format that is tailored for machine learning libraries. Unlike text-based NLP datasets, genomic data is structured differently and must be properly formatted before use in a model.\n\n### What Are FASTA Files?\n\nA **FASTA file** is a simple text-based format used for storing biological sequences. Each entry in a FASTA file consists of: 1. A **header line** (starting with `>`), which contains metadata such as gene IDs and chromosome locations. 2. A **sequence** line, which contains the nucleotide or protein sequence.\n\nThere is a very comprehensive Wikipedia entry on the FASTA format.\n\n\"Sequences may be protein sequences or nucleic acid sequences, and they can contain gaps or alignment characters (see sequence alignment). Sequences are expected to be represented in the standard IUB/IUPAC amino acid and nucleic acid codes, with these exceptions: lower-case letters are accepted and are mapped into upper-case; a single hyphen or dash can be used to represent a gap character; and in amino acid sequences, U and \\* are acceptable letters (see below). Numerical digits are not allowed but are used in some databases to indicate the position in the sequence.\" ((source: <https://en.wikipedia.org/wiki/FASTA_format>))\n\n| Nucleic Acid Code | Meaning | Mnemonic |\n|:----------------------:|:----------------------:|:----------------------:|\n| A | A | [**A**denine](https://en.wikipedia.org/wiki/Adenine \"Adenine\") |\n| C | C | [**C**ytosine](https://en.wikipedia.org/wiki/Cytosine \"Cytosine\") |\n| G | G | [**G**uanine](https://en.wikipedia.org/wiki/Guanine \"Guanine\") |\n| T | T | [**T**hymine](https://en.wikipedia.org/wiki/Thymine \"Thymine\") |\n| U | U | [**U**racil](https://en.wikipedia.org/wiki/Uracil \"Uracil\") |\n| \\(i\\) | i | [**i**nosine](https://en.wikipedia.org/wiki/Inosine \"Inosine\") (non-standard) |\n| R | A or G (I) | [pu**R**ine](https://en.wikipedia.org/wiki/Purine \"Purine\") |\n| Y | C, T or U | [p**Y**rimidines](https://en.wikipedia.org/wiki/Pyrimidine \"Pyrimidine\") |\n| K | G, T or U | bases which are [**K**etones](https://en.wikipedia.org/wiki/Ketone \"Ketone\") |\n| M | A or C | bases with [a**M**ino groups](https://en.wikipedia.org/wiki/Amino \"Amino\") |\n| S | C or G | **S**trong interaction |\n| W | A, T or U | **W**eak interaction |\n| B | not A (i.e. C, G, T or U) | **B** comes after A |\n| D | not C (i.e. A, G, T or U) | **D** comes after C |\n| H | not G (i.e., A, C, T or U) | **H** comes after G |\n| V | neither T nor U (i.e. A, C or G) | **V** comes after U |\n| N | A C G T U | **N**ucleic acid |\n| \\- | gap of indeterminate length |  |\n\nThe amino acid codes supported (22 amino acids and 3 special codes) are:\n\n| Amino Acid Code | Meaning |\n|:----------------------------------:|:----------------------------------:|\n| A | [Alanine](https://en.wikipedia.org/wiki/Alanine \"Alanine\") |\n| B | [Aspartic acid](https://en.wikipedia.org/wiki/Aspartic_acid \"Aspartic acid\") (D) or [Asparagine](https://en.wikipedia.org/wiki/Asparagine \"Asparagine\") (N) |\n| C | [Cysteine](https://en.wikipedia.org/wiki/Cysteine \"Cysteine\") |\n| D | [Aspartic acid](https://en.wikipedia.org/wiki/Aspartic_acid \"Aspartic acid\") |\n| E | [Glutamic acid](https://en.wikipedia.org/wiki/Glutamic_acid \"Glutamic acid\") |\n| F | [Phenylalanine](https://en.wikipedia.org/wiki/Phenylalanine \"Phenylalanine\") |\n| G | [Glycine](https://en.wikipedia.org/wiki/Glycine \"Glycine\") |\n| H | [Histidine](https://en.wikipedia.org/wiki/Histidine \"Histidine\") |\n| I | [Isoleucine](https://en.wikipedia.org/wiki/Isoleucine \"Isoleucine\") |\n| J | [Leucine](https://en.wikipedia.org/wiki/Leucine \"Leucine\") (L) or [Isoleucine](https://en.wikipedia.org/wiki/Isoleucine \"Isoleucine\") (I) |\n| K | [Lysine](https://en.wikipedia.org/wiki/Lysine \"Lysine\") |\n| L | [Leucine](https://en.wikipedia.org/wiki/Leucine \"Leucine\") |\n| M | [Methionine](https://en.wikipedia.org/wiki/Methionine \"Methionine\")/[Start codon](https://en.wikipedia.org/wiki/Start_codon \"Start codon\") |\n| N | [Asparagine](https://en.wikipedia.org/wiki/Asparagine \"Asparagine\") |\n| O | [Pyrrolysine](https://en.wikipedia.org/wiki/Pyrrolysine \"Pyrrolysine\") (rare) |\n| P | [Proline](https://en.wikipedia.org/wiki/Proline \"Proline\") |\n| Q | [Glutamine](https://en.wikipedia.org/wiki/Glutamine \"Glutamine\") |\n| R | [Arginine](https://en.wikipedia.org/wiki/Arginine \"Arginine\") |\n| S | [Serine](https://en.wikipedia.org/wiki/Serine \"Serine\") |\n| T | [Threonine](https://en.wikipedia.org/wiki/Threonine \"Threonine\") |\n| U | [Selenocysteine](https://en.wikipedia.org/wiki/Selenocysteine \"Selenocysteine\") (rare) |\n| V | [Valine](https://en.wikipedia.org/wiki/Valine \"Valine\") |\n| W | [Tryptophan](https://en.wikipedia.org/wiki/Tryptophan \"Tryptophan\") |\n| Y | [Tyrosine](https://en.wikipedia.org/wiki/Tyrosine \"Tyrosine\") |\n| Z | [Glutamic acid](https://en.wikipedia.org/wiki/Glutamic_acid \"Glutamic acid\") (E) or [Glutamine](https://en.wikipedia.org/wiki/Glutamine \"Glutamine\") (Q) |\n| X | any |\n| \\* | translation stop |\n| \\- | gap of indeterminate length |\n\nYou'll notice the FASTA format has a well defined structure, and it could be leveraged to build a complete tokenizer, for now though our 4 character (+6 special characters) tokenizer will have to do.\n\n### Why Focus on Coding DNA Sequences (CDS)?\n\nIn the example, we retrieve the **human coding DNA sequences (CDS)**, which represent the DNA sequence of protein-coding regions of genes.\n\nWhile our ultimate goal is to model the entire human genome—and potentially multiple genomes across species or individuals—such tasks require significant computational resources. The tutorials in this book are designed to be accessible, running efficiently on a MacBook or Google Colab. Therefore, we focus on **CDS**, which are highly structured DNA sequences within genes, that directly transcribed into RNA which is in turn translate into proteins. the Table below contains the direct translation from 3 letter DNA sequence to amino-acid (which are the building blocks of proteins).\n\n![The Genetic code to translate codins (3 leter DNA sequences) to amino-acids that are in turn the building blocks of proteins (source: https://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/\\_images/P7_image1.png)](images/paste-3.png){fig-align=\"center\"}\n\nIn contrast, much of the rest of the genome consists of regulatory regions, which are more complex and less structured. CDS sequences provide a strong foundation for a small DNA-based language model because they contain well-defined biological structure, making them a logical starting point before expanding to more complex genomic elements.\n\n## Why Upload DNA Data to Hugging Face?\n\nHugging Face provides a robust ecosystem for hosting and sharing datasets, particularly for machine learning applications. Some key advantages include: - **Easy accessibility**: Researchers and models can easily retrieve datasets. - **Standardized format**: Datasets are structured for seamless integration with deep learning frameworks. - **Direct integration with Hugging Face tools**: The data on the Hugging Face Hub integrates seamlessly with their **Transformers** and **Trainer** Python libraries, making it easy to load datasets and train models. - **Version control and updates**: Data can be refined and expanded over time.\n\nBy storing our dataset on Hugging Face, we enable efficient training and collaboration for DNA language modeling.\n\n## The Script: Downloading and Formatting Human CDS Data\n\nBelow is the R script that downloads human CDS from Ensembl using BioMart, extracts metadata, and saves the processed data into a CSV file. the package we use, `biomartr` isn't the official R package but its a great option! it has very extensive documentation, so if you want to download other sequences in the future make sure to start here: <https://docs.ropensci.org/biomartr/>\n\n``` r\n# Install necessary packages\ninstall.packages(\"biomartr\", dependencies = TRUE)\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n  install.packages(\"BiocManager\")\n}\n\n# Load required libraries\nlibrary(Biostrings)\nlibrary(biomartr)\n\n# Download the human CDS dataset from Ensembl\nHS.cds.ensembl <- getCDS(db = \"ensembl\", organism = \"Homo sapiens\", path = file.path(\"_ncbi_downloads\",\"CDS\"))\n\n# Read CDS data as a Biostrings object\nHuman_CDS <- read_cds(file = HS.cds.ensembl, obj.type = \"Biostrings\")\n\n# Extract headers and sequences\nheaders <- names(Human_CDS)\nsequences <- as.character(Human_CDS)\n\n# Function to extract metadata from headers\nextract_metadata <- function(header) {\n  transcript_id <- sub(\"^>([^ ]+).*\", \"\\\\1\", header)\n  chromosome <- sub(\".*chromosome:([^ ]+).*\", \"\\\\1\", header)\n  start <- sub(\".*chromosome:[^:]+:([^:]+).*\", \"\\\\1\", header)\n  end <- sub(\".*chromosome:[^:]+:[^:]+:([^:]+).*\", \"\\\\1\", header)\n  strand <- sub(\".*chromosome:[^:]+:[^:]+:[^:]+:([^ ]+).*\", \"\\\\1\", header)\n  gene_id <- sub(\".*gene:([^ ]+).*\", \"\\\\1\", header)\n  gene_biotype <- sub(\".*gene_biotype:([^ ]+).*\", \"\\\\1\", header)\n  transcript_biotype <- sub(\".*transcript_biotype:([^ ]+).*\", \"\\\\1\", header)\n  gene_symbol <- sub(\".*gene_symbol:([^ ]+).*\", \"\\\\1\", header)\n  description <- sub(\".*description:(.*)\", \"\\\\1\", header)\n\n  list(\n    transcript_id = transcript_id,\n    chromosome = chromosome,\n    start = start,\n    end = end,\n    strand = strand,\n    gene_id = gene_id,\n    gene_biotype = gene_biotype,\n    transcript_biotype = transcript_biotype,\n    gene_symbol = gene_symbol,\n    description = description\n  )\n}\n\n# Extract metadata from headers\nmetadata_list <- lapply(headers, extract_metadata)\nmetadata_df <- do.call(rbind, lapply(metadata_list, as.data.frame))\nmetadata_df$sequence <- sequences\n\n# Save data to CSV\nwrite.csv(metadata_df, \"genome_sequences.csv\", row.names = FALSE, quote = TRUE)\n\n# Print sample data\nhead(metadata_df)\n```\n\nYou can run the script yourself, but I have also gone ahead and uploaded it to huggingface: <https://huggingface.co/datasets/MichelNivard/Human-genome-CDS-GRCh38>\n\n## Summary\n\nIn this chapter, we: - Introduced **Ensembl** and **BioMart** as tools for retrieving genomic data. - Explained **FASTA files** and **human CDS**, which form the core of our dataset. - Discussed the advantages of **uploading datasets to Hugging Face**, emphasizing its integration with **Transformers** and **Trainer** libraries. - Provided an **R script** to download, process, and store human CDS in a structured format.\n\nIn the next chapter, we will explore **preprocessing techniques like tokenization** and strategies for encoding DNA sequences into a format suitable for training a deep learning language model, and we use Huggingface Transformers and Trainer library to train our first little DNA language model!","srcMarkdownNoYaml":"\n\n::: callout-tip\n## Abstract\n\nIn this chapter we'll learn how to create a high fidelity DNA datasets. Two datasets I have created are available on Huggingface: [the human CDS](https://huggingface.co/datasets/MichelNivard/Human-genome-CDS-GRCh38) and [the CDS of 13 vertebrae species](https://huggingface.co/datasets/MichelNivard/Coding_Sequences_13_Species).\n:::\n\n## Garbage in garbage out\n\nHigh-quality data is essential for training effective machine learning models. In natural language processing (NLP), researchers invest significant effort in curating clean datasets from raw internet text. For example a dataset like 'fineweb-edu' contains English text that is of very high quality. Models trained on fineweb-edu (and similar high quality datasets) will improve MUCH faster then the equivalent model trained on other less carefully processed and evaluated datasets.\n\n![Relative training efficiency using a high quality dataset like fineweb-edu and other massive datasets of English language texts. Image obtained from: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu](https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/QqXOM8h_ZjjhuCv71xmV7.png){fig-align=\"center\"}\n\nThose with experience with **genetics** will know most of what I am about to explain when it comes to the sources of genetic data and the file formats these come in. Those with an **ML** background will be very familiar with Huggingface and the amazing data/model/training integration their libraries offer. Assembing high quality genomics datasets for language modeling requires familiarity with both. When working with DNA, RNA, and protein sequences, the authoritative sources of data are specialized databases and infrastructures rather than data scraped from the internet. If you want to learn to train DNA/RNA/Protein-based language model, we must learn how, and where, to retrieve data and convert it into a structured format.\n\nIn this tutorial, we will begin by collecting coding DNA sequences (CDS) from the human genome using the Ensembl database via BioMart. The goal is to store these sequences in a format suitable for training a DNA-based language model. We will then upload the processed data to [Huggingface](https://huggingface.co), a platform for hosting datasets and models for machine learning tasks.\n\nThis chapter will introduce key biological and computational concepts, ensuring that both biology newcomers and those unfamiliar with language modeling tools can follow along.\n\n## Understanding Ensembl and BioMart\n\nFortunately for us, there is decades of work cleaning up genomic data and we can just go and get it from US government funded websites, where it is deposited by the global scientific community. **Ensembl** is a genomic database that provides extensive annotations for various species, including humans. It offers access to gene sequences, transcript annotations, and protein-coding information. One of its most powerful tools is **BioMart**, a flexible data retrieval system that allows users to download specific genomic datasets easily.\n\nIf we want t work with the data in a language model its efficient to store it in a format that is tailored for machine learning libraries. Unlike text-based NLP datasets, genomic data is structured differently and must be properly formatted before use in a model.\n\n### What Are FASTA Files?\n\nA **FASTA file** is a simple text-based format used for storing biological sequences. Each entry in a FASTA file consists of: 1. A **header line** (starting with `>`), which contains metadata such as gene IDs and chromosome locations. 2. A **sequence** line, which contains the nucleotide or protein sequence.\n\nThere is a very comprehensive Wikipedia entry on the FASTA format.\n\n\"Sequences may be protein sequences or nucleic acid sequences, and they can contain gaps or alignment characters (see sequence alignment). Sequences are expected to be represented in the standard IUB/IUPAC amino acid and nucleic acid codes, with these exceptions: lower-case letters are accepted and are mapped into upper-case; a single hyphen or dash can be used to represent a gap character; and in amino acid sequences, U and \\* are acceptable letters (see below). Numerical digits are not allowed but are used in some databases to indicate the position in the sequence.\" ((source: <https://en.wikipedia.org/wiki/FASTA_format>))\n\n| Nucleic Acid Code | Meaning | Mnemonic |\n|:----------------------:|:----------------------:|:----------------------:|\n| A | A | [**A**denine](https://en.wikipedia.org/wiki/Adenine \"Adenine\") |\n| C | C | [**C**ytosine](https://en.wikipedia.org/wiki/Cytosine \"Cytosine\") |\n| G | G | [**G**uanine](https://en.wikipedia.org/wiki/Guanine \"Guanine\") |\n| T | T | [**T**hymine](https://en.wikipedia.org/wiki/Thymine \"Thymine\") |\n| U | U | [**U**racil](https://en.wikipedia.org/wiki/Uracil \"Uracil\") |\n| \\(i\\) | i | [**i**nosine](https://en.wikipedia.org/wiki/Inosine \"Inosine\") (non-standard) |\n| R | A or G (I) | [pu**R**ine](https://en.wikipedia.org/wiki/Purine \"Purine\") |\n| Y | C, T or U | [p**Y**rimidines](https://en.wikipedia.org/wiki/Pyrimidine \"Pyrimidine\") |\n| K | G, T or U | bases which are [**K**etones](https://en.wikipedia.org/wiki/Ketone \"Ketone\") |\n| M | A or C | bases with [a**M**ino groups](https://en.wikipedia.org/wiki/Amino \"Amino\") |\n| S | C or G | **S**trong interaction |\n| W | A, T or U | **W**eak interaction |\n| B | not A (i.e. C, G, T or U) | **B** comes after A |\n| D | not C (i.e. A, G, T or U) | **D** comes after C |\n| H | not G (i.e., A, C, T or U) | **H** comes after G |\n| V | neither T nor U (i.e. A, C or G) | **V** comes after U |\n| N | A C G T U | **N**ucleic acid |\n| \\- | gap of indeterminate length |  |\n\nThe amino acid codes supported (22 amino acids and 3 special codes) are:\n\n| Amino Acid Code | Meaning |\n|:----------------------------------:|:----------------------------------:|\n| A | [Alanine](https://en.wikipedia.org/wiki/Alanine \"Alanine\") |\n| B | [Aspartic acid](https://en.wikipedia.org/wiki/Aspartic_acid \"Aspartic acid\") (D) or [Asparagine](https://en.wikipedia.org/wiki/Asparagine \"Asparagine\") (N) |\n| C | [Cysteine](https://en.wikipedia.org/wiki/Cysteine \"Cysteine\") |\n| D | [Aspartic acid](https://en.wikipedia.org/wiki/Aspartic_acid \"Aspartic acid\") |\n| E | [Glutamic acid](https://en.wikipedia.org/wiki/Glutamic_acid \"Glutamic acid\") |\n| F | [Phenylalanine](https://en.wikipedia.org/wiki/Phenylalanine \"Phenylalanine\") |\n| G | [Glycine](https://en.wikipedia.org/wiki/Glycine \"Glycine\") |\n| H | [Histidine](https://en.wikipedia.org/wiki/Histidine \"Histidine\") |\n| I | [Isoleucine](https://en.wikipedia.org/wiki/Isoleucine \"Isoleucine\") |\n| J | [Leucine](https://en.wikipedia.org/wiki/Leucine \"Leucine\") (L) or [Isoleucine](https://en.wikipedia.org/wiki/Isoleucine \"Isoleucine\") (I) |\n| K | [Lysine](https://en.wikipedia.org/wiki/Lysine \"Lysine\") |\n| L | [Leucine](https://en.wikipedia.org/wiki/Leucine \"Leucine\") |\n| M | [Methionine](https://en.wikipedia.org/wiki/Methionine \"Methionine\")/[Start codon](https://en.wikipedia.org/wiki/Start_codon \"Start codon\") |\n| N | [Asparagine](https://en.wikipedia.org/wiki/Asparagine \"Asparagine\") |\n| O | [Pyrrolysine](https://en.wikipedia.org/wiki/Pyrrolysine \"Pyrrolysine\") (rare) |\n| P | [Proline](https://en.wikipedia.org/wiki/Proline \"Proline\") |\n| Q | [Glutamine](https://en.wikipedia.org/wiki/Glutamine \"Glutamine\") |\n| R | [Arginine](https://en.wikipedia.org/wiki/Arginine \"Arginine\") |\n| S | [Serine](https://en.wikipedia.org/wiki/Serine \"Serine\") |\n| T | [Threonine](https://en.wikipedia.org/wiki/Threonine \"Threonine\") |\n| U | [Selenocysteine](https://en.wikipedia.org/wiki/Selenocysteine \"Selenocysteine\") (rare) |\n| V | [Valine](https://en.wikipedia.org/wiki/Valine \"Valine\") |\n| W | [Tryptophan](https://en.wikipedia.org/wiki/Tryptophan \"Tryptophan\") |\n| Y | [Tyrosine](https://en.wikipedia.org/wiki/Tyrosine \"Tyrosine\") |\n| Z | [Glutamic acid](https://en.wikipedia.org/wiki/Glutamic_acid \"Glutamic acid\") (E) or [Glutamine](https://en.wikipedia.org/wiki/Glutamine \"Glutamine\") (Q) |\n| X | any |\n| \\* | translation stop |\n| \\- | gap of indeterminate length |\n\nYou'll notice the FASTA format has a well defined structure, and it could be leveraged to build a complete tokenizer, for now though our 4 character (+6 special characters) tokenizer will have to do.\n\n### Why Focus on Coding DNA Sequences (CDS)?\n\nIn the example, we retrieve the **human coding DNA sequences (CDS)**, which represent the DNA sequence of protein-coding regions of genes.\n\nWhile our ultimate goal is to model the entire human genome—and potentially multiple genomes across species or individuals—such tasks require significant computational resources. The tutorials in this book are designed to be accessible, running efficiently on a MacBook or Google Colab. Therefore, we focus on **CDS**, which are highly structured DNA sequences within genes, that directly transcribed into RNA which is in turn translate into proteins. the Table below contains the direct translation from 3 letter DNA sequence to amino-acid (which are the building blocks of proteins).\n\n![The Genetic code to translate codins (3 leter DNA sequences) to amino-acids that are in turn the building blocks of proteins (source: https://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/\\_images/P7_image1.png)](images/paste-3.png){fig-align=\"center\"}\n\nIn contrast, much of the rest of the genome consists of regulatory regions, which are more complex and less structured. CDS sequences provide a strong foundation for a small DNA-based language model because they contain well-defined biological structure, making them a logical starting point before expanding to more complex genomic elements.\n\n## Why Upload DNA Data to Hugging Face?\n\nHugging Face provides a robust ecosystem for hosting and sharing datasets, particularly for machine learning applications. Some key advantages include: - **Easy accessibility**: Researchers and models can easily retrieve datasets. - **Standardized format**: Datasets are structured for seamless integration with deep learning frameworks. - **Direct integration with Hugging Face tools**: The data on the Hugging Face Hub integrates seamlessly with their **Transformers** and **Trainer** Python libraries, making it easy to load datasets and train models. - **Version control and updates**: Data can be refined and expanded over time.\n\nBy storing our dataset on Hugging Face, we enable efficient training and collaboration for DNA language modeling.\n\n## The Script: Downloading and Formatting Human CDS Data\n\nBelow is the R script that downloads human CDS from Ensembl using BioMart, extracts metadata, and saves the processed data into a CSV file. the package we use, `biomartr` isn't the official R package but its a great option! it has very extensive documentation, so if you want to download other sequences in the future make sure to start here: <https://docs.ropensci.org/biomartr/>\n\n``` r\n# Install necessary packages\ninstall.packages(\"biomartr\", dependencies = TRUE)\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n  install.packages(\"BiocManager\")\n}\n\n# Load required libraries\nlibrary(Biostrings)\nlibrary(biomartr)\n\n# Download the human CDS dataset from Ensembl\nHS.cds.ensembl <- getCDS(db = \"ensembl\", organism = \"Homo sapiens\", path = file.path(\"_ncbi_downloads\",\"CDS\"))\n\n# Read CDS data as a Biostrings object\nHuman_CDS <- read_cds(file = HS.cds.ensembl, obj.type = \"Biostrings\")\n\n# Extract headers and sequences\nheaders <- names(Human_CDS)\nsequences <- as.character(Human_CDS)\n\n# Function to extract metadata from headers\nextract_metadata <- function(header) {\n  transcript_id <- sub(\"^>([^ ]+).*\", \"\\\\1\", header)\n  chromosome <- sub(\".*chromosome:([^ ]+).*\", \"\\\\1\", header)\n  start <- sub(\".*chromosome:[^:]+:([^:]+).*\", \"\\\\1\", header)\n  end <- sub(\".*chromosome:[^:]+:[^:]+:([^:]+).*\", \"\\\\1\", header)\n  strand <- sub(\".*chromosome:[^:]+:[^:]+:[^:]+:([^ ]+).*\", \"\\\\1\", header)\n  gene_id <- sub(\".*gene:([^ ]+).*\", \"\\\\1\", header)\n  gene_biotype <- sub(\".*gene_biotype:([^ ]+).*\", \"\\\\1\", header)\n  transcript_biotype <- sub(\".*transcript_biotype:([^ ]+).*\", \"\\\\1\", header)\n  gene_symbol <- sub(\".*gene_symbol:([^ ]+).*\", \"\\\\1\", header)\n  description <- sub(\".*description:(.*)\", \"\\\\1\", header)\n\n  list(\n    transcript_id = transcript_id,\n    chromosome = chromosome,\n    start = start,\n    end = end,\n    strand = strand,\n    gene_id = gene_id,\n    gene_biotype = gene_biotype,\n    transcript_biotype = transcript_biotype,\n    gene_symbol = gene_symbol,\n    description = description\n  )\n}\n\n# Extract metadata from headers\nmetadata_list <- lapply(headers, extract_metadata)\nmetadata_df <- do.call(rbind, lapply(metadata_list, as.data.frame))\nmetadata_df$sequence <- sequences\n\n# Save data to CSV\nwrite.csv(metadata_df, \"genome_sequences.csv\", row.names = FALSE, quote = TRUE)\n\n# Print sample data\nhead(metadata_df)\n```\n\nYou can run the script yourself, but I have also gone ahead and uploaded it to huggingface: <https://huggingface.co/datasets/MichelNivard/Human-genome-CDS-GRCh38>\n\n## Summary\n\nIn this chapter, we: - Introduced **Ensembl** and **BioMart** as tools for retrieving genomic data. - Explained **FASTA files** and **human CDS**, which form the core of our dataset. - Discussed the advantages of **uploading datasets to Hugging Face**, emphasizing its integration with **Transformers** and **Trainer** libraries. - Provided an **R script** to download, process, and store human CDS in a structured format.\n\nIn the next chapter, we will explore **preprocessing techniques like tokenization** and strategies for encoding DNA sequences into a format suitable for training a deep learning language model, and we use Huggingface Transformers and Trainer library to train our first little DNA language model!"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"Chapter1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.40","bibliography":["references.bib"],"theme":["cosmo","style.scss"],"title":"Preparing DNA data for training"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}