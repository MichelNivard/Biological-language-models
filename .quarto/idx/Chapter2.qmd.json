{"title":"Training our first DNA Language Model: BERT","markdown":{"yaml":{"title":"Training our first DNA Language Model: BERT","format":"html"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n\nNow that we have collected and formatted a DNA dataset in Chapter 1, we can proceed to train a language model using BERT. In this chapter, we will walk through the process of tokenizing DNA sequences, configuring a BERT model, and training it using the **Masked Language Modeling (MLM)** objective.\n\nWe will cover the key concepts behind tokenization, BERT, and MLM before diving into the Python script.\n\n## Understanding Tokenization\n\n### What is a Tokenizer?\n\nA **tokenizer** is a fundamental component of any language model. Language models are essentially large multinomial models that predict the next token in a sequence based on previous tokens, or predict a masked token using the surrounding tokens. Since machine learning models operate on numerical representations, tokenized text must be converted into integers, which serve as indices for lookup in a vocabulary.\n\nThese integers, however, **have no inherent numeric value**â€”they simply act as categorical labels that the model learns to associate with semantic meaning. For example, if we tokenize the sentence:\n\n> \"The quick brown fox jumps over the lazy dog\"\n\nat the **word level**, we might obtain a numerical sequence like:\n\n> `[4, 123, 678, 89, 245, 983, 56, 4564]`\n\nwhere each number corresponds to a word based on a pre-defined tokenization dictionary, such as:\n\n```         \n{\"the\": 4, \"quick\": 123, \"brown\": 678, \"fox\": 89, \"jumps\": 245, \"over\": 983, \"lazy\": 56, \"dog\": 4564}\n```\n\nSimilarly, for DNA sequences, each nucleotide (A, T, C, G) is assigned a unique integer ID.\n\n### Our DNA Tokenizer\n\nOur tokenizer uses a **character-level** approach, where each nucleotide is assigned a unique integer ID. Special tokens are also included for various purposes:\n\n-   `[UNK]` (unknown token)\n-   `[PAD]` (padding token for equal-length sequences)\n-   `[CLS]` (classification token, useful for downstream tasks)\n-   `[SEP]` (separator token, used in tasks like sequence-pair classification)\n-   `[MASK]` (used for masked language modeling training)\n\n**Python Code:**\n\n``` python\nimport torch\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.pre_tokenizers import Split\nfrom transformers import PreTrainedTokenizerFast\n\n# --------------------------------\n# 1. DNA Tokenizer with Full FASTA Nucleic Acid Code\n# --------------------------------\n\n# Define vocabulary to include all FASTA nucleotides and symbols\ndna_vocab = {\n    \"A\": 0, \"T\": 1, \"C\": 2, \"G\": 3, \"N\": 4, \"U\": 5, \"i\": 6,  # Standard bases + Inosine\n    \"R\": 7, \"Y\": 8, \"K\": 9, \"M\": 10, \"S\": 11, \"W\": 12,  # Ambiguous bases\n    \"B\": 13, \"D\": 14, \"H\": 15, \"V\": 16,  # More ambiguity codes\n    \"-\": 17,  # Gap character\n    \"[UNK]\": 18, \"[PAD]\": 19, \"[CLS]\": 20, \"[SEP]\": 21, \"[MASK]\": 22\n}\n\n# Create tokenizer\ntokenizer = Tokenizer(WordLevel(vocab=dna_vocab, unk_token=\"[UNK]\"))\ntokenizer.pre_tokenizer = Split(\"\", \"isolated\")  # Character-level splitting\n\n# Convert to Hugging Face-compatible tokenizer\nhf_tokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\"\n)\n```\n\n### Other Tokenization Strategies for DNA, RNA, and Proteins\n\nWhile character-level tokenization is effective, other tokenization approaches can offer different tradeoffs:\n\n#### Byte Pair Encoding (BPE)\n\nBPE is widely used in NLP and allows frequently occurring patterns to be merged into subword units. While BPE could be useful for repetitive genome sequences, it may not capture biologically meaningful units.\n\n#### K-mer Tokenization\n\nK-mer tokenization groups nucleotides into fixed-length substrings (e.g., 3-mers like \"ATG\"). This approach retains local sequence structure but can lead to a large vocabulary size.\n\n#### Tiktoken and Similar Models\n\nSome modern tokenization methods, such as Tiktoken, optimize speed and efficiency by precomputing merges. These are often optimized for large-scale NLP tasks but could be adapted for biological sequences.\n\nChoosing the best tokenizer depends on the specific modeling task. For example, RNA secondary structures or protein folding models might benefit from more complex tokenization strategies.\n\nSource: [RPubs Tokenization Review](https://rpubs.com/yuchenz585/1161578)\n\n## Loading and Tokenizing the DNA Dataset\n\n### Understanding the Dataset\n\nWe will use a pre-existing dataset, **Human-genome-CDS-GRCh38**, which contains coding sequences from the human genome.\n\n### Tokenizing the Dataset\n\nTo prepare the dataset for training, we must **apply the tokenizer to each sequence** while ensuring:\n\n-   Sequences are truncated or padded to a fixed length (512 tokens)\n-   Unwanted columns are removed\n\n**Python Code:**\n\n``` python\nfrom datasets import load_dataset\n\ndataset_name = \"MichelNivard/Human-genome-CDS-GRCh38\"\ndataset = load_dataset(dataset_name)\n\ncolumn_name = \"sequence\"\n\ndef tokenize_function(examples):\n    return hf_tokenizer(examples[column_name], truncation=True, padding=\"max_length\", max_length=512)\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[column_name])\n```\n\n### Saving and Preparing the Dataset for Training\n\nOnce tokenized, we save the dataset for efficient access during training.\n\n**Python Code:**\n\n``` python\ntokenized_dataset.save_to_disk(\"tokenized_dna_dataset\")\n```\n\n## Understanding BERT and Masked Language Modeling (MLM)\n\n### What is BERT?\n\nBERT (**Bidirectional Encoder Representations from Transformers**) is a powerful transformer-based language model. Unlike traditional left-to-right models, BERT **learns bidirectional context**, allowing it to understand sequences more effectively.\n\nReturning to our earlier example sentence:\n\n> \"The quick brown fox jumps over the lazy dog\"\n\nBERT does not process words one at a time but instead considers the entire sequence simultaneously. This bidirectional approach allows BERT to infer missing words based on context from both directions.\n\n### What is Masked Language Modeling (MLM)?\n\nMLM is a self-supervised learning objective where the model learns by predicting missing tokens in a sequence. During training:\n\n-   **Some tokens are randomly replaced with `[MASK]`**\n-   The model must **predict the original token** based on surrounding context\n\nFor example, if we mask the word \"fox\" in our sentence:\n\n> \"The quick brown \\[MASK\\] jumps over the lazy dog\"\n\nBERT will analyze the remaining words and attempt to predict \"fox.\"\n\nThis technique enables BERT to **learn useful representations** without requiring labeled data.\n\n### Understanding Transformer Layers, Attention Heads, and Hidden Size\n\nA **transformer layer** consists of self-attention and feed-forward layers that help the model learn relationships between tokens. The number of **transformer layers** determines how deep the model is.\n\nAn **attention head** is a component of the self-attention mechanism that learns different types of relationships within the data. Having **multiple attention heads** allows the model to capture various dependencies between tokens.\n\nReturning to our example:\n\n-   One attention head might focus on subject-verb relationships, recognizing that \"fox\" is the subject of \"jumps.\"\n-   Another head might capture adjective-noun relationships, linking \"brown\" to \"fox.\"\n\nThe **hidden size** defines the dimensionality of the modelâ€™s internal representations. A larger hidden size enables the model to capture more complex patterns, but also increases computational cost.\n\nBy stacking multiple transformer layers and attention heads, BERT gains a deep understanding of sentence structures.\n\n## Defining the BERT Model for DNA Sequences\n\nWhile the \"quick brown fox\" example helps us understand how BERT processes natural language, our goal is to apply the same principles toÂ **DNA sequences**. Instead of predicting missing words in a sentence, we want our model to learnÂ **biological patterns**Â andÂ **genomic structure**Â by predicting masked nucleotides within DNA sequences.\n\nInÂ **DNA modeling**, understanding sequence context is just as critical as in language modeling. Just as BERT learns that \"fox\" fits within a given sentence structure, our model should learn thatÂ **specific nucleotide sequences appear in biologically meaningful patterns**. This could involve recognizingÂ **gene coding regions, regulatory motifs, or conserved sequence elements**Â across different genomes.\n\nTo accomplish this, we define aÂ **custom BERT model**Â designed specifically for processing DNA sequences. Unlike traditional text-based models, our DNA BERT model uses aÂ **character-level vocabulary**Â of nucleotides (A, T, C, G) and special tokens to represent sequence structure. By leveragingÂ **masked language modeling (MLM)**, the model will learn to predict missing nucleotides based on surrounding context, allowing it to capture meaningful genomic features.\n\nWith this in mind, let's move forward and define our BERT architecture for DNA sequences.\n\n**Python Code:**\n\n``` python\nfrom transformers import ModernBertConfig, ModernBertForMaskedLM\n\nconfig = ModernBertConfig(\n    vocab_size=len(dna_vocab),\n    hidden_size=256,\n    num_hidden_layers=8,\n    num_attention_heads=8,\n    intermediate_size=512,\n    max_position_embeddings=512,\n    type_vocab_size=1,\n)\nconfig.pad_token_id = dna_vocab[\"[PAD]\"]\nmodel = ModernBertForMaskedLM(config)\n```\n\n### Configuring Training for DNA BERT\n\nNow that we have defined our BERT model for DNA sequences, we need to set up the **training process**. This involves specifying various **training hyperparameters**, handling **masked language modeling (MLM)** data, and preparing for efficient learning.\n\nUnlike traditional NLP tasks where models are trained on massive text corpora, training on DNA sequences presents unique challenges. The structure of genomic data, sequence length, and biological patterns all influence how the model learns. Our configuration ensures that the training process is efficient while maintaining biological relevance.\n\n------------------------------------------------------------------------\n\n### Setting Training Parameters\n\nTo train our DNA BERT model, we use the **Hugging Face `TrainingArguments` class**, which allows us to define key training settings. These include:\n\n-   **Batch size:** We set a batch size of `16` for both training and evaluation. This determines how many sequences are processed at once.\n-   **Logging & Saving:** We log loss every `50` steps and save model checkpoints every `100` steps to monitor training progress.\n-   **Learning Rate:** We use a learning rate of `5e-5`, a common choice for transformer models that balances learning speed and stability.\n-   **Weight Decay:** A value of `0.01` is used to prevent overfitting by applying **L2 regularization** to model weights.\n-   **Training Steps:** The model is trained for `4000` steps. This ensures sufficient learning without excessive computation.\n-   **Model Saving:** The model checkpoints are stored in `./bert-dna`, allowing us to resume training if needed.\n\n**Python Code:**\n\n``` python\ntraining_args = TrainingArguments(\n    output_dir=\"./bert-dna\",\n    overwrite_output_dir=True,\n    logging_steps=50,  # Log loss every step\n    save_steps=100,\n    save_total_limit=2,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    max_steps=4000,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    push_to_hub=False,\n    report_to=\"none\",  # Disables wandb logging\n)\n```\n\n------------------------------------------------------------------------\n\n### Preparing for Masked Language Modeling (MLM)\n\nSince we are training our DNA BERT model using **masked language modeling (MLM)**, we need to handle **masked tokens** properly. This is done using the **`DataCollatorForLanguageModeling`**, which:\n\n-   **Randomly masks nucleotides** in the training sequences.\n-   **Creates `labels` automatically**, meaning the model learns by trying to predict these masked tokens.\n-   **Uses a masking probability of 5%**, ensuring that a small but meaningful portion of the sequence is masked during training.\n\nBy applying MLM, we allow the model to **generalize nucleotide relationships** and capture **sequence dependencies**, just like how BERT learns relationships between words in text.\n\n**Python Code:**\n\n``` python\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=hf_tokenizer,\n    mlm=True,\n    mlm_probability=0.05\n)\n```\n\n------------------------------------------------------------------------\n\n### Training the DNA BERT Model\n\nWith our configuration and data collator in place, we now **train the model**. We use the **Hugging Face `Trainer` API**, which simplifies the training process by handling:\n\n-   **Dataset iteration:** Automatically loads and batches training sequences.\n-   **Gradient updates:** Adjusts model weights based on training loss.\n-   **Logging & saving:** Tracks training progress and stores checkpoints.\n\nOnce training begins, the model will gradually **learn nucleotide dependencies** and improve its ability to predict missing DNA bases.\n\n**Python Code:**\n\n``` python\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=hf_tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()\n```\n\nyou set up free wandb logging (go to <https://wandb.ai/site> for more info) you can track your training runs online, wherever they are running. You then get a dashboard full of pretty loss vs progress plots like the one below which I screencapped about Â± 30 minutes into training on my macbook.\n\n![](images/paste-4.png)\n\n### Saving the Trained Model\n\nAfter training completes, we save both the **model** and **tokenizer** so they can be used for future predictions or fine-tuning.\n\n-   The **model weights** are stored in `./bert-dna`, allowing us to reload the trained model.\n-   The **tokenizer** is also saved, ensuring that input sequences can be processed the same way during inference.\n\nFinally, a success message is printed, confirming that the training process has been completed.\n\n**Python Code:**\n\n``` python\n# Save the final model and tokenizer\ntrainer.save_model(\"./bert-dna\")\nhf_tokenizer.save_pretrained(\"./bert-dna\")\n\nprint(\"ðŸŽ‰ Training complete! Model saved to ./bert-dna\")\n```\n\n### Summary\n\nIn this section, we:\n\n-   Defined **training hyperparameters** such as batch size, learning rate, and training steps.\n-   Used **masked language modeling (MLM)** to train the model on DNA sequences.\n-   Leveraged the **Hugging Face `Trainer` API** to automate model training.\n-   Saved the **final trained model and tokenizer** for future use.\n\nWith this trained model, we can now **fine-tune or apply it to various genomic tasks**, such as predicting genetic variations or classifying functional DNA sequences. In the next chapter, we will explore **how to fine-tune our DNA BERT model for specific applications**.\n\n","srcMarkdownNoYaml":"\n\n\n## Introduction\n\nNow that we have collected and formatted a DNA dataset in Chapter 1, we can proceed to train a language model using BERT. In this chapter, we will walk through the process of tokenizing DNA sequences, configuring a BERT model, and training it using the **Masked Language Modeling (MLM)** objective.\n\nWe will cover the key concepts behind tokenization, BERT, and MLM before diving into the Python script.\n\n## Understanding Tokenization\n\n### What is a Tokenizer?\n\nA **tokenizer** is a fundamental component of any language model. Language models are essentially large multinomial models that predict the next token in a sequence based on previous tokens, or predict a masked token using the surrounding tokens. Since machine learning models operate on numerical representations, tokenized text must be converted into integers, which serve as indices for lookup in a vocabulary.\n\nThese integers, however, **have no inherent numeric value**â€”they simply act as categorical labels that the model learns to associate with semantic meaning. For example, if we tokenize the sentence:\n\n> \"The quick brown fox jumps over the lazy dog\"\n\nat the **word level**, we might obtain a numerical sequence like:\n\n> `[4, 123, 678, 89, 245, 983, 56, 4564]`\n\nwhere each number corresponds to a word based on a pre-defined tokenization dictionary, such as:\n\n```         \n{\"the\": 4, \"quick\": 123, \"brown\": 678, \"fox\": 89, \"jumps\": 245, \"over\": 983, \"lazy\": 56, \"dog\": 4564}\n```\n\nSimilarly, for DNA sequences, each nucleotide (A, T, C, G) is assigned a unique integer ID.\n\n### Our DNA Tokenizer\n\nOur tokenizer uses a **character-level** approach, where each nucleotide is assigned a unique integer ID. Special tokens are also included for various purposes:\n\n-   `[UNK]` (unknown token)\n-   `[PAD]` (padding token for equal-length sequences)\n-   `[CLS]` (classification token, useful for downstream tasks)\n-   `[SEP]` (separator token, used in tasks like sequence-pair classification)\n-   `[MASK]` (used for masked language modeling training)\n\n**Python Code:**\n\n``` python\nimport torch\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.pre_tokenizers import Split\nfrom transformers import PreTrainedTokenizerFast\n\n# --------------------------------\n# 1. DNA Tokenizer with Full FASTA Nucleic Acid Code\n# --------------------------------\n\n# Define vocabulary to include all FASTA nucleotides and symbols\ndna_vocab = {\n    \"A\": 0, \"T\": 1, \"C\": 2, \"G\": 3, \"N\": 4, \"U\": 5, \"i\": 6,  # Standard bases + Inosine\n    \"R\": 7, \"Y\": 8, \"K\": 9, \"M\": 10, \"S\": 11, \"W\": 12,  # Ambiguous bases\n    \"B\": 13, \"D\": 14, \"H\": 15, \"V\": 16,  # More ambiguity codes\n    \"-\": 17,  # Gap character\n    \"[UNK]\": 18, \"[PAD]\": 19, \"[CLS]\": 20, \"[SEP]\": 21, \"[MASK]\": 22\n}\n\n# Create tokenizer\ntokenizer = Tokenizer(WordLevel(vocab=dna_vocab, unk_token=\"[UNK]\"))\ntokenizer.pre_tokenizer = Split(\"\", \"isolated\")  # Character-level splitting\n\n# Convert to Hugging Face-compatible tokenizer\nhf_tokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\"\n)\n```\n\n### Other Tokenization Strategies for DNA, RNA, and Proteins\n\nWhile character-level tokenization is effective, other tokenization approaches can offer different tradeoffs:\n\n#### Byte Pair Encoding (BPE)\n\nBPE is widely used in NLP and allows frequently occurring patterns to be merged into subword units. While BPE could be useful for repetitive genome sequences, it may not capture biologically meaningful units.\n\n#### K-mer Tokenization\n\nK-mer tokenization groups nucleotides into fixed-length substrings (e.g., 3-mers like \"ATG\"). This approach retains local sequence structure but can lead to a large vocabulary size.\n\n#### Tiktoken and Similar Models\n\nSome modern tokenization methods, such as Tiktoken, optimize speed and efficiency by precomputing merges. These are often optimized for large-scale NLP tasks but could be adapted for biological sequences.\n\nChoosing the best tokenizer depends on the specific modeling task. For example, RNA secondary structures or protein folding models might benefit from more complex tokenization strategies.\n\nSource: [RPubs Tokenization Review](https://rpubs.com/yuchenz585/1161578)\n\n## Loading and Tokenizing the DNA Dataset\n\n### Understanding the Dataset\n\nWe will use a pre-existing dataset, **Human-genome-CDS-GRCh38**, which contains coding sequences from the human genome.\n\n### Tokenizing the Dataset\n\nTo prepare the dataset for training, we must **apply the tokenizer to each sequence** while ensuring:\n\n-   Sequences are truncated or padded to a fixed length (512 tokens)\n-   Unwanted columns are removed\n\n**Python Code:**\n\n``` python\nfrom datasets import load_dataset\n\ndataset_name = \"MichelNivard/Human-genome-CDS-GRCh38\"\ndataset = load_dataset(dataset_name)\n\ncolumn_name = \"sequence\"\n\ndef tokenize_function(examples):\n    return hf_tokenizer(examples[column_name], truncation=True, padding=\"max_length\", max_length=512)\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[column_name])\n```\n\n### Saving and Preparing the Dataset for Training\n\nOnce tokenized, we save the dataset for efficient access during training.\n\n**Python Code:**\n\n``` python\ntokenized_dataset.save_to_disk(\"tokenized_dna_dataset\")\n```\n\n## Understanding BERT and Masked Language Modeling (MLM)\n\n### What is BERT?\n\nBERT (**Bidirectional Encoder Representations from Transformers**) is a powerful transformer-based language model. Unlike traditional left-to-right models, BERT **learns bidirectional context**, allowing it to understand sequences more effectively.\n\nReturning to our earlier example sentence:\n\n> \"The quick brown fox jumps over the lazy dog\"\n\nBERT does not process words one at a time but instead considers the entire sequence simultaneously. This bidirectional approach allows BERT to infer missing words based on context from both directions.\n\n### What is Masked Language Modeling (MLM)?\n\nMLM is a self-supervised learning objective where the model learns by predicting missing tokens in a sequence. During training:\n\n-   **Some tokens are randomly replaced with `[MASK]`**\n-   The model must **predict the original token** based on surrounding context\n\nFor example, if we mask the word \"fox\" in our sentence:\n\n> \"The quick brown \\[MASK\\] jumps over the lazy dog\"\n\nBERT will analyze the remaining words and attempt to predict \"fox.\"\n\nThis technique enables BERT to **learn useful representations** without requiring labeled data.\n\n### Understanding Transformer Layers, Attention Heads, and Hidden Size\n\nA **transformer layer** consists of self-attention and feed-forward layers that help the model learn relationships between tokens. The number of **transformer layers** determines how deep the model is.\n\nAn **attention head** is a component of the self-attention mechanism that learns different types of relationships within the data. Having **multiple attention heads** allows the model to capture various dependencies between tokens.\n\nReturning to our example:\n\n-   One attention head might focus on subject-verb relationships, recognizing that \"fox\" is the subject of \"jumps.\"\n-   Another head might capture adjective-noun relationships, linking \"brown\" to \"fox.\"\n\nThe **hidden size** defines the dimensionality of the modelâ€™s internal representations. A larger hidden size enables the model to capture more complex patterns, but also increases computational cost.\n\nBy stacking multiple transformer layers and attention heads, BERT gains a deep understanding of sentence structures.\n\n## Defining the BERT Model for DNA Sequences\n\nWhile the \"quick brown fox\" example helps us understand how BERT processes natural language, our goal is to apply the same principles toÂ **DNA sequences**. Instead of predicting missing words in a sentence, we want our model to learnÂ **biological patterns**Â andÂ **genomic structure**Â by predicting masked nucleotides within DNA sequences.\n\nInÂ **DNA modeling**, understanding sequence context is just as critical as in language modeling. Just as BERT learns that \"fox\" fits within a given sentence structure, our model should learn thatÂ **specific nucleotide sequences appear in biologically meaningful patterns**. This could involve recognizingÂ **gene coding regions, regulatory motifs, or conserved sequence elements**Â across different genomes.\n\nTo accomplish this, we define aÂ **custom BERT model**Â designed specifically for processing DNA sequences. Unlike traditional text-based models, our DNA BERT model uses aÂ **character-level vocabulary**Â of nucleotides (A, T, C, G) and special tokens to represent sequence structure. By leveragingÂ **masked language modeling (MLM)**, the model will learn to predict missing nucleotides based on surrounding context, allowing it to capture meaningful genomic features.\n\nWith this in mind, let's move forward and define our BERT architecture for DNA sequences.\n\n**Python Code:**\n\n``` python\nfrom transformers import ModernBertConfig, ModernBertForMaskedLM\n\nconfig = ModernBertConfig(\n    vocab_size=len(dna_vocab),\n    hidden_size=256,\n    num_hidden_layers=8,\n    num_attention_heads=8,\n    intermediate_size=512,\n    max_position_embeddings=512,\n    type_vocab_size=1,\n)\nconfig.pad_token_id = dna_vocab[\"[PAD]\"]\nmodel = ModernBertForMaskedLM(config)\n```\n\n### Configuring Training for DNA BERT\n\nNow that we have defined our BERT model for DNA sequences, we need to set up the **training process**. This involves specifying various **training hyperparameters**, handling **masked language modeling (MLM)** data, and preparing for efficient learning.\n\nUnlike traditional NLP tasks where models are trained on massive text corpora, training on DNA sequences presents unique challenges. The structure of genomic data, sequence length, and biological patterns all influence how the model learns. Our configuration ensures that the training process is efficient while maintaining biological relevance.\n\n------------------------------------------------------------------------\n\n### Setting Training Parameters\n\nTo train our DNA BERT model, we use the **Hugging Face `TrainingArguments` class**, which allows us to define key training settings. These include:\n\n-   **Batch size:** We set a batch size of `16` for both training and evaluation. This determines how many sequences are processed at once.\n-   **Logging & Saving:** We log loss every `50` steps and save model checkpoints every `100` steps to monitor training progress.\n-   **Learning Rate:** We use a learning rate of `5e-5`, a common choice for transformer models that balances learning speed and stability.\n-   **Weight Decay:** A value of `0.01` is used to prevent overfitting by applying **L2 regularization** to model weights.\n-   **Training Steps:** The model is trained for `4000` steps. This ensures sufficient learning without excessive computation.\n-   **Model Saving:** The model checkpoints are stored in `./bert-dna`, allowing us to resume training if needed.\n\n**Python Code:**\n\n``` python\ntraining_args = TrainingArguments(\n    output_dir=\"./bert-dna\",\n    overwrite_output_dir=True,\n    logging_steps=50,  # Log loss every step\n    save_steps=100,\n    save_total_limit=2,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    max_steps=4000,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    push_to_hub=False,\n    report_to=\"none\",  # Disables wandb logging\n)\n```\n\n------------------------------------------------------------------------\n\n### Preparing for Masked Language Modeling (MLM)\n\nSince we are training our DNA BERT model using **masked language modeling (MLM)**, we need to handle **masked tokens** properly. This is done using the **`DataCollatorForLanguageModeling`**, which:\n\n-   **Randomly masks nucleotides** in the training sequences.\n-   **Creates `labels` automatically**, meaning the model learns by trying to predict these masked tokens.\n-   **Uses a masking probability of 5%**, ensuring that a small but meaningful portion of the sequence is masked during training.\n\nBy applying MLM, we allow the model to **generalize nucleotide relationships** and capture **sequence dependencies**, just like how BERT learns relationships between words in text.\n\n**Python Code:**\n\n``` python\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=hf_tokenizer,\n    mlm=True,\n    mlm_probability=0.05\n)\n```\n\n------------------------------------------------------------------------\n\n### Training the DNA BERT Model\n\nWith our configuration and data collator in place, we now **train the model**. We use the **Hugging Face `Trainer` API**, which simplifies the training process by handling:\n\n-   **Dataset iteration:** Automatically loads and batches training sequences.\n-   **Gradient updates:** Adjusts model weights based on training loss.\n-   **Logging & saving:** Tracks training progress and stores checkpoints.\n\nOnce training begins, the model will gradually **learn nucleotide dependencies** and improve its ability to predict missing DNA bases.\n\n**Python Code:**\n\n``` python\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=hf_tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()\n```\n\nyou set up free wandb logging (go to <https://wandb.ai/site> for more info) you can track your training runs online, wherever they are running. You then get a dashboard full of pretty loss vs progress plots like the one below which I screencapped about Â± 30 minutes into training on my macbook.\n\n![](images/paste-4.png)\n\n### Saving the Trained Model\n\nAfter training completes, we save both the **model** and **tokenizer** so they can be used for future predictions or fine-tuning.\n\n-   The **model weights** are stored in `./bert-dna`, allowing us to reload the trained model.\n-   The **tokenizer** is also saved, ensuring that input sequences can be processed the same way during inference.\n\nFinally, a success message is printed, confirming that the training process has been completed.\n\n**Python Code:**\n\n``` python\n# Save the final model and tokenizer\ntrainer.save_model(\"./bert-dna\")\nhf_tokenizer.save_pretrained(\"./bert-dna\")\n\nprint(\"ðŸŽ‰ Training complete! Model saved to ./bert-dna\")\n```\n\n### Summary\n\nIn this section, we:\n\n-   Defined **training hyperparameters** such as batch size, learning rate, and training steps.\n-   Used **masked language modeling (MLM)** to train the model on DNA sequences.\n-   Leveraged the **Hugging Face `Trainer` API** to automate model training.\n-   Saved the **final trained model and tokenizer** for future use.\n\nWith this trained model, we can now **fine-tune or apply it to various genomic tasks**, such as predicting genetic variations or classifying functional DNA sequences. In the next chapter, we will explore **how to fine-tune our DNA BERT model for specific applications**.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"Chapter2.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.40","bibliography":["references.bib"],"theme":["cosmo","brand"],"title":"Training our first DNA Language Model: BERT"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}