---
title: "Weaving Together Models"
format: html
---

::: callout-tip
## Abstract

In Chapter 5, we will put these two models — Vanilla BERT and GPN-MSA-BERT — to the test. We will evaluate their performance on:

-   Predicting masked bases (MLM accuracy).
-   Predicting the functional impact of mutations.

This head-to-head comparison will highlight the strengths and weaknesses of each approach. Finally **we'll build a hybrid model**, in which the DNA sequence is model is trained in evolutionary context and in its sequence context and the results are blended, a model architecture that is a little like alphafold, the Nobel winning protein language model developed at google deepmind.

All scripts for this chapter are found here: <https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/DNA/Chapter_5>
:::

So we have trained two models. Admittedly with very different architectures, and goals. We can now go can compare these models (though It remains to be seen whether we can do so fairly). The comparisons we'll make first is whether the two model accurately predict the human base. This is an entirely unfair comparison, the deck is stacked massively towards the GPN Bert model we trained in Chapter 4, as for that model we hard code evolutionary history in the embedding, and when masking we only mask the human base. So we'll do a few things to level the playing field. First we trained the GPN model with access to only 12 out of 100 auxiliary species. As you might recall the DNABert model developed and trained in Chapter 2, a version of which is available on huggingface ([MichelNivard/DNABert-CDS-13Species-v0.1](https://huggingface.co/MichelNivard/DNABert-CDS-13Species-v0.1)) was trained on Human coding sequences and those of 12 further species. At a very minimum the two models saw approximately the same amount of genomics content during training (though the content is used in different ways). Then we also evaluate the GPN model while masking the aux sequences (by setting all aux species bases for the focal base to "-"). This scenario is comparable to inferences on a patients genome where the patient has a genomic feature (a small inversion, insertion, deletion or duplication etc ) that doesnt align to other species give it is novel, but we still want to infer the potential deleteriousness. **Our first hypotheses is** that without the help of the auxiliary sequences the GPN model's accuracy takes a dive. **Our second hypothesis i**s that for bases where the human base differs from the most frequently observed ancestral base, the GPN model accuracy will take a dive, but DNABert might not. One of these hypothesis will proof true...

## Evaluating Model accuracy (code)

Below is a minimal code example where we 1. load the two models and write a helper function (`get_predictions`) that evaluates the likelihood of a given base in both models. This function can the repetitive be apply to generate all kinds of comparisons. the full; (and somewhat verbose) code for those evaluations is found in the script`Chapter5_competative_eval.py` available on GitHub: <https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/DNA/Chapter_5>

The important skill we learn here is to laod two models, both trained on slightly different datasets, with slightly different DNA tokenizers, and apply them both to a single dataset, so we csn make a direct comparison.

``` python
# --------------------------------
# 1. Competative evaluations!!
# --------------------------------

# Load GPN-enhanced ModernBERT (your custom model)
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

model_gpn = torch.load("./bert-dna-gpn/gpn_bert_model.pt") # Assuming it's already trained and loaded
tokenizer_gpn = AutoTokenizer.from_pretrained("./bert-dna-gpn")
# Load the full model
model_gpn.eval()

# Load DNABert-CDS-13Species
model_name_dnabert = "MichelNivard/DNABert-CDS-13Species-v0.1"
tokenizer_dnabert = AutoTokenizer.from_pretrained(model_name_dnabert)
model_dnabert = AutoModelForMaskedLM.from_pretrained(model_name_dnabert).to(device)
model_dnabert.eval()

# Helper to get vocab mapping
id_to_token_gpn = {v: k for k, v in tokenizer_gpn.get_vocab().items()}
id_to_token_dnabert = {v: k for k, v in tokenizer_dnabert.get_vocab().items()}


# Helper function to get predictions of the same base for both models it requires all the modle specific elements, tthe models themselves, attention masks, tokenizers, tokenized inputs, for both models.

def get_predictions(pos, input_ids_gpn, attention_mask_gpn, aux_features,
                   input_ids_dnabert, attention_mask_dnabert,
                   model_gpn, model_dnabert, tokenizer_gpn, tokenizer_dnabert,
                   device):
    """Helper function to get predictions from both models for a specific position"""
    
    # Mask the position in both models
    masked_input_ids_gpn = input_ids_gpn.clone()
    masked_input_ids_gpn[0, pos] = tokenizer_gpn.mask_token_id
    
    masked_input_ids_dnabert = input_ids_dnabert.clone()
    masked_input_ids_dnabert[0, pos] = tokenizer_dnabert.mask_token_id
    
    # Get predictions from GPN
    with torch.no_grad():
        output_gpn = model_gpn(
            input_ids=masked_input_ids_gpn,
            attention_mask=attention_mask_gpn,
            aux_features=aux_features
        )
        logits_gpn = output_gpn.logits
        log_probs_gpn = torch.log_softmax(logits_gpn[0, pos], dim=-1)
        
        # Get predictions from DNABERT
        output_dnabert = model_dnabert(
            masked_input_ids_dnabert,
            attention_mask=attention_mask_dnabert
        )
        logits_dnabert = output_dnabert.logits
        log_probs_dnabert = torch.log_softmax(logits_dnabert[0, pos], dim=-1)
    
    # Get top predictions
    top_preds_gpn = torch.topk(log_probs_gpn, k=4)
    top_preds_dnabert = torch.topk(log_probs_dnabert, k=4)
    
    return {
        'gpn_probs': top_preds_gpn,
        'dnabert_probs': top_preds_dnabert
    }
```

## Evaluating Model accuracy (results)

When evaluating up to 500 bases (first 500) for the first 30 genes, and all bases where the human base doesn't match the most frequently observed ancestral base across 100 species we can generate model accuracy stats. Here we use the mean \# bases correctly called by the model (where called means the model assigns the true human references base the highest probability when predicting it.). As expected the GPN based Bert model we trained, which has access to the evolutionary history of the base, blows vanilla DNA Bert out of the water. This isn't unexpected though, very large portions of the coding sequences are conserved across species, and training on human species and 12 allingned species, where those other species are available at inferences time just makes prediction VERY easy. However as per out first hypothesis the GPN model doesn not do well if we mask the bases evolutionary history. In those cases its bested by DNA Bert, which is trained to predict sequences without explicit evolutionary history and so does do a reasonable job at it.

``` python
# 1. Overall Accuracy Bar Plot
plt.figure(figsize=(10, 6))
accuracies = {
    'GPN (normal)': df['gpn_normal_correct'].mean(),
    'DNABERT': df['dnabert_normal_correct'].mean(),
    'GPN (no species)': df['gpn_missing_correct'].mean()
}
plt.bar(accuracies.keys(), accuracies.values(), color=['blue', 'green', 'red'])
plt.title('Model Accuracy Comparison')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

![**Figure 1**: Model prediction accuracy compared across a common set of coding sequences of 30 genes (up to ±15k bases). GPN (no species) are predictions by the GPN moel but with other species base masked (changed to "-" , the missing/sequences gap token) during prediction. ](images/paste-20.png)

I had expected that if considering bases where the human references base differs fro the majority of the other species bases, and the base is highly conserved (\> 75%) then GPN models might be thrown off, and perhaps DNA Bert would not? It appears though (See **Figure 2**) that all models drop in accuracy, but DNA BErt drops more than the GPN model! So are second hypothesis wasn't confirmed. Now this is a textbook like document, these models are under trained, we only evaluated the first 500 bases in 30 genes, within them there are only a few hundred bases where the human and dominant ancestral base differ, so please don't generalize any of these concussions!

![**Figure 2:** The model prediction accuracy for bases where the human base differs from the base most frequently observed in other species.](images/paste-21.png)

Finally we can plot model accuracy as a function of "conservation score" which is an ad-hoc measure of the % of valid bases (so A, T, C, G) across species that is the dominant most frequent) base. You could argue a more comprehensive conservation score might account for the number of species where the part of the sequences could not be aligned (e.g. the percentage of "-" bases), and that might be a great excersize for you!

``` Python
# 4. Conservation Score vs Accuracy
plt.figure(figsize=(12, 6))
conservation_bins = np.linspace(0, 1, 11)
df['conservation_bin'] = pd.cut(df['conservation_score'], bins=conservation_bins)

for model, color in zip(['gpn_normal', 'dnabert_normal', 'gpn_missing'], ['blue', 'green', 'red']):
    accuracy_by_conservation = df.groupby('conservation_bin')[f'{model}_correct'].mean()
    plt.plot(conservation_bins[:-1] + 0.05, accuracy_by_conservation, 
            marker='o', label=model.replace('_', ' ').title(), color=color)

plt.title('Model Accuracy vs Conservation Score')
plt.xlabel('Conservation Score')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()
```

![](images/paste-22.png)