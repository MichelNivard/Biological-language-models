<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Training our first DNA Language Model – Biological language models &amp; Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Chapter3.html" rel="next">
<link href="./Chapter1.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-687258f6a899824ebad24e93c100d594.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Biological language models &amp; Neural Networks</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Chapter1.html">DNA</a></li><li class="breadcrumb-item"><a href="./Chapter2.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Training our first DNA Language Model</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">DNA</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preparing DNA data for training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Training our first DNA Language Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluating DNA Language Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">2.1</span> Introduction</a></li>
  <li><a href="#why-would-we-train-dna-language-models" id="toc-why-would-we-train-dna-language-models" class="nav-link" data-scroll-target="#why-would-we-train-dna-language-models"><span class="header-section-number">2.2</span> Why would we train DNA language models?</a></li>
  <li><a href="#understanding-tokenization" id="toc-understanding-tokenization" class="nav-link" data-scroll-target="#understanding-tokenization"><span class="header-section-number">2.3</span> Understanding Tokenization</a>
  <ul class="collapse">
  <li><a href="#what-is-a-tokenizer" id="toc-what-is-a-tokenizer" class="nav-link" data-scroll-target="#what-is-a-tokenizer"><span class="header-section-number">2.3.1</span> What is a Tokenizer?</a></li>
  <li><a href="#our-dna-tokenizer" id="toc-our-dna-tokenizer" class="nav-link" data-scroll-target="#our-dna-tokenizer"><span class="header-section-number">2.3.2</span> Our DNA Tokenizer</a></li>
  <li><a href="#other-tokenization-strategies-for-dna-rna-and-proteins" id="toc-other-tokenization-strategies-for-dna-rna-and-proteins" class="nav-link" data-scroll-target="#other-tokenization-strategies-for-dna-rna-and-proteins"><span class="header-section-number">2.3.3</span> Other Tokenization Strategies for DNA, RNA, and Proteins</a></li>
  </ul></li>
  <li><a href="#loading-and-tokenizing-the-dna-dataset" id="toc-loading-and-tokenizing-the-dna-dataset" class="nav-link" data-scroll-target="#loading-and-tokenizing-the-dna-dataset"><span class="header-section-number">2.4</span> Loading and Tokenizing the DNA Dataset</a>
  <ul class="collapse">
  <li><a href="#understanding-the-dataset" id="toc-understanding-the-dataset" class="nav-link" data-scroll-target="#understanding-the-dataset"><span class="header-section-number">2.4.1</span> Understanding the Dataset</a></li>
  <li><a href="#tokenizing-the-dataset" id="toc-tokenizing-the-dataset" class="nav-link" data-scroll-target="#tokenizing-the-dataset"><span class="header-section-number">2.4.2</span> Tokenizing the Dataset</a></li>
  <li><a href="#saving-and-preparing-the-dataset-for-training" id="toc-saving-and-preparing-the-dataset-for-training" class="nav-link" data-scroll-target="#saving-and-preparing-the-dataset-for-training"><span class="header-section-number">2.4.3</span> Saving and Preparing the Dataset for Training</a></li>
  </ul></li>
  <li><a href="#understanding-bert-and-masked-language-modeling-mlm" id="toc-understanding-bert-and-masked-language-modeling-mlm" class="nav-link" data-scroll-target="#understanding-bert-and-masked-language-modeling-mlm"><span class="header-section-number">2.5</span> Understanding BERT and Masked Language Modeling (MLM)</a>
  <ul class="collapse">
  <li><a href="#what-is-bert" id="toc-what-is-bert" class="nav-link" data-scroll-target="#what-is-bert"><span class="header-section-number">2.5.1</span> What is BERT?</a></li>
  <li><a href="#what-is-masked-language-modeling-mlm" id="toc-what-is-masked-language-modeling-mlm" class="nav-link" data-scroll-target="#what-is-masked-language-modeling-mlm"><span class="header-section-number">2.5.2</span> What is Masked Language Modeling (MLM)?</a></li>
  <li><a href="#understanding-transformer-layers-attention-heads-and-hidden-size" id="toc-understanding-transformer-layers-attention-heads-and-hidden-size" class="nav-link" data-scroll-target="#understanding-transformer-layers-attention-heads-and-hidden-size"><span class="header-section-number">2.5.3</span> Understanding Transformer Layers, Attention Heads, and Hidden Size</a></li>
  </ul></li>
  <li><a href="#defining-the-bert-model-for-dna-sequences" id="toc-defining-the-bert-model-for-dna-sequences" class="nav-link" data-scroll-target="#defining-the-bert-model-for-dna-sequences"><span class="header-section-number">2.6</span> Defining the BERT Model for DNA Sequences</a>
  <ul class="collapse">
  <li><a href="#configuring-training-for-dna-bert" id="toc-configuring-training-for-dna-bert" class="nav-link" data-scroll-target="#configuring-training-for-dna-bert"><span class="header-section-number">2.6.1</span> Configuring Training for DNA BERT</a></li>
  <li><a href="#setting-training-parameters" id="toc-setting-training-parameters" class="nav-link" data-scroll-target="#setting-training-parameters"><span class="header-section-number">2.6.2</span> Setting Training Parameters</a></li>
  <li><a href="#preparing-for-masked-language-modeling-mlm" id="toc-preparing-for-masked-language-modeling-mlm" class="nav-link" data-scroll-target="#preparing-for-masked-language-modeling-mlm"><span class="header-section-number">2.6.3</span> Preparing for Masked Language Modeling (MLM)</a></li>
  <li><a href="#training-the-dna-bert-model" id="toc-training-the-dna-bert-model" class="nav-link" data-scroll-target="#training-the-dna-bert-model"><span class="header-section-number">2.6.4</span> Training the DNA BERT Model</a></li>
  <li><a href="#saving-the-trained-model" id="toc-saving-the-trained-model" class="nav-link" data-scroll-target="#saving-the-trained-model"><span class="header-section-number">2.6.5</span> Saving the Trained Model</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">2.6.6</span> Summary</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Chapter1.html">DNA</a></li><li class="breadcrumb-item"><a href="./Chapter2.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Training our first DNA Language Model</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Training our first DNA Language Model</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Abstract
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this chapter we’ll train an DNA language model. To do a full training run you’ll need newer Macbook (±24 hours), a gaming PC with GPU (10-12 hours) or google colab with the A100 GPU (±6 hours). If you don’t have the compute you can download <a href="https://huggingface.co/MichelNivard/DNABert-CDS-13Species-v0.1">DNABert</a>, a model trained based on this code fur use in the next few chapters.</p>
<p>All scripts for this chapter are found here: <a href="https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/Chapter_2" class="uri">https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/Chapter_2</a></p>
</div>
</div>
<section id="introduction" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">2.1</span> Introduction</h2>
<p>Now that we have collected and formatted a DNA dataset in Chapter 1, we can proceed to train a language model using the (Modern)BERT model architecture. In this chapter, we will walk through the process of tokenizing DNA sequences, configuring a BERT model, and training it using the <strong>Masked Language Modeling (MLM)</strong> objective.</p>
<p>We will cover the utility and rational behing DNA language models, and the key concepts behind tokenization, the BERT model, and the idea of masked language modeling (MLM) before diving into the Python script that trains the actual model.</p>
</section>
<section id="why-would-we-train-dna-language-models" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="why-would-we-train-dna-language-models"><span class="header-section-number">2.2</span> Why would we train DNA language models?</h2>
<p>For a full review of the utility of language models you should dig into the literature. I can recommend <span class="citation" data-cites="benegas2025">(<a href="references.html#ref-benegas2025" role="doc-biblioref">Benegas et al. 2025</a>)</span> for example. Genomic language models (gLMs) apply AI techniques to DNA sequences, enabling breakthroughs in variant effect prediction, sequence design, and genomic analysis.</p>
<p>Like larger language models like chatGPT DNA language models have “emergent properties:. If you train a gnomic Language models (gLM) on the reference genome sequence of humans, and various other species then the model that emerges is able to detect damaging mutations, without ever being trained on mutations (as mutations are defined as deviations from the reference)<span class="citation" data-cites="benegas2023">(<a href="references.html#ref-benegas2023" role="doc-biblioref">Benegas, Batra, and Song 2023</a>)</span>. To assess functional constraints, a widely used metric is the&nbsp;<strong>log-likelihood ratio (LLR)</strong>&nbsp;between two alleles. This measures the probability of a nucleotide variant appearing in a given context, with lower probabilities indicating potential deleterious effects. This application will be one of the examples I use throughout, simply because my experience in genetics align with it.</p>
<p>Another key application is transfer learning, where pretrained gLMs improve predictions in tasks like gene expression and chromatin accessibility. However, training effective models is difficult due to the vast, complex, and often non-functional nature of genomes. Unlike protein models, gLMs struggle with limited genomic diversity in training data and require more sophisticated benchmarks for evaluation.</p>
<p>Future advancements will focus on improving long-range genomic interactions, integrating multimodal biological data, and refining sequence design for practical applications. Despite challenges, gLMs hold great promise for revolutionizing genome research, advancing genetic disease understanding, and enabling synthetic biology innovations.</p>
</section>
<section id="understanding-tokenization" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="understanding-tokenization"><span class="header-section-number">2.3</span> Understanding Tokenization</h2>
<section id="what-is-a-tokenizer" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="what-is-a-tokenizer"><span class="header-section-number">2.3.1</span> What is a Tokenizer?</h3>
<p>A <strong>tokenizer</strong> is a fundamental component of any language model. Language models are essentially large multinomial models that predict the next token in a sequence based on previous tokens, or predict a masked token using the surrounding tokens. Since machine learning models operate on numerical representations, tokenized text must be converted into integers, which serve as indices for lookup in a vocabulary.</p>
<p>These integers, however, <strong>have no inherent numeric value</strong>—they simply act as categorical labels that the model learns to associate with semantic meaning. For example, if we tokenize the sentence:</p>
<blockquote class="blockquote">
<p>“The quick brown fox jumps over the lazy dog”</p>
</blockquote>
<p>at the <strong>word level</strong>, we might obtain a numerical sequence like:</p>
<blockquote class="blockquote">
<p><code>[4, 123, 678, 89, 245, 983, 56, 4564]</code></p>
</blockquote>
<p>where each number corresponds to a word based on a pre-defined tokenization dictionary, such as:</p>
<pre><code>{"the": 4, "quick": 123, "brown": 678, "fox": 89, "jumps": 245, "over": 983, "lazy": 56, "dog": 4564}</code></pre>
<p>Similarly, for DNA sequences, each nucleotide (A, T, C, G) is assigned a unique integer ID.</p>
</section>
<section id="our-dna-tokenizer" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="our-dna-tokenizer"><span class="header-section-number">2.3.2</span> Our DNA Tokenizer</h3>
<p>Our tokenizer uses a <strong>character-level</strong> approach, where each nucleotide is assigned a unique integer ID. Special tokens are also included for various purposes:</p>
<ul>
<li><code>[UNK]</code> (unknown token)</li>
<li><code>[PAD]</code> (padding token for equal-length sequences)</li>
<li><code>[CLS]</code> (classification token, useful for downstream tasks)</li>
<li><code>[SEP]</code> (separator token, used in tasks like sequence-pair classification)</li>
<li><code>[MASK]</code> (used for masked language modeling training)</li>
</ul>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers <span class="im">import</span> Tokenizer</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers.models <span class="im">import</span> WordLevel</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers.pre_tokenizers <span class="im">import</span> Split</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> PreTrainedTokenizerFast</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. DNA Tokenizer with Full FASTA Nucleic Acid Code</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Define vocabulary to include all FASTA nucleotides and symbols</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>dna_vocab <span class="op">=</span> {</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A"</span>: <span class="dv">0</span>, <span class="st">"T"</span>: <span class="dv">1</span>, <span class="st">"C"</span>: <span class="dv">2</span>, <span class="st">"G"</span>: <span class="dv">3</span>, <span class="st">"N"</span>: <span class="dv">4</span>, <span class="st">"U"</span>: <span class="dv">5</span>, <span class="st">"i"</span>: <span class="dv">6</span>,  <span class="co"># Standard bases + Inosine</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"R"</span>: <span class="dv">7</span>, <span class="st">"Y"</span>: <span class="dv">8</span>, <span class="st">"K"</span>: <span class="dv">9</span>, <span class="st">"M"</span>: <span class="dv">10</span>, <span class="st">"S"</span>: <span class="dv">11</span>, <span class="st">"W"</span>: <span class="dv">12</span>,  <span class="co"># Ambiguous bases</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"B"</span>: <span class="dv">13</span>, <span class="st">"D"</span>: <span class="dv">14</span>, <span class="st">"H"</span>: <span class="dv">15</span>, <span class="st">"V"</span>: <span class="dv">16</span>,  <span class="co"># More ambiguity codes</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"-"</span>: <span class="dv">17</span>,  <span class="co"># Gap character</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"[UNK]"</span>: <span class="dv">18</span>, <span class="st">"[PAD]"</span>: <span class="dv">19</span>, <span class="st">"[CLS]"</span>: <span class="dv">20</span>, <span class="st">"[SEP]"</span>: <span class="dv">21</span>, <span class="st">"[MASK]"</span>: <span class="dv">22</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Create tokenizer</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer(WordLevel(vocab<span class="op">=</span>dna_vocab, unk_token<span class="op">=</span><span class="st">"[UNK]"</span>))</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>tokenizer.pre_tokenizer <span class="op">=</span> Split(<span class="st">""</span>, <span class="st">"isolated"</span>)  <span class="co"># Character-level splitting</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to Hugging Face-compatible tokenizer</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>hf_tokenizer <span class="op">=</span> PreTrainedTokenizerFast(</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    tokenizer_object<span class="op">=</span>tokenizer,</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    unk_token<span class="op">=</span><span class="st">"[UNK]"</span>,</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    pad_token<span class="op">=</span><span class="st">"[PAD]"</span>,</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    cls_token<span class="op">=</span><span class="st">"[CLS]"</span>,</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    sep_token<span class="op">=</span><span class="st">"[SEP]"</span>,</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    mask_token<span class="op">=</span><span class="st">"[MASK]"</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="other-tokenization-strategies-for-dna-rna-and-proteins" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="other-tokenization-strategies-for-dna-rna-and-proteins"><span class="header-section-number">2.3.3</span> Other Tokenization Strategies for DNA, RNA, and Proteins</h3>
<p>While character-level tokenization is effective, other tokenization approaches can offer different tradeoffs:</p>
<section id="byte-pair-encoding-bpe" class="level4" data-number="2.3.3.1">
<h4 data-number="2.3.3.1" class="anchored" data-anchor-id="byte-pair-encoding-bpe"><span class="header-section-number">2.3.3.1</span> Byte Pair Encoding (BPE)</h4>
<p>BPE is widely used in NLP and allows frequently occurring patterns to be merged into subword units. While BPE could be useful for repetitive genome sequences, it may not capture biologically meaningful units.</p>
</section>
<section id="k-mer-tokenization" class="level4" data-number="2.3.3.2">
<h4 data-number="2.3.3.2" class="anchored" data-anchor-id="k-mer-tokenization"><span class="header-section-number">2.3.3.2</span> K-mer Tokenization</h4>
<p>K-mer tokenization groups nucleotides into fixed-length substrings (e.g., 3-mers like “ATG”). This approach retains local sequence structure but can lead to a large vocabulary size.</p>
</section>
<section id="tiktoken-and-similar-models" class="level4" data-number="2.3.3.3">
<h4 data-number="2.3.3.3" class="anchored" data-anchor-id="tiktoken-and-similar-models"><span class="header-section-number">2.3.3.3</span> Tiktoken and Similar Models</h4>
<p>Some modern tokenization methods, such as Tiktoken, optimize speed and efficiency by precomputing merges. These are often optimized for large-scale NLP tasks but could be adapted for biological sequences.</p>
<p>Choosing the best tokenizer depends on the specific modeling task. For example, RNA secondary structures or protein folding models might benefit from more complex tokenization strategies.</p>
<p>Source: <a href="https://rpubs.com/yuchenz585/1161578">RPubs Tokenization Review</a></p>
</section>
</section>
</section>
<section id="loading-and-tokenizing-the-dna-dataset" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="loading-and-tokenizing-the-dna-dataset"><span class="header-section-number">2.4</span> Loading and Tokenizing the DNA Dataset</h2>
<section id="understanding-the-dataset" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="understanding-the-dataset"><span class="header-section-number">2.4.1</span> Understanding the Dataset</h3>
<p>We will use a pre-existing dataset, <strong>Human-genome-CDS-GRCh38</strong>, which contains coding sequences from the human genome.</p>
</section>
<section id="tokenizing-the-dataset" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="tokenizing-the-dataset"><span class="header-section-number">2.4.2</span> Tokenizing the Dataset</h3>
<p>To prepare the dataset for training, we must <strong>apply the tokenizer to each sequence</strong> while ensuring:</p>
<ul>
<li>Sequences are truncated or padded to a fixed length (512 tokens)</li>
<li>Unwanted columns are removed</li>
</ul>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>dataset_name <span class="op">=</span> <span class="st">"MichelNivard/Human-genome-CDS-GRCh38"</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(dataset_name)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>column_name <span class="op">=</span> <span class="st">"sequence"</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(examples):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hf_tokenizer(examples[column_name], truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="st">"max_length"</span>, max_length<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize dataset</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>tokenized_dataset <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize_function, batched<span class="op">=</span><span class="va">True</span>, remove_columns<span class="op">=</span>[column_name])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="saving-and-preparing-the-dataset-for-training" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="saving-and-preparing-the-dataset-for-training"><span class="header-section-number">2.4.3</span> Saving and Preparing the Dataset for Training</h3>
<p>Once tokenized, we save the dataset for efficient access during training.</p>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>tokenized_dataset.save_to_disk(<span class="st">"tokenized_dna_dataset"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="understanding-bert-and-masked-language-modeling-mlm" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="understanding-bert-and-masked-language-modeling-mlm"><span class="header-section-number">2.5</span> Understanding BERT and Masked Language Modeling (MLM)</h2>
<section id="what-is-bert" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="what-is-bert"><span class="header-section-number">2.5.1</span> What is BERT?</h3>
<p>BERT (<strong>Bidirectional Encoder Representations from Transformers</strong>) is a powerful transformer-based language model. Unlike traditional left-to-right models, BERT <strong>learns bidirectional context</strong>, allowing it to understand sequences more effectively.</p>
<p>Returning to our earlier example sentence:</p>
<blockquote class="blockquote">
<p>“The quick brown fox jumps over the lazy dog”</p>
</blockquote>
<p>BERT does not process words one at a time but instead considers the entire sequence simultaneously. This bidirectional approach allows BERT to infer missing words based on context from both directions.</p>
</section>
<section id="what-is-masked-language-modeling-mlm" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="what-is-masked-language-modeling-mlm"><span class="header-section-number">2.5.2</span> What is Masked Language Modeling (MLM)?</h3>
<p>MLM is a self-supervised learning objective where the model learns by predicting missing tokens in a sequence. During training:</p>
<ul>
<li><strong>Some tokens are randomly replaced with <code>[MASK]</code></strong></li>
<li>The model must <strong>predict the original token</strong> based on surrounding context</li>
</ul>
<p>For example, if we mask the word “fox” in our sentence:</p>
<blockquote class="blockquote">
<p>“The quick brown [MASK] jumps over the lazy dog”</p>
</blockquote>
<p>BERT will analyze the remaining words and attempt to predict “fox.”</p>
<p>This technique enables BERT to <strong>learn useful representations</strong> without requiring labeled data.</p>
</section>
<section id="understanding-transformer-layers-attention-heads-and-hidden-size" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="understanding-transformer-layers-attention-heads-and-hidden-size"><span class="header-section-number">2.5.3</span> Understanding Transformer Layers, Attention Heads, and Hidden Size</h3>
<p>A <strong>transformer layer</strong> consists of self-attention and feed-forward layers that help the model learn relationships between tokens. The number of <strong>transformer layers</strong> determines how deep the model is.</p>
<p>An <strong>attention head</strong> is a component of the self-attention mechanism that learns different types of relationships within the data. Having <strong>multiple attention heads</strong> allows the model to capture various dependencies between tokens.</p>
<p>Returning to our example:</p>
<ul>
<li>One attention head might focus on subject-verb relationships, recognizing that “fox” is the subject of “jumps.”</li>
<li>Another head might capture adjective-noun relationships, linking “brown” to “fox.”</li>
</ul>
<p>The <strong>hidden size</strong> defines the dimensionality of the model’s internal representations. A larger hidden size enables the model to capture more complex patterns, but also increases computational cost.</p>
<p>By stacking multiple transformer layers and attention heads, BERT gains a deep understanding of sentence structures.</p>
</section>
</section>
<section id="defining-the-bert-model-for-dna-sequences" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="defining-the-bert-model-for-dna-sequences"><span class="header-section-number">2.6</span> Defining the BERT Model for DNA Sequences</h2>
<p>While the “quick brown fox” example helps us understand how BERT processes natural language, our goal is to apply the same principles to&nbsp;<strong>DNA sequences</strong>. Instead of predicting missing words in a sentence, we want our model to learn&nbsp;<strong>biological patterns</strong>&nbsp;and&nbsp;<strong>genomic structure</strong>&nbsp;by predicting masked nucleotides within DNA sequences.</p>
<p>In&nbsp;<strong>DNA modeling</strong>, understanding sequence context is just as critical as in language modeling. Just as BERT learns that “fox” fits within a given sentence structure, our model should learn that&nbsp;<strong>specific nucleotide sequences appear in biologically meaningful patterns</strong>. This could involve recognizing&nbsp;<strong>gene coding regions, regulatory motifs, or conserved sequence elements</strong>&nbsp;across different genomes.</p>
<p>To accomplish this, we define a&nbsp;<strong>custom BERT model</strong>&nbsp;designed specifically for processing DNA sequences. Unlike traditional text-based models, our DNA BERT model uses a&nbsp;<strong>character-level vocabulary</strong>&nbsp;of nucleotides (A, T, C, G) and special tokens to represent sequence structure. By leveraging&nbsp;<strong>masked language modeling (MLM)</strong>, the model will learn to predict missing nucleotides based on surrounding context, allowing it to capture meaningful genomic features.</p>
<p>With this in mind, let’s move forward and define our BERT architecture for DNA sequences.</p>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> ModernBertConfig, ModernBertForMaskedLM</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> ModernBertConfig(</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="bu">len</span>(dna_vocab),</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    hidden_size<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    num_hidden_layers<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    num_attention_heads<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    intermediate_size<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    max_position_embeddings<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    type_vocab_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>config.pad_token_id <span class="op">=</span> dna_vocab[<span class="st">"[PAD]"</span>]</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ModernBertForMaskedLM(config)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="configuring-training-for-dna-bert" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="configuring-training-for-dna-bert"><span class="header-section-number">2.6.1</span> Configuring Training for DNA BERT</h3>
<p>Now that we have defined our BERT model for DNA sequences, we need to set up the <strong>training process</strong>. This involves specifying various <strong>training hyperparameters</strong>, handling <strong>masked language modeling (MLM)</strong> data, and preparing for efficient learning.</p>
<p>Unlike traditional NLP tasks where models are trained on massive text corpora, training on DNA sequences presents unique challenges. The structure of genomic data, sequence length, and biological patterns all influence how the model learns. Our configuration ensures that the training process is efficient while maintaining biological relevance.</p>
<hr>
</section>
<section id="setting-training-parameters" class="level3" data-number="2.6.2">
<h3 data-number="2.6.2" class="anchored" data-anchor-id="setting-training-parameters"><span class="header-section-number">2.6.2</span> Setting Training Parameters</h3>
<p>To train our DNA BERT model, we use the <strong>Hugging Face <code>TrainingArguments</code> class</strong>, which allows us to define key training settings. These include:</p>
<ul>
<li><strong>Batch size:</strong> We set a batch size of <code>16</code> for both training and evaluation. This determines how many sequences are processed at once.</li>
<li><strong>Logging &amp; Saving:</strong> We log loss every <code>50</code> steps and save model checkpoints every <code>100</code> steps to monitor training progress.</li>
<li><strong>Learning Rate:</strong> We use a learning rate of <code>5e-5</code>, a common choice for transformer models that balances learning speed and stability.</li>
<li><strong>Weight Decay:</strong> A value of <code>0.01</code> is used to prevent overfitting by applying <strong>L2 regularization</strong> to model weights.</li>
<li><strong>Training Steps:</strong> The model is trained for <code>4000</code> steps. This ensures sufficient learning without excessive computation.</li>
<li><strong>Model Saving:</strong> The model checkpoints are stored in <code>./bert-dna</code>, allowing us to resume training if needed.</li>
</ul>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./bert-dna"</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    overwrite_output_dir<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">50</span>,  <span class="co"># Log loss every step</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    save_steps<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    save_total_limit<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    max_steps<span class="op">=</span><span class="dv">4000</span>,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">5e-5</span>,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    push_to_hub<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>,  <span class="co"># Disables wandb logging</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="preparing-for-masked-language-modeling-mlm" class="level3" data-number="2.6.3">
<h3 data-number="2.6.3" class="anchored" data-anchor-id="preparing-for-masked-language-modeling-mlm"><span class="header-section-number">2.6.3</span> Preparing for Masked Language Modeling (MLM)</h3>
<p>Since we are training our DNA BERT model using <strong>masked language modeling (MLM)</strong>, we need to handle <strong>masked tokens</strong> properly. This is done using the <strong><code>DataCollatorForLanguageModeling</code></strong>, which:</p>
<ul>
<li><strong>Randomly masks nucleotides</strong> in the training sequences.</li>
<li><strong>Creates <code>labels</code> automatically</strong>, meaning the model learns by trying to predict these masked tokens.</li>
<li><strong>Uses a masking probability of 5%</strong>, ensuring that a small but meaningful portion of the sequence is masked during training.</li>
</ul>
<p>By applying MLM, we allow the model to <strong>generalize nucleotide relationships</strong> and capture <strong>sequence dependencies</strong>, just like how BERT learns relationships between words in text.</p>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorForLanguageModeling(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>hf_tokenizer,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    mlm<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    mlm_probability<span class="op">=</span><span class="fl">0.05</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="training-the-dna-bert-model" class="level3" data-number="2.6.4">
<h3 data-number="2.6.4" class="anchored" data-anchor-id="training-the-dna-bert-model"><span class="header-section-number">2.6.4</span> Training the DNA BERT Model</h3>
<p>With our configuration and data collator in place, we now <strong>train the model</strong>. We use the <strong>Hugging Face <code>Trainer</code> API</strong>, which simplifies the training process by handling:</p>
<ul>
<li><strong>Dataset iteration:</strong> Automatically loads and batches training sequences.</li>
<li><strong>Gradient updates:</strong> Adjusts model weights based on training loss.</li>
<li><strong>Logging &amp; saving:</strong> Tracks training progress and stores checkpoints.</li>
</ul>
<p>Once training begins, the model will gradually <strong>learn nucleotide dependencies</strong> and improve its ability to predict missing DNA bases.</p>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>tokenized_dataset,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>hf_tokenizer,</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collator,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>trainer.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>you set up free wandb logging (go to <a href="https://wandb.ai/site" class="uri">https://wandb.ai/site</a> for more info) you can track your training runs online, wherever they are running. You then get a dashboard full of pretty loss vs progress plots like the one below which I screencapped about ± 30 minutes into training on my macbook.</p>
<p><img src="images/paste-4.png" class="img-fluid"></p>
</section>
<section id="saving-the-trained-model" class="level3" data-number="2.6.5">
<h3 data-number="2.6.5" class="anchored" data-anchor-id="saving-the-trained-model"><span class="header-section-number">2.6.5</span> Saving the Trained Model</h3>
<p>After training completes, we save both the <strong>model</strong> and <strong>tokenizer</strong> so they can be used for future predictions or fine-tuning.</p>
<ul>
<li>The <strong>model weights</strong> are stored in <code>./bert-dna</code>, allowing us to reload the trained model.</li>
<li>The <strong>tokenizer</strong> is also saved, ensuring that input sequences can be processed the same way during inference.</li>
</ul>
<p>Finally, a success message is printed, confirming that the training process has been completed.</p>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the final model and tokenizer</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>trainer.save_model(<span class="st">"./bert-dna"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>hf_tokenizer.save_pretrained(<span class="st">"./bert-dna"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"🎉 Training complete! Model saved to ./bert-dna"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="summary" class="level3" data-number="2.6.6">
<h3 data-number="2.6.6" class="anchored" data-anchor-id="summary"><span class="header-section-number">2.6.6</span> Summary</h3>
<p>In this section, we:</p>
<ul>
<li>Defined <strong>training hyperparameters</strong> such as batch size, learning rate, and training steps.</li>
<li>Used <strong>masked language modeling (MLM)</strong> to train the model on DNA sequences.</li>
<li>Leveraged the <strong>Hugging Face <code>Trainer</code> API</strong> to automate model training.</li>
<li>Saved the <strong>final trained model and tokenizer</strong> for future use.</li>
</ul>
<p>With this trained model, we can now <strong>fine-tune or apply it to various genomic tasks</strong>, such as predicting genetic variations or classifying functional DNA sequences. In the next chapter, we will explore <strong>how to fine-tune our DNA BERT model for specific applications</strong>.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-benegas2023" class="csl-entry" role="listitem">
Benegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. <span>“DNA Language Models Are Powerful Predictors of Genome-Wide Variant Effects.”</span> <em>Proceedings of the National Academy of Sciences</em> 120 (44). <a href="https://doi.org/10.1073/pnas.2311219120">https://doi.org/10.1073/pnas.2311219120</a>.
</div>
<div id="ref-benegas2025" class="csl-entry" role="listitem">
Benegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun S. Song. 2025. <span>“Genomic Language Models: Opportunities and Challenges.”</span> <em>Trends in Genetics</em>, January. <a href="https://doi.org/10.1016/j.tig.2024.11.013">https://doi.org/10.1016/j.tig.2024.11.013</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Chapter1.html" class="pagination-link" aria-label="Preparing DNA data for training">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preparing DNA data for training</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Chapter3.html" class="pagination-link" aria-label="Evaluating DNA Language Models">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluating DNA Language Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>