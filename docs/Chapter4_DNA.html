<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Evolution-Aware Encoders – Biological Language Models &amp; Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Chapter5_DNA.html" rel="next">
<link href="./Chapter3_DNA.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e84559ba8659b1a571faa725acb99328.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Biological Language Models &amp; Neural Networks</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Chapter1_DNA.html">DNA</a></li><li class="breadcrumb-item"><a href="./Chapter4_DNA.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evolution-Aware Encoders</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Preamble1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">What is this Book About?</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Preamble2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to Read this Book</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">DNA</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter1_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preparing DNA data for training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter2_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Training our first DNA Language Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter3_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluating DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter4_DNA.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evolution-Aware Encoders</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter5_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Weaving Together Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter6_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">A Review of Current DNA Language Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Scaling_training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Scale up Training</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Proteins</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter1_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Proteins: from sequence to structure</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter2_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Selecting and curating protein sequences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter3_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Training our first Protein Language Model</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Multi-Modal-Biology</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">4.1</span> Introduction</a></li>
  <li><a href="#tokenization-and-embedding-in-language-models" id="toc-tokenization-and-embedding-in-language-models" class="nav-link" data-scroll-target="#tokenization-and-embedding-in-language-models"><span class="header-section-number">4.2</span> Tokenization and Embedding in Language Models</a>
  <ul class="collapse">
  <li><a href="#tokenization-in-natural-language" id="toc-tokenization-in-natural-language" class="nav-link" data-scroll-target="#tokenization-in-natural-language"><span class="header-section-number">4.2.1</span> Tokenization in Natural Language</a></li>
  <li><a href="#the-embedding-process-nlp-bert" id="toc-the-embedding-process-nlp-bert" class="nav-link" data-scroll-target="#the-embedding-process-nlp-bert"><span class="header-section-number">4.2.2</span> The Embedding Process (NLP BERT)</a></li>
  <li><a href="#language-evolution-is-decayed" id="toc-language-evolution-is-decayed" class="nav-link" data-scroll-target="#language-evolution-is-decayed"><span class="header-section-number">4.2.3</span> Language Evolution is Decayed</a></li>
  <li><a href="#biological-sequences-are-fundamentally-different" id="toc-biological-sequences-are-fundamentally-different" class="nav-link" data-scroll-target="#biological-sequences-are-fundamentally-different"><span class="header-section-number">4.2.4</span> Biological Sequences are Fundamentally Different</a></li>
  <li><a href="#evolutionary-context-as-an-embedding" id="toc-evolutionary-context-as-an-embedding" class="nav-link" data-scroll-target="#evolutionary-context-as-an-embedding"><span class="header-section-number">4.2.5</span> Evolutionary Context as an Embedding</a></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section"><span class="header-section-number">4.2.6</span> </a></li>
  </ul></li>
  <li><a href="#introducing-gpn-msa-bert" id="toc-introducing-gpn-msa-bert" class="nav-link" data-scroll-target="#introducing-gpn-msa-bert"><span class="header-section-number">4.3</span> 4. Introducing GPN-MSA-BERT</a>
  <ul class="collapse">
  <li><a href="#key-idea-dynamic-position-embeddings" id="toc-key-idea-dynamic-position-embeddings" class="nav-link" data-scroll-target="#key-idea-dynamic-position-embeddings"><span class="header-section-number">4.3.1</span> Key Idea: Dynamic Position Embeddings</a></li>
  <li><a href="#visualization" id="toc-visualization" class="nav-link" data-scroll-target="#visualization"><span class="header-section-number">4.3.2</span> Visualization</a></li>
  <li><a href="#practical-implementation---replacing-the-bert-encoder" id="toc-practical-implementation---replacing-the-bert-encoder" class="nav-link" data-scroll-target="#practical-implementation---replacing-the-bert-encoder"><span class="header-section-number">4.3.3</span> Practical Implementation - Replacing the BERT Encoder</a></li>
  </ul></li>
  <li><a href="#what-are-we-masking" id="toc-what-are-we-masking" class="nav-link" data-scroll-target="#what-are-we-masking"><span class="header-section-number">4.4</span> What are we masking?</a></li>
  <li><a href="#recap-of-our-approach" id="toc-recap-of-our-approach" class="nav-link" data-scroll-target="#recap-of-our-approach"><span class="header-section-number">4.5</span> Recap of Our Approach</a></li>
  <li><a href="#preview-of-chapter-5" id="toc-preview-of-chapter-5" class="nav-link" data-scroll-target="#preview-of-chapter-5"><span class="header-section-number">4.6</span> Preview of Chapter 5</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Chapter1_DNA.html">DNA</a></li><li class="breadcrumb-item"><a href="./Chapter4_DNA.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evolution-Aware Encoders</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evolution-Aware Encoders</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Abstract
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this chapter, we’ll evaluate the architecture of natural language models, which we have up to this point uncritically adopted for DNA modeling from our NLP/corporate brethren. We’ll discuss how some researchers have begun to move on from applying known model architectures to DNA and started to (re)designing model architectures specifically with DNA in mind. These models lean into our very extensive knowledge of the evolutionary history of the Genome.</p>
<p>In this chapter, we are forced to confront some fundamental questions: what is prediction? What do we want a model to learn?</p>
<p>All scripts for this chapter are found here: <a href="https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/DNA/Chapter_4" class="uri">https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/DNA/Chapter_4</a></p>
</div>
</div>
<section id="introduction" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">4.1</span> Introduction</h2>
<p>In previous chapters, we introduced the basic principles of <strong>BERT for DNA sequences</strong>. We took inspiration from natural language processing (NLP), treating DNA as a <strong>language</strong>, where sequences of nucleotides (A, T, C, G, -) could be processed using transformers. This approach, while powerful, carries over several assumptions from natural language that do not perfectly align with biological sequences. In this chapter, we will re-examine how we encode genomic data and introduce <strong>a new design paradigm — evolutionary-aware encoding — inspired by the recently proposed GPN (Genomic Pre-trained Network).</strong></p>
</section>
<section id="tokenization-and-embedding-in-language-models" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="tokenization-and-embedding-in-language-models"><span class="header-section-number">4.2</span> Tokenization and Embedding in Language Models</h2>
<p>Modern language models, whether <strong>BERT</strong>, <strong>GPT</strong>, or similar architectures, rely heavily on <strong>how input sequences are tokenized and encoded before they ever reach the attention layers</strong>. This initial step — often overlooked — plays a profound role in shaping how the model learns.</p>
<section id="tokenization-in-natural-language" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="tokenization-in-natural-language"><span class="header-section-number">4.2.1</span> Tokenization in Natural Language</h3>
<p>In human languages like English or French, the vocabulary is <strong>large</strong>, often comprising tens of thousands of tokens. These tokens could be:</p>
<ul>
<li>Whole words (“cat”, “sat”).</li>
<li>Subwords (“cat” might break into “c”, “at”).</li>
<li>Even characters (in rare cases).</li>
</ul>
<p>Since the number of tokens is so large, <strong>each token is assigned a unique vector embedding — a dense, learnable representation of its “meaning”</strong>. These embeddings are gradually refined during training as the model learns how tokens behave in different contexts. The model learns, based on the massive amounts of training data, what the word means, what other words have similar or related meanings. This is essential because linguists and those who study language have vast knowledge of word meaning, numerically encoding that knowledge so that a computational model could process isn’t currently a feasible task. Therefore, in a natural (as opposed to biological) large language model, word embeddings are learned from the data, the data being all the text on the internet.</p>
</section>
<section id="the-embedding-process-nlp-bert" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="the-embedding-process-nlp-bert"><span class="header-section-number">4.2.2</span> The Embedding Process (NLP BERT)</h3>
<pre><code>Input Sentence:  "The cat sat on the mat"

Step 1 - Tokenization:
    ["The", "cat", "sat", "on", "the", "mat"]

Step 2 - Lookup:
    Each token gets a fixed vector from an embedding table.

    "The" -&gt; [0.25, 0.13, -0.11, ..., 0.04]
    "cat" -&gt; [0.88, -0.23, 0.45, ..., -0.67]

Step 3 - Transformer Layers:
    These embeddings are updated based on surrounding words (context).</code></pre>
</section>
<section id="language-evolution-is-decayed" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="language-evolution-is-decayed"><span class="header-section-number">4.2.3</span> Language Evolution is Decayed</h3>
<p>The design of these token embeddings reflects a key fact about human languages: <strong>the evolutionary history of words might be relevant to understanding their meaning today, but the words’ context in text is way more informative</strong>. While linguistic etymology exists, the meaning of “cat” today does not rely on whether the word originated from Latin or Proto-Indo-European. Context (the words around “cat”) matters far more than distant etymology. Even if I am unfairly discounting the importance of etymology in linguistics (I am no linguist, don’t take my word for it), the quantity of older texts, relative to the quantity of modern texts, the lack of an obvious coding scheme for embedding a word in its etymological history are problematic and would have to be very effective given how effective “word in textual context” embeddings are. However, biology, and DNA in particular, is different.</p>
</section>
<section id="biological-sequences-are-fundamentally-different" class="level3" data-number="4.2.4">
<h3 data-number="4.2.4" class="anchored" data-anchor-id="biological-sequences-are-fundamentally-different"><span class="header-section-number">4.2.4</span> Biological Sequences are Fundamentally Different</h3>
<p>The DNA encoding we have been working with (A, T, G, C, -) has 5 tokens, perhaps 20 if we encode all the codes used in genetics to code for ambiguous or missing bases. Protein language models we’ll cover later have ±20 amino-acids commonly found in proteins. If we use longer vocabularies, like k-mer or BPE tokenizer vocabularies, it’s not clear the longer sequences we obtain really are comparable or interchangeable. The point of embedding is to cluster similar and dissimilarities, in order to predict the next or a masked token if the presence of up to 128,000 tokens to choose from, some of which have very similar meanings or could fully alter the meaning of a sentence (by negation or omission). In biology, we have a small vocabulary, 5 or 20, or if you wish up to a few hundred tokens. We do, however, have an incredible understanding of the evolutionary history (<strong>Figure 4.1</strong>) of each base in the genome, we know its place in the genome of other species and can align those to each other!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://evogeneao.s3.amazonaws.com/images/tree_of_life/tree-of-life_2000.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure 4.1</strong> The Evogeneao Tree of Life diagram, all rights reserved Leonard Eisenberg (2008 &amp; 2017). Get posters and relevant teaching materials here: <a href="https://www.evogeneao.com/en/learn/tree-of-life" class="uri">https://www.evogeneao.com/en/learn/tree-of-life</a></figcaption>
</figure>
</div>
</section>
<section id="evolutionary-context-as-an-embedding" class="level3" data-number="4.2.5">
<h3 data-number="4.2.5" class="anchored" data-anchor-id="evolutionary-context-as-an-embedding"><span class="header-section-number">4.2.5</span> Evolutionary Context as an Embedding</h3>
<p>The <strong>evolutionary history of a genomic position — how conserved it is, how it varies across species — directly influences our estimation of its importance and its tolerance to mutation</strong>. A nucleotide in a highly conserved enhancer region requires different levels of attention (from the model or us scientists) than a nucleotide in a rapidly evolving spacer.</p>
</section>
<section id="section" class="level3" data-number="4.2.6">
<h3 data-number="4.2.6" class="anchored" data-anchor-id="section"><span class="header-section-number">4.2.6</span> </h3>
<table class="caption-top table">
<caption><strong>Table 4.1</strong> Key Differences Between Language and DNA</caption>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Natural Language</th>
<th>Genomics</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Number of Tokens</td>
<td>Tens of thousands</td>
<td>~5 (A, T, G, C, -)</td>
</tr>
<tr class="even">
<td>Meaning</td>
<td>Flexible, evolves over time</td>
<td>Biochemically fixed</td>
</tr>
<tr class="odd">
<td>Evolutionary Context</td>
<td>Mostly irrelevant to meaning</td>
<td>Often crucial (conservation, divergence)</td>
</tr>
<tr class="even">
<td>Token Embedding</td>
<td>Fully learned</td>
<td>No unique encoding for each token, but predefined based on token-specific evolutionary history</td>
</tr>
<tr class="odd">
<td>Neighboring Context</td>
<td>Defines meaning</td>
<td>Defines local motifs, but evolutionary context adds extra layer</td>
</tr>
</tbody>
</table>
<p>To capture this <strong>cross-species evolutionary context</strong>, we need an <strong>embedding strategy that combines:</strong></p>
<ol type="1">
<li><strong>The identity of the nucleotide itself (A, T, G, C, -)</strong>.</li>
<li><strong>The state of this position in aligned species (what bases appear at the same position in other species).</strong></li>
</ol>
<p>This evolutionary-aware encoding is at the heart of the <strong>Genomic Pre-trained Network (GPN)</strong> architecture and various famous protein language models like AlphaFold<span class="citation" data-cites="Benegas2024 Lupo2022 jumper2021">(<a href="references.html#ref-Benegas2024" role="doc-biblioref">Benegas et al. 2023</a>; <a href="references.html#ref-Lupo2022" role="doc-biblioref">Lupo, Sgarbossa, and Bitbol 2022</a>; <a href="references.html#ref-jumper2021" role="doc-biblioref">Jumper et al. 2021</a>)</span>. In DNA networks, we’ll discuss in this chapter, the encoding is computed for each base given its history. So while the model has 5 tokens (G, C, T, A, and -), these tokens do not map to a fixed embedding; rather, the base “A” maps to an encoding (one-hot encoding) for A, but then also for the same base in aligned sequences of 99 non-human species. This fundamentally changes the model architecture, changing it from a language model applied to DNA as we did in Chapter 3 to a DNA language model, or maybe even just a DNA model.</p>
</section>
</section>
<section id="introducing-gpn-msa-bert" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="introducing-gpn-msa-bert"><span class="header-section-number">4.3</span> 4. Introducing GPN-MSA-BERT</h2>
<p>GPN-MSA-BERT (inspired by <span class="citation" data-cites="Benegas2024">Benegas et al. (<a href="references.html#ref-Benegas2024" role="doc-biblioref">2023</a>)</span>) adapts BERT-style masked language modeling (MLM) to DNA sequences, but <strong>incorporates multispecies alignment (MSA) data directly into the model’s input</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/paste-15.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure 4.2</strong> An example multiple sequence alignment (MSA) across 7 sequences (usually species). Source: <a href="https://www.biorender.com/template/multiple-sequence-alignment-dna" class="uri">https://www.biorender.com/template/multiple-sequence-alignment-dna</a> author: <a href="https://app.biorender.com/profile/eunice_huang">Eunice Huang</a></figcaption>
</figure>
</div>
<section id="key-idea-dynamic-position-embeddings" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="key-idea-dynamic-position-embeddings"><span class="header-section-number">4.3.1</span> Key Idea: Dynamic Position Embeddings</h3>
<p>For each position in the human genome, the model receives:</p>
<ul>
<li>The human base (A, T, G, C, -) — this is the usual input.</li>
<li>The aligned bases from other species — these are additional features.</li>
<li>These aligned bases are one-hot encoded and concatenated to the human base’s embedding.</li>
</ul>
<p>This turns a simple nucleotide embedding, for any given nucleotide, into a dynamic, position-specific vector that depends on its evolutionary context across species.</p>
</section>
<section id="visualization" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="visualization"><span class="header-section-number">4.3.2</span> Visualization</h3>
<pre><code>Human Position:     A
Aligned Species:    A  G  A  (species 1, species 2, species 3)

Embedding:
    [ OneHot_A | OneHot_A | OneHot_G | OneHot_A ]</code></pre>
<p>This combined vector captures:</p>
<ul>
<li>What the human base is.</li>
<li>How conserved the site is.</li>
<li>Which substitutions are tolerated across species.</li>
</ul>
</section>
<section id="practical-implementation---replacing-the-bert-encoder" class="level3" data-number="4.3.3">
<h3 data-number="4.3.3" class="anchored" data-anchor-id="practical-implementation---replacing-the-bert-encoder"><span class="header-section-number">4.3.3</span> Practical Implementation - Replacing the BERT Encoder</h3>
<p>To implement this in practice, we can directly modify a Hugging Face model class (like <code>ModernBertForMaskedLM</code>) to use our custom <strong>GPNEmbedding</strong> layer in place of the standard token embedding layer.</p>
<p>This requires:</p>
<ul>
<li>Defining a tokenizer that tokenizes each base and aligned bases in other species into the structure expected by the embedding.</li>
<li>Defining a <strong>GPNEmbedding</strong> class that can handle one-hot human base with species features and builds the embedding for each base.</li>
<li>Replacing <code>ModernBertForMaskedLM</code> with a custom <code>GPNBERTMaskedLM</code> class.</li>
<li>Ensuring all <code>forward</code> methods accept both <code>input_ids</code> and <code>aux_features</code>, which are passed into the embedding layer.</li>
<li>We additionally define our own tokenizer and data collator (not shown here but available in the full script).</li>
</ul>
<p>The code below takes a human sequence, encodes it in a one-hot encoding (so A: 10000, T: 01000, G: 00100, C: 00010, -: 00001 for example), does the same for any auxiliary aligned sequences from other species. Then the embedding function combines both into one.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Encode Human and Auxiliary Species Sequences in </span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_hot_encode_base(base):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""One-hot encodes A, T, G, C, - (5 bases total)."""</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    base_to_idx <span class="op">=</span> {<span class="st">"A"</span>: <span class="dv">0</span>, <span class="st">"T"</span>: <span class="dv">1</span>, <span class="st">"C"</span>: <span class="dv">2</span>, <span class="st">"G"</span>: <span class="dv">3</span>, <span class="st">"-"</span>: <span class="dv">4</span>}</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    one_hot <span class="op">=</span> np.zeros(<span class="dv">5</span>, dtype<span class="op">=</span>np.float32)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> base <span class="kw">in</span> base_to_idx:</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        one_hot[base_to_idx[base]] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> one_hot</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_with_aux(examples):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    human_seq <span class="op">=</span> clean_sequence(examples[<span class="st">"human_sequence"</span>])</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Drop first 10 species (closest relatives)</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    species_seqs <span class="op">=</span> [clean_sequence(seq) <span class="cf">for</span> seq <span class="kw">in</span> examples[<span class="st">"species_sequences"</span>]]</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    species_seqs <span class="op">=</span> species_seqs[<span class="dv">10</span>:]  <span class="co"># &lt;-- This line omits the first 10 species</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenize human sequence</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> hf_tokenizer(human_seq, truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="st">"max_length"</span>, max_length<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> tokens[<span class="st">"input_ids"</span>]</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process species sequences into concatenated one-hot vectors (aux features)</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    seq_len <span class="op">=</span> <span class="bu">len</span>(input_ids)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    num_species <span class="op">=</span> <span class="bu">len</span>(species_seqs)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    aux_features <span class="op">=</span> np.zeros((seq_len, num_species <span class="op">*</span> <span class="dv">5</span>), dtype<span class="op">=</span>np.float32)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> pos <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pos <span class="op">&gt;=</span> <span class="bu">len</span>(human_seq):  <span class="co"># Handle padding case</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> species_idx, species_seq <span class="kw">in</span> <span class="bu">enumerate</span>(species_seqs):</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> pos <span class="op">&lt;</span> <span class="bu">len</span>(species_seq):</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>                aux_features[pos, species_idx <span class="op">*</span> <span class="dv">5</span>:(species_idx <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> <span class="dv">5</span>] <span class="op">=</span> one_hot_encode_base(species_seq[pos])</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    tokens[<span class="st">"aux_features"</span>] <span class="op">=</span> aux_features.tolist()</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokens</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------</span></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 8. Define GPNEmbedding </span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------</span></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPNEmbedding(nn.Module):</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config, n_species):</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_species <span class="op">=</span> n_species</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> <span class="dv">5</span>  <span class="co"># A, T, G, C, -</span></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.species_feature_size <span class="op">=</span> n_species <span class="op">*</span> <span class="va">self</span>.vocab_size</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids, aux_features):</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>        one_hot <span class="op">=</span> F.one_hot(input_ids, num_classes<span class="op">=</span><span class="va">self</span>.config.vocab_size).<span class="bu">float</span>()</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine human one-hot with species aux_features</span></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>        combined <span class="op">=</span> torch.cat([one_hot, aux_features], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> combined.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">&lt;</span> <span class="va">self</span>.config.hidden_size:</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>            pad <span class="op">=</span> <span class="va">self</span>.config.hidden_size <span class="op">-</span> combined.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>            combined <span class="op">=</span> F.pad(combined, (<span class="dv">0</span>, pad))</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> combined</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>From here on out, things are fairly standard. A transformer model (here BERT but could be anything really) is initialized to learn the relationship between adjacent tokens using a masked language model training regime. In the code below, the custom embeddings are introduced into the masked language model (ModernBert in this case) while the encoder part of the model (the core part of the model that learns the relation between adjacent tokens) remains unchanged.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. GPNBERTMaskedLM</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPNBERTMaskedLM(nn.Module):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config, n_species):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_species <span class="op">=</span> n_species</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> <span class="dv">5</span>  <span class="co"># A, T, G, C, -</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.species_feature_size <span class="op">=</span> n_species <span class="op">*</span> <span class="va">self</span>.vocab_size</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> ModernBertModel(config)  <span class="co"># Directly initialize the transformer backbone</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls <span class="op">=</span> nn.Linear(config.hidden_size, config.vocab_size)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> GPNEmbedding(config, n_species)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids<span class="op">=</span><span class="va">None</span>, aux_features<span class="op">=</span><span class="va">None</span>, labels<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        embeddings <span class="op">=</span> <span class="va">self</span>.embedding(input_ids, aux_features)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Only pass valid args to the encoder</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        encoder_kwargs <span class="op">=</span> {k: v <span class="cf">for</span> k, v <span class="kw">in</span> kwargs.items() <span class="cf">if</span> k <span class="kw">in</span> {<span class="st">"attention_mask"</span>, <span class="st">"position_ids"</span>, <span class="st">"head_mask"</span>}}</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> <span class="va">self</span>.encoder(inputs_embeds<span class="op">=</span>embeddings, <span class="op">**</span>encoder_kwargs)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        sequence_output <span class="op">=</span> outputs.last_hidden_state</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        prediction_scores <span class="op">=</span> <span class="va">self</span>.cls(sequence_output)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> labels <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>            loss_fct <span class="op">=</span> nn.CrossEntropyLoss(ignore_index<span class="op">=-</span><span class="dv">100</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_fct(prediction_scores.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.config.vocab_size), labels.view(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> MaskedLMOutput(</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>            loss<span class="op">=</span>loss,</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>            logits<span class="op">=</span>prediction_scores,</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>            hidden_states<span class="op">=</span>outputs.hidden_states,</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>            attentions<span class="op">=</span>outputs.attentions,</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="what-are-we-masking" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="what-are-we-masking"><span class="header-section-number">4.4</span> What are we masking?</h2>
<p>I promised we’d have to deeply consider what we count as prediction, and that’s going to have to happen right now. In Chapter 2, we trained a DNA language model, and in Chapter 3, we saw how well it did and did not predict specific features. In <strong>Figure 3.3</strong>, you saw the model predicts the true bases in the DRD2 gene with about 40%. What if I told you I can predict the bases in the human reference genome with &gt; 95% probability with a “model” based on a supervised model? To do so, I’d just pick the consensus base across other species! Human DNA and chimpanzee DNA are &gt; 90% identical, and human and mouse genomes are remarkably similar (85%, I think). In <strong>Figure 4.3</strong> (from <span class="citation" data-cites="initial2002">(<a href="references.html#ref-initial2002" role="doc-biblioref"><span>“Initial Sequencing and Comparative Analysis of the Mouse Genome”</span> 2002</a>)</span>), we see the human sequences that are preserved well in the mouse genome, and they cover a staggering portion of it. This means we can “beat” base predictions made by our previous model by a mile, simply by picking the base that is most frequent across evolutionary history.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/paste-18.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure 4.3</strong> The mouse genome, with sections of the human genome (color-coded) that are largely preserved across evolution (Figure 3 in <span class="citation" data-cites="initial2002">(<a href="references.html#ref-initial2002" role="doc-biblioref"><span>“Initial Sequencing and Comparative Analysis of the Mouse Genome”</span> 2002</a>)</span>)</figcaption>
</figure>
</div>
<p>In training our GPNBert model with auxiliary sequences, we train by <strong>masking the human base only</strong> (following <span class="citation" data-cites="Benegas2024">(<a href="references.html#ref-Benegas2024" role="doc-biblioref">Benegas et al. 2023</a>)</span>). This very obviously and dramatically improves base prediction, and does so very quickly. After a few hundred iterations (trained on about 5,000 genes), the model learns that it should just assign the base most often found in other species. But as the evaluations in the original GPN-MSA paper make clear, eventually the model learns more than that. The model outperforms just picking the consensus base across species. They are able to show fairly convincingly that the predicted probabilities are a better predictor of the allele frequency in humans than just picking the allele frequency across species (Figure 2B in <span class="citation" data-cites="Benegas2024">Benegas et al. (<a href="references.html#ref-Benegas2024" role="doc-biblioref">2023</a>)</span>) as an estimate of the allele frequency within humans. Furthermore, their model is able to identify deleterious mutations better than CADD scores, based on supervised machine learning (<span class="citation" data-cites="rentzsch2021">(<a href="references.html#ref-rentzsch2021" role="doc-biblioref">Rentzsch et al. 2021</a>)</span>), or than sophisticated evolutionary constraint scores (<span class="citation" data-cites="sullivan2023">(<a href="references.html#ref-sullivan2023" role="doc-biblioref">Sullivan et al. 2023</a>)</span>).</p>
</section>
<section id="recap-of-our-approach" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="recap-of-our-approach"><span class="header-section-number">4.5</span> Recap of Our Approach</h2>
<p>In Chapter 2, we trained a vanilla BERT on DNA sequences alone — treating DNA as just another language. That model only had access to the human sequence, with no evolutionary context.</p>
<p>In this chapter, we’ve re-imagined that process. Instead of treating A, T, G, C, - as abstract symbols, leaving the model entirely unsupervised when picking embeddings, we inject evolutionary history directly into the embedding. This allows our model to:</p>
<ul>
<li>Use the aligned species data as a rich evolutionary prior.</li>
<li>Still leverage transformers for learning sequence motifs.</li>
<li>Predict masked human bases using both local sequence and cross-species evolutionary patterns.</li>
</ul>
</section>
<section id="preview-of-chapter-5" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="preview-of-chapter-5"><span class="header-section-number">4.6</span> Preview of Chapter 5</h2>
<p>In <strong>Chapter 5</strong>, we will put these two models — <strong>Vanilla BERT</strong> and <strong>GPN-BERT</strong> — to the test. We will evaluate their performance on:</p>
<ul>
<li>Predicting masked bases (MLM accuracy).</li>
<li>Predicting the functional impact of mutations.</li>
</ul>
<p>This head-to-head comparison will highlight the <strong>strengths and weaknesses</strong> of each approach and show the value of embedding <strong>evolutionary context directly into genomic language models</strong>.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Benegas2024" class="csl-entry" role="listitem">
Benegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2023. <span>“GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.”</span> <a href="http://dx.doi.org/10.1101/2023.10.10.561776">http://dx.doi.org/10.1101/2023.10.10.561776</a>.
</div>
<div id="ref-initial2002" class="csl-entry" role="listitem">
<span>“Initial Sequencing and Comparative Analysis of the Mouse Genome.”</span> 2002. <em>Nature</em> 420 (6915): 520–62. <a href="https://doi.org/10.1038/nature01262">https://doi.org/10.1038/nature01262</a>.
</div>
<div id="ref-jumper2021" class="csl-entry" role="listitem">
Jumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. <span>“Highly Accurate Protein Structure Prediction with AlphaFold.”</span> <em>Nature</em> 596 (7873): 583–89. <a href="https://doi.org/10.1038/s41586-021-03819-2">https://doi.org/10.1038/s41586-021-03819-2</a>.
</div>
<div id="ref-Lupo2022" class="csl-entry" role="listitem">
Lupo, Umberto, Damiano Sgarbossa, and Anne-Florence Bitbol. 2022. <span>“Protein Language Models Trained on Multiple Sequence Alignments Learn Phylogenetic Relationships.”</span> <em>Nature Communications</em> 13 (1). <a href="https://doi.org/10.1038/s41467-022-34032-y">https://doi.org/10.1038/s41467-022-34032-y</a>.
</div>
<div id="ref-rentzsch2021" class="csl-entry" role="listitem">
Rentzsch, Philipp, Max Schubach, Jay Shendure, and Martin Kircher. 2021. <span>“CADD-Splice<span></span>improving Genome-Wide Variant Effect Prediction Using Deep Learning-Derived Splice Scores.”</span> <em>Genome Medicine</em> 13 (1). <a href="https://doi.org/10.1186/s13073-021-00835-9">https://doi.org/10.1186/s13073-021-00835-9</a>.
</div>
<div id="ref-sullivan2023" class="csl-entry" role="listitem">
Sullivan, Patrick F., Jennifer R. S. Meadows, Steven Gazal, BaDoi N. Phan, Xue Li, Diane P. Genereux, Michael X. Dong, et al. 2023. <span>“Leveraging Base-Pair Mammalian Constraint to Understand Genetic Variation and Human Disease.”</span> <em>Science</em> 380 (6643). <a href="https://doi.org/10.1126/science.abn2937">https://doi.org/10.1126/science.abn2937</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="MichelNivard/Biological-language-models" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Chapter3_DNA.html" class="pagination-link" aria-label="Evaluating DNA Language Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluating DNA Language Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Chapter5_DNA.html" class="pagination-link" aria-label="Weaving Together Models">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Weaving Together Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>