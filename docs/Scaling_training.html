<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Scale up Training – Biological Language Models &amp; Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Chapter1_Proteins.html" rel="next">
<link href="./Chapter6_DNA.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e84559ba8659b1a571faa725acb99328.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./DNA.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Biological Language Models &amp; Neural Networks</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Scaling_training.html">Scale up Training</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Preamble1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">What is this Book About?</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Preamble2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to Read this Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Preamble3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The software stack</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">DNA</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter1_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preparing DNA data for training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter2_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Training our first DNA Language Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter3_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluating DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter4_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evolution-Aware Encoders</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter5_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Weaving Together Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter6_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">A Review of Current DNA Language Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Scaling_training.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Scale up Training</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Proteins</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter1_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Proteins: from sequence to structure</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter2_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Selecting and curating protein sequences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter3_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Training our first Protein Language Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter4_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Protein contact maps from attention maps</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter5_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Integrated protein diffusion language models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Multi-Modal-Biology</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#dont-try-to-win-the-compute-race" id="toc-dont-try-to-win-the-compute-race" class="nav-link active" data-scroll-target="#dont-try-to-win-the-compute-race">Don’t Try to Win the Compute Race</a></li>
  <li><a href="#focus-on-a-specific-question" id="toc-focus-on-a-specific-question" class="nav-link" data-scroll-target="#focus-on-a-specific-question">Focus on a specific question</a></li>
  <li><a href="#smart-architectures" id="toc-smart-architectures" class="nav-link" data-scroll-target="#smart-architectures">Smart Architectures</a></li>
  <li><a href="#get-most-out-of-your-gpu" id="toc-get-most-out-of-your-gpu" class="nav-link" data-scroll-target="#get-most-out-of-your-gpu">Get most out of your GPU</a>
  <ul class="collapse">
  <li><a href="#optimisation-steps-anyone-should-take" id="toc-optimisation-steps-anyone-should-take" class="nav-link" data-scroll-target="#optimisation-steps-anyone-should-take">Optimisation steps anyone should take</a></li>
  <li><a href="#batch-processing" id="toc-batch-processing" class="nav-link" data-scroll-target="#batch-processing">Batch processing</a></li>
  <li><a href="#lower-numerical-precision-quantize" id="toc-lower-numerical-precision-quantize" class="nav-link" data-scroll-target="#lower-numerical-precision-quantize">Lower numerical precision (quantize)</a></li>
  <li><a href="#optimizer-choice" id="toc-optimizer-choice" class="nav-link" data-scroll-target="#optimizer-choice">Optimizer choice</a></li>
  </ul></li>
  <li><a href="#parallel-training" id="toc-parallel-training" class="nav-link" data-scroll-target="#parallel-training">Parallel Training</a>
  <ul class="collapse">
  <li><a href="#different-kinds-of-parallel" id="toc-different-kinds-of-parallel" class="nav-link" data-scroll-target="#different-kinds-of-parallel">Different kinds of parallel</a></li>
  </ul></li>
  <li><a href="#aim-for-the-stars.." id="toc-aim-for-the-stars.." class="nav-link" data-scroll-target="#aim-for-the-stars..">Aim for the stars..</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Scale up Training</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter isn’t a part of the “DNA” section of the book, because the lessons are really quite general, but it comes after because we needed a little bit of experience with language model training before even considering training a serious model. This is also a somewhat awkward chapter for me to write, especially for the part of the readership that has a background in ML. See, I am a psychologist by training (though I have worked in genetic epidemiology for years and years), and while a lot of my academic work is fairly computational, I am not an expert in language model scaling by any means! Remember, the preamble to the book explains that this book is an account of me learning about biological language models and taking others along for the ride, not an authoritative text!</p>
<section id="dont-try-to-win-the-compute-race" class="level2">
<h2 class="anchored" data-anchor-id="dont-try-to-win-the-compute-race">Don’t Try to Win the Compute Race</h2>
<p>Among the DNA models I could find on Hugging Face are 7B parameter models like <a href="https://huggingface.co/genbio-ai/AIDO.DNA-7B" class="uri">https://huggingface.co/genbio-ai/AIDO.DNA-7B</a>. AIDO is trained on “256 H100 GPUs” in “8 days”. The training data consisted of 10.6 billion bases. That’s not even a particularly large model in the grand scheme of things, but if you consider a cost of approximately $2 per hour per H100, you are going to spend $100k. Obviously, there are academic compute resources you can get access to by appointment or based on fair use at your institute, university, or through collaborative national infrastructure, but even those are finite.</p>
<p>You have to consider feasibility. Today (March 2025), the Dutch national computer cluster for research (Snellius at SURF Sara) has 88 nodes with 4 H100 GPUs and 72 nodes with 4 A100 GPUs. TACC, the University of Texas at Austin compute provider, has approximately 80 A100 nodes (each with 3 GPUs). Those are two examples of reasonably well-funded HPC providers in academia. In my experience, you could get time reserved for your research at your local academic HPC provider at steep discounts, and these systems are likely large enough to train that 7B model I linked to. However, note how on either TACC or Snellius, 256 GPUs for 8 days would block the entire system for over a week. Perhaps you could apply for access to larger national research clusters, like <a href="https://www.bristol.ac.uk/news/2023/november/supercomputer-announcement.html">Isambard-AI</a> in the UK (being built in Bristol right now, a motivation for me to write this) which has 5,000 H100 GPUs. However, in general, it is likely you are going to be relatively limited by compute resources. Don’t be discouraged though—most breakthroughs are not going to be compute-based, and there are immense efficiency gains to be made that will level the playing field.</p>
</section>
<section id="focus-on-a-specific-question" class="level2">
<h2 class="anchored" data-anchor-id="focus-on-a-specific-question">Focus on a specific question</h2>
<p>It is tempting to train a model on all DNA in the known universe, but honestly, there is actually more DNA than people have even started training on. The models discussed so far often train on the reference sequence/genome, a sort of modal or consensus genome, but individual people’s genomes are different from that reference. You could consider thousands of species, or (tens/hundreds of) thousands of individual human genomes. That would require a lot of bioinformatics, which if your background is in bio might set you apart from other ML/AI researchers. You’d have to phase the genomes to untangle the maternal and paternal strand, you’d have to decide whether you want to get rid of the reference entirely and build a specific reference/genome for each individual, you might require some reference, or a graph genome? It’s also worth considering whether your task really requires the whole genome. Are you performing gene-centric tasks (mutation consequence prediction, gene expression prediction, alternative splice modeling)? If your specific tasks don’t require the whole genome, why not consider training on coding sequences only or the sequences of genes and a few thousand bases around them?</p>
</section>
<section id="smart-architectures" class="level2">
<h2 class="anchored" data-anchor-id="smart-architectures">Smart Architectures</h2>
<p>In Chapter 4, we studied smarter, DNA-specific model architectures. The GPN model inspired by <span class="citation" data-cites="Benegas2024">Benegas et al. (<a href="references.html#ref-Benegas2024" role="doc-biblioref">2023</a>)</span> that we introduced can outperform a standard BERT in an hour of training on my 2022 MacBook Air (the BERT we trained and compared to our GPN-BERT trained for approximately 8 hours on a strong GPU). The massive efficiency gain may mean you can beat the 7B BERT-like model we took as an example of compute costs with a fraction of the compute! As briefly remarked on in Chapter 6, researchers have designed alternatives for the transformer module in order to expand its context window up to 1 million bases, with far less compute requirement than the transformer <span class="citation" data-cites="nguyen2023">(<a href="references.html#ref-nguyen2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. If you are to design and run your own model, it will likely pay off if you implement some of these architectural innovations.</p>
</section>
<section id="get-most-out-of-your-gpu" class="level2">
<h2 class="anchored" data-anchor-id="get-most-out-of-your-gpu">Get most out of your GPU</h2>
<p>There is a healthy culture of extreme optimization. A good early example of this is the paper “Craamming: training a language model on a Single GPU in a Day” <span class="citation" data-cites="geiping2022">(<a href="references.html#ref-geiping2022" role="doc-biblioref">Geiping and Goldstein 2022</a>)</span>. Other neat examples are this GitHub repo of repeated attempts at training a GPT-2 equivalent (the OG OpenAI model that sort of set the LLM hype cycle in motion) as fast as possible (now in under 3 minutes on 8 H100 GPUs) (<a href="https://github.com/KellerJordan/modded-nanogpt" class="uri">https://github.com/KellerJordan/modded-nanogpt</a>). Some of the innovation people made cramming these models won’t generalize to your model, but consider giving their Muon optimizer a go (for GPT-2 it’s a serious efficiency gain), spend time finding optimal learning rates, or consider spending a few extra days/weeks cleaning data. If you do all these things before your big run, it’ll save some serious compute, which means you can push more data through the model in the same compute budget.</p>
<section id="optimisation-steps-anyone-should-take" class="level3">
<h3 class="anchored" data-anchor-id="optimisation-steps-anyone-should-take">Optimisation steps anyone should take</h3>
<p>Optimization doesn’t have to be a full-time job though, there are some easy steps anyone can take to get more training out of the same hardware. Full writeups on simple optimizations are found <a href="https://huggingface.co/docs/transformers/en/perf_train_gpu_one">here</a>, <a href="https://www.digitalocean.com/community/tutorials/find-optimal-batch-size">here</a> and <a href="https://neptune.ai/blog/optimizing-gpu-usage-during-model-training-with-neptune">here</a>. But I’ll cover the low-hanging fruit right here. Follow these steps and you’ll likely get nearly the same results in half the compute.</p>
</section>
<section id="batch-processing" class="level3">
<h3 class="anchored" data-anchor-id="batch-processing">Batch processing</h3>
<p>Transformers are designed with batch processing in mind. All the weight matrices have an extra (second or third, depending on which matrix) dimension to hold multiple sequences in parallel and apply the training step over all of them. We can very easily change the batch size in the training arguments:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(..., </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                                  per_device_train_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>                                  ...,)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you increase the batch size too much, you’ll have a crash and an out-of-memory warning:</p>
<pre><code>RuntimeError: CUDA out of memory. Tried to allocate X MiB ...</code></pre>
<p>We do so because many elements of a GPU, tensor core, shader units, CUDA cores come in powers of 2, and if your batch is 17, and you happen to have 16 tensor cores (or whatever element in the stack), that means processing 16, then 1, then 16 etc. You can go all the way until you run out of memory, but I wouldn’t. Training is bound by the limits of compute (as of writing, perhaps NVIDIA or AMD innovates rapidly in 2025, and this might change). So, once you find a batch size that hits 100% GPU use during training (you can check with the <code>nvidia-smi</code> command line tool or <code>rocm-smi</code> for AMD GPUs).</p>
</section>
<section id="lower-numerical-precision-quantize" class="level3">
<h3 class="anchored" data-anchor-id="lower-numerical-precision-quantize">Lower numerical precision (quantize)</h3>
<p>Lower numerical precision. Numbers are stored in 32-bit, which effectively means you have 6-9 significant digits, and the number can be zero or can range from <span class="math inline">\(-3.4*10^{38}\)</span> to <span class="math inline">\(-1.2*10^{-38}\)</span>, or from <span class="math inline">\(1.2*10^{-38}\)</span> to <span class="math inline">\(3.4*10^{38}\)</span>. It’s not entirely uncommon for scientific computations to run into numbers that can’t be represented well in 32-bits, but in order to speed up large models, people have actually gone down to <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">16-bit numbers</a>.</p>
<p>In <code>transformers</code> training arguments you can specify 16-bit training, but with 32-bit parameter accumulation (storage) of results where higher precision is needed, so-called mixed precision training with a simple command:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(..., </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>                                  fp16<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                                  ...,)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Or if you have a GPU capable of it (and most are) you can use the more efficient <code>bf16</code> mixed precision, which has worse precision than <code>fp16</code> but more dynamic range (can represent a larger range of values).</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(..., </span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>                                  bf16<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>                                  ...,)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Finally, on NVIDIA GPUs, you can use some serious dark magic: <code>tf32</code>, which is actually a 19-bit number (it drops precision but keeps dynamic range), which for most purposes is as precise as <code>fp32</code>. In many versions of PyTorch and transformers, <code>tf32</code> is automatically enabled. But, if you work on a cluster with older versions of PyTorch pre-compiled, you can manually activate <code>tf32</code>, and you can combine it with <code>bf16</code> for mixed precision training:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>torch.backends.cuda.matmul.allow_tf32 <span class="op">=</span> <span class="va">True</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(..., </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>                                  bf16<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>                                  tf32<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>                                  ...,)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The combination of <code>bf16</code> and <code>tf32</code> can result in 4x to 12x speedups, though as <code>tf32</code> is often already activated as a default the advantage is baked in. There are ways to take this further, use 8-bit numerical representations on modern hardware. The advantage of 8-bit models is greatest for really big models, if you have the means and skills to train those kinds of models, you have no need for this book.</p>
<p><strong>Final recommendation:</strong> use <code>bf16</code> with <code>tf32</code> unless you are training models that are so large as to require 8-bit data formats, unheard of in biological deep learning so far.</p>
</section>
<section id="optimizer-choice" class="level3">
<h3 class="anchored" data-anchor-id="optimizer-choice">Optimizer choice</h3>
<p>We haven’t really spoken about the optimizer itself, the default optimizers used these days are almost always variations of <code>adam</code> <span class="citation" data-cites="kingma2014">(<a href="references.html#ref-kingma2014" role="doc-biblioref">Kingma and Ba 2014</a>)</span>. Adam, and a modern implementation of it, like <code>adamW</code> are workhorses. Because they store a rolling average of recent gradients, they are robust if the gradients are very noisy. Because they don’t rely on 2nd order derivatives they are able to deal with very large models, and they are relatively efficient (when optimizing a model with <code>adamW</code> we store 8 bits per parameter). Papers that promise to beat <code>adamW</code>, to then fail, are a bit of a running gag in machine learning. However, you can in fact do better, both in terms of speed and in terms of memory utilization.</p>
<p><u><strong><code>adafactor</code>:</strong></u> adafactor is slower, but far more memory efficient. Where <code>adam</code> stores rolling averages of the gradients of weight matrices, <code>adafactor</code> stores row and column averages of those, meaning it only requires 4 bytes per parameter, significantly reducing the memory usage. This optimizer is a drop-in replacement and while slightly less efficient (takes longer to get to the same minimum) a great option if you are short on memory.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(..., </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>                                  optim<span class="op">=</span><span class="st">"adafactor"</span>, </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>                                  ...)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><u><strong><code>paged_adam_8bit</code>:</strong></u> Bits and Bytes is a library that deeply compresses certain optimizer states to 8-bits during training and actually even further during fine-tuning or inferences. It’s less of a direct drop-in replacement but it’s fast, almost as fast as <code>adamw</code>. If you are really memory-bound (say you have a 12Gb or 24Gb GPU but bigger ambitions) then this can be an option. The integration section of the manual is found <a href="https://huggingface.co/docs/bitsandbytes/v0.45.4/integrations">here</a>.</p>
<p><u><strong><code>muon</code></strong></u><strong>:</strong> You can refer to <a href="https://kellerjordan.github.io/posts/muon/">this writeup</a>for details of the optimizer. It’s a bit of a weird optimizer, as it’s meant ONLY for the inner layers of a transformer, the first and the last layer are still optimized with <code>adamW</code>. This also means you’ll need to write your own code, this certainly isn’t a simple case of dropping an argument into Trainer. The advantage of <code>muon</code> is that it’s both faster, and the loss drops more steeply. In other words, it learns more per token than other optimizers. This is actually very relevant from protein and DNA language models. As we’ll learn in the next few chapters on protein language models, there is way less usable data in the biological sphere than there is for natural language models. Facebook <code>LLama</code> is trained on 15 Trillion tokens, the largest protein language models on 780 billion tokens, which required including massive amounts of meta-genomic, and synthetic protein data. We are running out of data, and considering an optimizer that squeezes a little more out of each amino-acid might be worth your time! To implement <code>moun</code> you’d need to apply it to all 2D layers, and apply a separate optimizer to all other (1D) layers, from the GitHub:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.90, 0.95), weight_decay=0.01)</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> muon <span class="im">import</span> Muon</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Find ≥2D parameters in the body of the network -- these should be optimized by Muon</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>muon_params <span class="op">=</span> [p <span class="cf">for</span> p <span class="kw">in</span> model.body.parameters() <span class="cf">if</span> p.ndim <span class="op">&gt;=</span> <span class="dv">2</span>]</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Find everything else -- these should be optimized by AdamW</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>adamw_params <span class="op">=</span> ([p <span class="cf">for</span> p <span class="kw">in</span> model.body.parameters() <span class="cf">if</span> p.ndim <span class="op">&lt;</span> <span class="dv">2</span>]</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>              <span class="op">+</span> [<span class="op">*</span>model.head.parameters(), <span class="op">*</span>model.embed.parameters()])</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the optimizer</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>optimizers <span class="op">=</span> [Muon(muon_params, lr<span class="op">=</span><span class="fl">0.02</span>, momentum<span class="op">=</span><span class="fl">0.95</span>, rank<span class="op">=</span><span class="dv">0</span>, world_size<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>              torch.optim.AdamW(adamw_params, lr<span class="op">=</span><span class="fl">3e-4</span>, betas<span class="op">=</span>(<span class="fl">0.90</span>, <span class="fl">0.95</span>), weight_decay<span class="op">=</span><span class="fl">0.01</span>)]</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co"># in the training step</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> opt <span class="kw">in</span> optimizers:</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    opt.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The authors mention that <code>muon</code> isn’t tested for fine-tuning, and they don’t think it’ll work well with small batches.</p>
<p><strong>Final recommendation:</strong> Stick with <code>adamW</code> unless you are training a mid-size (&gt; 300m parameters) foundational model and risk running out of training data.</p>
</section>
</section>
<section id="parallel-training" class="level2">
<h2 class="anchored" data-anchor-id="parallel-training">Parallel Training</h2>
<p>After you have squeezed every drop out of your GPUs, the next performance step is training a model on multiple GPUs. Huggingface has a <a href="https://huggingface.co/docs/transformers/v4.49.0/en/perf_train_gpu_many">great read-me on training across multiple GPUs</a> you should check out, below I cover the basics. If you stick with Huggingface <code>transformers</code> library it’s actually quite simple! Their <code>accelerate</code> library enables us to launch a single script across multiple GPUs in one machine, or even multiple GPUs in multiple machines. It’s truly as simple as writing a training script (say <code>train.py</code>) and launching it with the accelerate command line tool from the terminal:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">accelerate</span> launch <span class="at">--multi-gpu</span> train.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If your model is a model that comes with the Huggingface <code>transformers</code> library this command should just find the GPUs, and get going. Though you’ll very likely want a bit more control. In my case, for example, this command kept trying to start a job on my 2 GPUs, AND on the internal GPU in the computer’s CPU package… That built-in GPU is of a different nature altogether and would only slow things down, so I had to specify the specific two GPUs I wanted to use:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="va">CUDA_VISIBLE_DEVICES</span><span class="op">=</span><span class="st">"0,1"</span> <span class="ex">accelerate</span> launch {script_name.py} </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This level of control can also come in handy if you want separate GPUs to run separate experiments. It is remarkable how plug-and-play <code>accelerate</code> really is. If you have one machine with multiple GPUs, and your model fits in the GPUs’ memory. In a few test runs on a cloud machine with 8 A100 GPUs, or 8 H100 GPUs I got my Protein language model running across 8 GPUs, munging through 500+ proteins a second (so ±500,000 tokens a second) without any script modifications.</p>
<section id="different-kinds-of-parallel" class="level3">
<h3 class="anchored" data-anchor-id="different-kinds-of-parallel">Different kinds of parallel</h3>
<section id="data-parallelism" class="level4">
<h4 class="anchored" data-anchor-id="data-parallelism">data parallelism</h4>
<p>There are a few different kinds of parallelism. There are data parallel (DP) and distributed data parallel (DDP), where each GPU has a copy of the model, and a unique batch of data (this is why the trainer argument is: <code>per_device_train_batch_size</code> each GPU gets a batch) and the loss, and gradients are averaged or pooled across GPUs after each training step.</p>
<p>Data parallelism requires your model, and a reasonable batch of data to fit on each GPU. If they do, then DDP and DP are the most straightforward option. If you model doesn’t fit your GPU (and large models frequently won’t!) you’ll need a different strategy. If you need to pick between DP and DDP consider that DP is less communication-heavy but slightly less optimal. So if you have 4 GPUs on a slow motherboard PCIe connection, I’d go with DP, but if you have the GPUs linked via NVlink or similar high-speed card-to-card connections, then DDP might be faster. You could inquire which might be best for you with your HPC team, but I find it easier to just try DDP and DP and see which is fastest over a 100-500 step trial run.</p>
<p>The most advanced version of data parallelism is “Fully sharded data parallelism” where a copy of the model isn’t kept on each GPU but the model is distributed across all GPUs, minimizing memory use, but increasing communication overhead. It’s a great option for large models on modern GPUs with fast (nvlink) interconnects) you can read more about its implementation in <code>accelerate</code> <a href="https://huggingface.co/docs/transformers/v4.49.0/en/fsdp">here</a>.</p>
</section>
<section id="modeltensor-parallelism" class="level4">
<h4 class="anchored" data-anchor-id="modeltensor-parallelism">model/tensor parallelism</h4>
<p>Once a model exceeds the size of the GPU, you have no choice but to distribute layers of the some across multiple GPUs. you could do so naively, say layer 1-4 GPU1, 5-8 GPU2 etc etc. The GPUs would have to wait for the GPU before it to finish, meaning you’d have a log of GPUs idling waiting for layers to be called on. But usually, there are smarter ways to pack things, companies like Google pioneered <a href="https://research.google/blog/introducing-gpipe-an-open-source-library-for-efficiently-training-large-scale-neural-network-models/">Gpipe</a>. There are <a href="https://pytorch.org/tutorials/intermediate/TP_tutorial.html">Pytorch</a> and Huggingface tutorials on model parallel training.</p>
<p><strong>Final recommendation:</strong> If you are in the audience for this book, then you might be ready for training, or fine-tuning a model in the 200m to 600m parameter range. You should be able to do that on a single machine with 4 or ideally 8 powerful GPUs. Stick with data parallel strategies that are easy to implement while you figure out the thousands of other choices you need to make to arrive at an optimal model. The reason to work your way up from smaller to larger models. The reason to start with smaller models is that there are always new surprises behind each corner. I just trained a protein language model where the data become progressively richer for mammalian proteins, and considered increasingly similar proteins. I thought that would enhance some application on human proteins, but it didn’t in a 45 million parameter and then 70 million parameter model. Had I trained a 300m parameter model on commercial hardware as I intended to without testing, I would have been out $1500 and would have ended up with a “meh” model at best. I did the math on training a 150M or 300M model on 8Xa6000 GPUs, or 8XA100 GPUs. In ~40 hours, and using the suggestions outlined above, you could train a near cutting-edge DNA/protein language model on one of these machines, and more powerful 8xH100 machines are coming online everywhere. Distributed data parallel training across 2,4 or 8 GPUs in one machine should be sufficient for the kind of learning you need to do at this stage, once you are ready to scale further there are serious resources.</p>
</section>
</section>
</section>
<section id="aim-for-the-stars.." class="level2">
<h2 class="anchored" data-anchor-id="aim-for-the-stars..">Aim for the stars..</h2>
<p>Should you get to the point where you exceed what’s feasible on a single very powerful machine with 8 GPUs, then you can move to a cluster. Tools like <code>accelerate</code> will work across multiple machines, and that is likely sufficient compute for almost anyone. But, if you ever get to this stage it’s good to read up on “ultra-scale’ training there are two great manuals you should flick through.</p>
<p>Google deep mind <a href="https://jax-ml.github.io/scaling-book/index">wrote a guide</a> to scaling large language models on TPU/Jax, but it covers all the math and concepts you’ll need in any framework in great detail.</p>
<p>Huggingface wrote an <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=memory_usage_in_transformers">interactive playbook</a> on ultra-scale training which is more practical.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Benegas2024" class="csl-entry" role="listitem">
Benegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2023. <span>“GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.”</span> <a href="http://dx.doi.org/10.1101/2023.10.10.561776">http://dx.doi.org/10.1101/2023.10.10.561776</a>.
</div>
<div id="ref-geiping2022" class="csl-entry" role="listitem">
Geiping, Jonas, and Tom Goldstein. 2022. <span>“Cramming: Training a Language Model on a Single GPU in One Day.”</span> <a href="https://doi.org/10.48550/ARXIV.2212.14034">https://doi.org/10.48550/ARXIV.2212.14034</a>.
</div>
<div id="ref-kingma2014" class="csl-entry" role="listitem">
Kingma, Diederik P., and Jimmy Ba. 2014. <span>“Adam: A Method for Stochastic Optimization.”</span> <a href="https://doi.org/10.48550/ARXIV.1412.6980">https://doi.org/10.48550/ARXIV.1412.6980</a>.
</div>
<div id="ref-nguyen2023" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. <span>“HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.”</span> <a href="https://doi.org/10.48550/ARXIV.2306.15794">https://doi.org/10.48550/ARXIV.2306.15794</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="MichelNivard/Biological-language-models" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Chapter6_DNA.html" class="pagination-link" aria-label="A Review of Current DNA Language Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">A Review of Current DNA Language Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Chapter1_Proteins.html" class="pagination-link" aria-label="Proteins: from sequence to structure">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Proteins: from sequence to structure</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>