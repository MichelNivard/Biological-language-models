<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Training our first DNA Language Model – Sequence Language Models &amp; Deep Learning in Genomics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Chapter3_DNA.html" rel="next">
<link href="./Chapter1_DNA.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e84559ba8659b1a571faa725acb99328.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./DNA.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Sequence Language Models &amp; Deep Learning in Genomics</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Chapter1_DNA.html">DNA</a></li><li class="breadcrumb-item"><a href="./Chapter2_DNA.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Training our first DNA Language Model</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Preamble1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">What is this Book About?</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Preamble2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to Read this Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Preamble3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The software stack</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Preamble4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Structure of an ML model</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">DNA</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter1_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preparing DNA data for training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter2_DNA.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Training our first DNA Language Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter3_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluating DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter4_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evolution-Aware Encoders</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter5_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Comparing Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter6_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">A Review of Current DNA Language Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Scaling_training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Scale up Training</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Proteins</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter1_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Proteins: from sequence to structure</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter2_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Selecting and curating protein sequences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter3_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Training our first Protein Language Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter4_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Protein contact maps from attention maps</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter5_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Integrated protein diffusion language models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Multi-Modal-Biology</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">2.1</span> Introduction</a></li>
  <li><a href="#why-would-we-train-dna-language-models" id="toc-why-would-we-train-dna-language-models" class="nav-link" data-scroll-target="#why-would-we-train-dna-language-models"><span class="header-section-number">2.2</span> Why would we train DNA language models?</a></li>
  <li><a href="#understanding-tokenization" id="toc-understanding-tokenization" class="nav-link" data-scroll-target="#understanding-tokenization"><span class="header-section-number">2.3</span> Understanding Tokenization</a>
  <ul class="collapse">
  <li><a href="#what-is-a-tokenizer" id="toc-what-is-a-tokenizer" class="nav-link" data-scroll-target="#what-is-a-tokenizer"><span class="header-section-number">2.3.1</span> What is a Tokenizer?</a></li>
  <li><a href="#our-dna-tokenizer" id="toc-our-dna-tokenizer" class="nav-link" data-scroll-target="#our-dna-tokenizer"><span class="header-section-number">2.3.2</span> Our DNA Tokenizer</a></li>
  <li><a href="#other-tokenization-strategies-for-dna-rna-and-proteins" id="toc-other-tokenization-strategies-for-dna-rna-and-proteins" class="nav-link" data-scroll-target="#other-tokenization-strategies-for-dna-rna-and-proteins"><span class="header-section-number">2.3.3</span> Other Tokenization Strategies for DNA, RNA, and Proteins</a></li>
  </ul></li>
  <li><a href="#loading-and-tokenizing-the-dna-dataset" id="toc-loading-and-tokenizing-the-dna-dataset" class="nav-link" data-scroll-target="#loading-and-tokenizing-the-dna-dataset"><span class="header-section-number">2.4</span> Loading and Tokenizing the DNA Dataset</a>
  <ul class="collapse">
  <li><a href="#understanding-the-dataset" id="toc-understanding-the-dataset" class="nav-link" data-scroll-target="#understanding-the-dataset"><span class="header-section-number">2.4.1</span> Understanding the Dataset</a></li>
  <li><a href="#tokenizing-the-dataset" id="toc-tokenizing-the-dataset" class="nav-link" data-scroll-target="#tokenizing-the-dataset"><span class="header-section-number">2.4.2</span> Tokenizing the Dataset</a></li>
  <li><a href="#saving-and-preparing-the-dataset-for-training" id="toc-saving-and-preparing-the-dataset-for-training" class="nav-link" data-scroll-target="#saving-and-preparing-the-dataset-for-training"><span class="header-section-number">2.4.3</span> Saving and Preparing the Dataset for Training</a></li>
  </ul></li>
  <li><a href="#understanding-bert-and-masked-language-modeling-mlm" id="toc-understanding-bert-and-masked-language-modeling-mlm" class="nav-link" data-scroll-target="#understanding-bert-and-masked-language-modeling-mlm"><span class="header-section-number">2.5</span> Understanding BERT and Masked Language Modeling (MLM)</a>
  <ul class="collapse">
  <li><a href="#what-is-bert" id="toc-what-is-bert" class="nav-link" data-scroll-target="#what-is-bert"><span class="header-section-number">2.5.1</span> What is BERT?</a></li>
  <li><a href="#what-is-masked-language-modeling-mlm" id="toc-what-is-masked-language-modeling-mlm" class="nav-link" data-scroll-target="#what-is-masked-language-modeling-mlm"><span class="header-section-number">2.5.2</span> What is Masked Language Modeling (MLM)?</a></li>
  <li><a href="#understanding-the-model-transformer-layers-attention-heads-and-hidden-size" id="toc-understanding-the-model-transformer-layers-attention-heads-and-hidden-size" class="nav-link" data-scroll-target="#understanding-the-model-transformer-layers-attention-heads-and-hidden-size"><span class="header-section-number">2.5.3</span> Understanding the model: Transformer Layers, Attention Heads, and Hidden Size</a></li>
  <li><a href="#defining-the-bert-model-for-dna-sequences" id="toc-defining-the-bert-model-for-dna-sequences" class="nav-link" data-scroll-target="#defining-the-bert-model-for-dna-sequences"><span class="header-section-number">2.5.4</span> Defining the BERT Model for DNA Sequences</a></li>
  <li><a href="#configuring-training-for-dna-bert" id="toc-configuring-training-for-dna-bert" class="nav-link" data-scroll-target="#configuring-training-for-dna-bert"><span class="header-section-number">2.5.5</span> Configuring Training for DNA BERT</a></li>
  <li><a href="#setting-training-parameters" id="toc-setting-training-parameters" class="nav-link" data-scroll-target="#setting-training-parameters"><span class="header-section-number">2.5.6</span> Setting Training Parameters</a></li>
  <li><a href="#preparing-for-masked-language-modeling-mlm" id="toc-preparing-for-masked-language-modeling-mlm" class="nav-link" data-scroll-target="#preparing-for-masked-language-modeling-mlm"><span class="header-section-number">2.5.7</span> Preparing for Masked Language Modeling (MLM)</a></li>
  <li><a href="#training-the-dna-bert-model" id="toc-training-the-dna-bert-model" class="nav-link" data-scroll-target="#training-the-dna-bert-model"><span class="header-section-number">2.5.8</span> Training the DNA BERT Model</a></li>
  <li><a href="#saving-the-trained-model" id="toc-saving-the-trained-model" class="nav-link" data-scroll-target="#saving-the-trained-model"><span class="header-section-number">2.5.9</span> Saving the Trained Model</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">2.5.10</span> Summary</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Chapter1_DNA.html">DNA</a></li><li class="breadcrumb-item"><a href="./Chapter2_DNA.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Training our first DNA Language Model</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-DNALM" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Training our first DNA Language Model</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Abstract
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this chapter, we’ll train a DNA language model. To do a full training run, you’ll need a newer MacBook (±24 hours), a gaming PC with a GPU (10-12 hours), or Google Colab with the A100 GPU (±6 hours).</p>
<p>A Google Colab notebook to train the model in the cloud (for cheap/free) is <a href="https://colab.research.google.com/drive/1tmO8Xe8j2Ivwiec636Jy3624KDL7eMJr?usp=sharing">available here</a>.</p>
<p>If you don’t have the compute to run your own training run, you can download a trained version of <a href="https://huggingface.co/MichelNivard/DNABert-CDS-13Species-v0.1">DNABert</a> for use in the next few chapters.</p>
<p>All scripts for this chapter are found here: <a href="https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/DNA/Chapter_2" class="uri">https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/DNA/Chapter_2</a></p>
</div>
</div>
<section id="introduction" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">2.1</span> Introduction</h2>
<p>Now that we have collected and formatted a DNA dataset in Chapter 1, we can proceed to train a language model using the (Modern)BERT model architecture. In this chapter, we will walk through the process of tokenizing DNA sequences, configuring a BERT model, and training it using the <strong>Masked Language Modeling (MLM)</strong> objective.</p>
<p>We will cover the utility and rationale behind DNA language models, and we’ll dive into the key concepts behind tokenization. Then we’ll disucss key parts of the acrchitecture the BERT model, and the logicl of masked language model(MLM) training, before diving into the Python script that trains the actual model. Note that while we briefly discuss key elements of the model architecture like attention we dont get into the nitty gritty just jett to make sure the chapters arent to dense, we’ll discuss individual parts fo the transformer model achrchitecture in far greater detail in later chapters.</p>
</section>
<section id="why-would-we-train-dna-language-models" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="why-would-we-train-dna-language-models"><span class="header-section-number">2.2</span> Why would we train DNA language models?</h2>
<p>For a full review of the utility of language models, you should dig into the literature. I can recommend <span class="citation" data-cites="benegas2025">(<a href="references.html#ref-benegas2025" role="doc-biblioref">Benegas et al. 2025</a>)</span> for example. Genomic language models (gLMs) apply AI techniques to DNA sequences, enabling breakthroughs in variant effect prediction, sequence design, and genomic analysis. After we learn about DNA language model we’ll briefly review the current offerings available in <a href="Chapter6_DNA.html" class="quarto-xref"><span>Chapter 6</span></a>.</p>
<p>Like larger language models like ChatGPT, DNA language models (DNA-LM) have “emergent properties”. If you train an DNA-LM, or a genomic Language models (gLM). which strictly speaking a slightly more general class of models than DNA-LMs, so also RNA or protein LMs, on the reference genome sequence of humans and various other species, then that model is able to detect damaging mutations. It can do so without ever being trained on mutations explicitly as damaging mutations are very unlikely to occur across the many references genomes on which the model is trained<span class="citation" data-cites="benegas2023">(<a href="references.html#ref-benegas2023" role="doc-biblioref">Benegas, Batra, and Song 2023</a>)</span>. To assess functional constraints, a widely used metric is the <strong>log-likelihood ratio (LLR)</strong>, the ratio of log likelihoods of the various alleles given the model. This measures the probability of a nucleotide variant appearing in a given context, with lower probabilities indicating potential deleterious effects. This application will be one of the examples I use throughout, simply because my experience in genetics aligns with it.</p>
<p>Another key application are types of “transfer learning”, where pre-trained DNA-LMs improve predictions of gene expression, regulatory sequence structure, or chromatin accessibility of a section of DNA in a specific tissue. However, training effective models is difficult due to the vast, complex, and often non-functional nature of genomes. Unlike protein models, DNA-LMs struggle with limited genomic diversity in training data and require sophisticated benchmarks for evaluation.</p>
<p>Current state of the art models<span class="citation" data-cites="nguyen2024">(<a href="references.html#ref-nguyen2024" role="doc-biblioref">Nguyen et al. 2024</a>)</span> are trained on hundreds of thousands of genomes and focus on improving long-range genomic interactions (i.e.&nbsp;are able to generate realistic genome segments of &gt; 1 million bases long), integrating multi-modal biological data (are effectively joint DNA/RNA/Protein models), and refining sequence design for practical applications. DNA-LM or gLMs hold great promise for revolutionizing genome research, advancing genetic disease understanding, and enabling synthetic biology innovations.</p>
</section>
<section id="understanding-tokenization" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="understanding-tokenization"><span class="header-section-number">2.3</span> Understanding Tokenization</h2>
<section id="what-is-a-tokenizer" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="what-is-a-tokenizer"><span class="header-section-number">2.3.1</span> What is a Tokenizer?</h3>
<p>A <strong>tokenizer</strong> is a fundamental component of any language model. Language models are essentially large multinomial models that predict the next token in a sequence based on previous tokens, or predict a masked token using the surrounding tokens. Since machine learning models operate on numerical representations, tokenized text must be converted into integers, which serve as indices for lookup in a vocabulary.</p>
<p>These integers, however, <strong>have no inherent numeric value</strong>—they simply act as categorical labels that the model learns to associate with semantic meaning. For example, if we tokenize the sentence:</p>
<blockquote class="blockquote">
<p>“The quick brown fox jumps over the lazy dog”</p>
</blockquote>
<p>at the <strong>word level</strong>, we might obtain a numerical sequence like:</p>
<blockquote class="blockquote">
<p><code>[4, 123, 678, 89, 245, 983, 56, 4564]</code></p>
</blockquote>
<p>where each number corresponds to a word based on a pre-defined tokenization dictionary, such as:</p>
<pre><code>{"the": 4, "quick": 123, "brown": 678, "fox": 89, "jumps": 245, "over": 983, "lazy": 56, "dog": 4564}</code></pre>
<p>Similarly, for DNA sequences, each nucleotide (A, T, C, G) is assigned a unique integer ID.</p>
</section>
<section id="our-dna-tokenizer" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="our-dna-tokenizer"><span class="header-section-number">2.3.2</span> Our DNA Tokenizer</h3>
<p>Our tokenizer uses a <strong>character-level</strong> approach, where each nucleotide is assigned a unique integer ID. Special tokens are also included for various purposes:</p>
<ul>
<li><code>[UNK]</code> (unknown token)</li>
<li><code>[PAD]</code> (padding token for equal-length sequences)</li>
<li><code>[CLS]</code> (classification token, useful for downstream tasks)</li>
<li><code>[SEP]</code> (separator token, used in tasks like sequence-pair classification)</li>
<li><code>[MASK]</code> (used for masked language modeling training)</li>
</ul>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers <span class="im">import</span> Tokenizer</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers.models <span class="im">import</span> WordLevel</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers.pre_tokenizers <span class="im">import</span> Split</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> PreTrainedTokenizerFast</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. DNA Tokenizer with Full FASTA Nucleic Acid Code</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Define vocabulary to include all FASTA nucleotides and symbols</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>dna_vocab <span class="op">=</span> {</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A"</span>: <span class="dv">0</span>, <span class="st">"T"</span>: <span class="dv">1</span>, <span class="st">"C"</span>: <span class="dv">2</span>, <span class="st">"G"</span>: <span class="dv">3</span>, <span class="st">"N"</span>: <span class="dv">4</span>, <span class="st">"U"</span>: <span class="dv">5</span>, <span class="st">"i"</span>: <span class="dv">6</span>,  <span class="co"># Standard bases + Inosine</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"R"</span>: <span class="dv">7</span>, <span class="st">"Y"</span>: <span class="dv">8</span>, <span class="st">"K"</span>: <span class="dv">9</span>, <span class="st">"M"</span>: <span class="dv">10</span>, <span class="st">"S"</span>: <span class="dv">11</span>, <span class="st">"W"</span>: <span class="dv">12</span>,  <span class="co"># Ambiguous bases</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"B"</span>: <span class="dv">13</span>, <span class="st">"D"</span>: <span class="dv">14</span>, <span class="st">"H"</span>: <span class="dv">15</span>, <span class="st">"V"</span>: <span class="dv">16</span>,  <span class="co"># More ambiguity codes</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"-"</span>: <span class="dv">17</span>,  <span class="co"># Gap character</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"[UNK]"</span>: <span class="dv">18</span>, <span class="st">"[PAD]"</span>: <span class="dv">19</span>, <span class="st">"[CLS]"</span>: <span class="dv">20</span>, <span class="st">"[SEP]"</span>: <span class="dv">21</span>, <span class="st">"[MASK]"</span>: <span class="dv">22</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Create tokenizer</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer(WordLevel(vocab<span class="op">=</span>dna_vocab, unk_token<span class="op">=</span><span class="st">"[UNK]"</span>))</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>tokenizer.pre_tokenizer <span class="op">=</span> Split(<span class="st">""</span>, <span class="st">"isolated"</span>)  <span class="co"># Character-level splitting</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to Hugging Face-compatible tokenizer</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>hf_tokenizer <span class="op">=</span> PreTrainedTokenizerFast(</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    tokenizer_object<span class="op">=</span>tokenizer,</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    unk_token<span class="op">=</span><span class="st">"[UNK]"</span>,</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    pad_token<span class="op">=</span><span class="st">"[PAD]"</span>,</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    cls_token<span class="op">=</span><span class="st">"[CLS]"</span>,</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    sep_token<span class="op">=</span><span class="st">"[SEP]"</span>,</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    mask_token<span class="op">=</span><span class="st">"[MASK]"</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="other-tokenization-strategies-for-dna-rna-and-proteins" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="other-tokenization-strategies-for-dna-rna-and-proteins"><span class="header-section-number">2.3.3</span> Other Tokenization Strategies for DNA, RNA, and Proteins</h3>
<p>While character-level tokenization is effective, other tokenization approaches can offer different tradeoffs:</p>
<section id="byte-pair-encoding-bpe" class="level4" data-number="2.3.3.1">
<h4 data-number="2.3.3.1" class="anchored" data-anchor-id="byte-pair-encoding-bpe"><span class="header-section-number">2.3.3.1</span> Byte Pair Encoding (BPE)</h4>
<p>BPE is widely used in NLP and allows frequently occurring patterns to be merged into subword units. While BPE could be useful for repetitive genome sequences, it may not capture biologically meaningful units.</p>
</section>
<section id="k-mer-tokenization" class="level4" data-number="2.3.3.2">
<h4 data-number="2.3.3.2" class="anchored" data-anchor-id="k-mer-tokenization"><span class="header-section-number">2.3.3.2</span> K-mer Tokenization</h4>
<p>K-mer tokenization groups nucleotides into fixed-length substrings (e.g., 3-mers like “ATG”). This approach retains local sequence structure but can lead to a large vocabulary size.</p>
</section>
<section id="tiktoken-and-similar-models" class="level4" data-number="2.3.3.3">
<h4 data-number="2.3.3.3" class="anchored" data-anchor-id="tiktoken-and-similar-models"><span class="header-section-number">2.3.3.3</span> Tiktoken and Similar Models</h4>
<p>Some modern tokenization methods, such as Tiktoken, optimize speed and efficiency by precomputing merges. These are often optimized for large-scale NLP tasks but could be adapted for biological sequences.</p>
<p>Choosing the best tokenizer depends on the specific modeling task. For example, RNA secondary structures or protein folding models might benefit from more complex tokenization strategies.</p>
<p>Source: <a href="https://rpubs.com/yuchenz585/1161578">RPubs Tokenization Review</a></p>
</section>
</section>
</section>
<section id="loading-and-tokenizing-the-dna-dataset" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="loading-and-tokenizing-the-dna-dataset"><span class="header-section-number">2.4</span> Loading and Tokenizing the DNA Dataset</h2>
<section id="understanding-the-dataset" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="understanding-the-dataset"><span class="header-section-number">2.4.1</span> Understanding the Dataset</h3>
<p>We will use a pre-existing dataset, <strong>Human-genome-CDS-GRCh38</strong>, which contains coding sequences from the human genome.</p>
</section>
<section id="tokenizing-the-dataset" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="tokenizing-the-dataset"><span class="header-section-number">2.4.2</span> Tokenizing the Dataset</h3>
<p>To prepare the dataset for training, we must apply the tokenizer to each sequence while ensuring:</p>
<ul>
<li>Sequences are truncated or padded to a fixed length (512 tokens)</li>
<li>Unwanted columns are removed</li>
</ul>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>dataset_name <span class="op">=</span> <span class="st">"MichelNivard/Human-genome-CDS-GRCh38"</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(dataset_name)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>column_name <span class="op">=</span> <span class="st">"sequence"</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(examples):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hf_tokenizer(examples[column_name], truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="st">"max_length"</span>, max_length<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize dataset</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>tokenized_dataset <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize_function, batched<span class="op">=</span><span class="va">True</span>, remove_columns<span class="op">=</span>[column_name])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="saving-and-preparing-the-dataset-for-training" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="saving-and-preparing-the-dataset-for-training"><span class="header-section-number">2.4.3</span> Saving and Preparing the Dataset for Training</h3>
<p>Once tokenized, we save the dataset for efficient access during training.</p>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>tokenized_dataset.save_to_disk(<span class="st">"tokenized_dna_dataset"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="understanding-bert-and-masked-language-modeling-mlm" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="understanding-bert-and-masked-language-modeling-mlm"><span class="header-section-number">2.5</span> Understanding BERT and Masked Language Modeling (MLM)</h2>
<section id="what-is-bert" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="what-is-bert"><span class="header-section-number">2.5.1</span> What is BERT?</h3>
<p>BERT (<strong>Bidirectional Encoder Representations from Transformers</strong>) is a powerful transformer-based language model. Unlike traditional left-to-right models, BERT learns bidirectional context, allowing it to understand sequences more effectively. for genomics, bidirectional models have proven more effective, while for natural language it appears auto-regressive “next work prediction” models appear most effective.</p>
<p>Returning to our earlier example sentence:</p>
<blockquote class="blockquote">
<p>“The quick brown fox jumps over the lazy dog”</p>
</blockquote>
<p>BERT does not process words one at a time but instead considers the entire sequence simultaneously. This bidirectional approach allows BERT to infer missing words based on context from both directions. Te inference is effectively a predicting of a specific token, in genomics models a base or amino-acid, and since the predicting results in a probability for each possible token, this fairly naturally translates into predicting the probability of a mutation, or th probability of the presence of a regulatory sequence motif near a specific gene.</p>
</section>
<section id="what-is-masked-language-modeling-mlm" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="what-is-masked-language-modeling-mlm"><span class="header-section-number">2.5.2</span> What is Masked Language Modeling (MLM)?</h3>
<p>MLM is a self-supervised learning objective where the model learns by predicting missing tokens in a sequence. During training:</p>
<ul>
<li>Some tokens are randomly replaced with <strong><code>[MASK]</code></strong></li>
<li>The model must predict the original token based on surrounding context</li>
</ul>
<p>For example, if we mask the word “fox” in our sentence:</p>
<blockquote class="blockquote">
<p>“The quick brown <strong><code>[MASK]</code></strong> jumps over the lazy dog”</p>
</blockquote>
<p>BERT will analyze the remaining words and attempt to predict “fox.”</p>
<p>This technique enables BERT to learn useful representations without requiring labeled data.</p>
</section>
<section id="understanding-the-model-transformer-layers-attention-heads-and-hidden-size" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="understanding-the-model-transformer-layers-attention-heads-and-hidden-size"><span class="header-section-number">2.5.3</span> Understanding the model: Transformer Layers, Attention Heads, and Hidden Size</h3>
<p>We’ll briefly have to discuss the general architecture of the model, in bold the key elements of the model, in bold and italic parameters we get to set to determine the size of the model.</p>
<p>A transformer model, like those used for DNA, RNA, or protein sequences, starts with an <strong>embedding layer</strong>, which plays a critical role in converting raw tokens — such as individual nucleotides (A, T, C, G) or amino acids — into numerical <strong>vectors</strong> that the model can process. Each token is mapped to a high-dimensional vector that captures some initial information about its identity and, in more advanced models, even its biochemical properties. This embedding layer acts as the interface between the raw sequence data and the deeper transformer layers, ensuring that the model works with continuous mathematical representations rather than raw symbolic letters.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The embedding layers design is intimately related to the nature of the data, here we simply use a standard BERT embedding layer but in Chapter 4 &amp; 5 we’ll dive deep into researchers efforts to design embedding layers for DNA specifically.</p>
</div>
</div>
<p>A <strong>transformer layer</strong> consists of two key components: a <strong>self-attention mechanism</strong> (discussed in great detail <a href="Chapter4_Proteins.html" class="quarto-xref"><span>Chapter 10</span></a> <strong>)</strong> and a <strong>feed-forward neural network</strong>. The self-attention mechanism allows the model to dynamically weigh the importance of every other token in the sequence when predicting a given token, helping it learn relationships across different parts of the sequence — whether those are between neighboring amino acids or between distant regulatory elements in a long DNA strand. After the self-attention step, the <strong>feed-forward neural network</strong> processes each token’s representation independently, applying a small multi-layer perceptron (MLP) to transform the token’s internal representation. This helps the model refine and enrich the learned features, capturing nonlinear combinations of the attention-derived information. While self-attention captures interactions across the sequence, the feed-forward layer focuses on how to represent each token itself in a biologically meaningful way, helping the model identify local biochemical properties, sequence motifs, or structural preferences. The <strong>number of transformer layers</strong> (<strong><code>num_hidden_layers</code></strong>) determines how deep the model is, with more layers giving the model more capacity to learn complex biological relationships, but also increasing training time and data requirements.</p>
<p>Within each transformer layer, there are multiple <strong>attention heads</strong> (<strong><code>num_attention_heads</code></strong>), each focusing on different types of relationships within the data. In a natural language example, one attention head might capture subject-verb relationships, while another tracks adjective-noun pairs. In biological sequences, one attention head might learn to link binding motifs in promoters to transcription start sites, while another might focus on co-evolving residues in proteins that contribute to structural stability. The <strong>hidden size</strong> (<strong><code>hidden_size</code></strong>) refers to the dimensionality of these internal vector representations, defining how much information the model can store at each position. Larger hidden sizes allow the model to capture richer biological context, but they also increase computational cost. By combining deep transformer stacks, multiple attention heads, and flexible embeddings, biological language models can develop a powerful and nuanced understanding of biological sequences, helping researchers uncover new regulatory elements, predict protein folding, or study the effects of mutations.</p>
</section>
<section id="defining-the-bert-model-for-dna-sequences" class="level3" data-number="2.5.4">
<h3 data-number="2.5.4" class="anchored" data-anchor-id="defining-the-bert-model-for-dna-sequences"><span class="header-section-number">2.5.4</span> Defining the BERT Model for DNA Sequences</h3>
<p>While the “quick brown fox” example helps us understand how BERT processes natural language, our goal is to apply the same principles to DNA sequences. Instead of predicting missing words in a sentence, we want our model to learn biological patterns and genomic structure by predicting masked nucleotides within DNA sequences.</p>
<p>In DNA modeling, understanding sequence context is just as critical as in language modeling. Just as BERT learns that “fox” fits within a given sentence structure, our model should learn that specific nucleotide sequences appear in biologically meaningful patterns. This could involve recognizing gene coding regions, regulatory motifs, or conserved sequence elements across different genomes.</p>
<p>To accomplish this, we define a custom BERT model designed specifically for processing DNA sequences. Unlike traditional text-based models, our DNA BERT model uses a character-level vocabulary of nucleotides (A, T, C, G) and special tokens to represent sequence structure. By leveraging masked language modeling (MLM), the model will learn to predict missing nucleotides based on surrounding context, allowing it to capture meaningful genomic features.</p>
<p>The <strong><code>max_position_embeddings</code></strong> defines the longest sequence the model can process at once, which is crucial for biological sequences like genomes or proteins that can vary widely in length. To help the model understand where each token appears in the sequence, position embeddings are added to the token embeddings, giving the model a sense of order and distance, which is especially important when analyzing long-range interactions, like regulatory elements controlling distant genes.</p>
<p>With this in mind, let’s move forward and define a standard BERT architecture, which we’ll apply to DNA sequences. Because we’ll train a standard model, we can basically get away with defining the dimensions of certain aspects of the model. Recall in the chapter on the software stack we discussed the general outlien of a machinne learning model? We coudl write a script like that for a BERT model, but the huggingface <code>transformers</code> library proviced standard models, while the <code>Trainer</code> class abstracts away ahaving to write a training loop.</p>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> ModernBertConfig, ModernBertForMaskedLM</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> ModernBertConfig(</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="bu">len</span>(dna_vocab),</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    hidden_size<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    num_hidden_layers<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    num_attention_heads<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    intermediate_size<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    max_position_embeddings<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    type_vocab_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>config.pad_token_id <span class="op">=</span> dna_vocab[<span class="st">"[PAD]"</span>]</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ModernBertForMaskedLM(config)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The keenly eyed among you see a lot of powers of 2, 8 is 2^3, 256, 512 are also powers of two, etc. Computer memory encodes in bits and bytes, and is designed around powers of two. Building matrices that are powers of 2, 16, 32, 64, etc. makes them fit in memory more efficiently, and this can have serious consequences for training efficiency (see <strong>Figure 1</strong>).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/paste-19.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure 1</strong> the power of powers of two.</figcaption>
</figure>
</div>
</section>
<section id="configuring-training-for-dna-bert" class="level3" data-number="2.5.5">
<h3 data-number="2.5.5" class="anchored" data-anchor-id="configuring-training-for-dna-bert"><span class="header-section-number">2.5.5</span> Configuring Training for DNA BERT</h3>
<p>Now that we have defined our BERT model for DNA sequences, we need to set up the training process. This involves specifying various training hyperparameters, handling masked language modeling (MLM) data, and preparing for efficient learning.</p>
<p>Unlike traditional NLP tasks where models are trained on massive text corpora, training on DNA sequences presents unique challenges. The structure of genomic data, sequence length, and biological patterns all influence how the model learns. Our configuration is again rather general and not yet tuned to DNA or biological data. If you would scale this model, you’d likely have to drop the learning rate down, for example. In “production” hyperparameter optimization becomes super important when you train a large model on all your data; each run might be costly, and setting optimal hyperparameters can lead to serious gains in training results.</p>
</section>
<section id="setting-training-parameters" class="level3" data-number="2.5.6">
<h3 data-number="2.5.6" class="anchored" data-anchor-id="setting-training-parameters"><span class="header-section-number">2.5.6</span> Setting Training Parameters</h3>
<p>To train our DNA BERT model, we use the Hugging Face <strong><code>TrainingArguments</code></strong> class, which allows us to define key training settings. These include:</p>
<ul>
<li><strong>Batch size:</strong> We set a batch size (<strong><code>per_device_train_batch_size</code></strong>) of <code>16</code> for both training and evaluation. This determines how many sequences are processed at once.</li>
<li><strong>Logging &amp; Saving:</strong> We log loss every <code>50</code> steps (<strong><code>logging_steps</code></strong>) and save model checkpoints every <code>100</code> steps to monitor training progress.</li>
<li><strong>Learning Rate:</strong> We use a learning rate of <code>5e-5</code> (<strong><code>learning_rate</code></strong>), a common choice for transformer models that balances learning speed and stability.</li>
<li><strong>Weight Decay:</strong> A value of <code>0.01</code> is used to prevent overfitting by applying L2 regularization to model weights.</li>
<li><strong>Training Steps:</strong> The model is trained for <code>4000</code> steps (<code>max_steps</code>), though on the Google Colab code I ran in the cloud, I trained for 2 whole epochs over all data (<strong><code>num_train_epochs = 2</code></strong>), which is a more precise way to ensure the model sees all the data twice.</li>
<li><strong>Model Saving:</strong> The model checkpoints are stored in <code>./bert-dna</code>, allowing us to resume training from a checkpoint if needed (after a computer crash, or after the model going off the rails, etc.).</li>
</ul>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./bert-dna"</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    overwrite_output_dir<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">50</span>,  <span class="co"># Log loss every step</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    save_steps<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    save_total_limit<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    max_steps<span class="op">=</span><span class="dv">4000</span>,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">5e-5</span>,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    push_to_hub<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>,  <span class="co"># Disables wandb logging, can enable if you have a wandb account so you can track your training</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>While I have enabled it here, I can recommend tracking your training runs on wandb. Go to <a href="www.wandb.ai">wandb.ai</a> (w and b meaning weights and biases, the core parameters in AI models) to make a free account. Now to some extent, this is like Strava but for AI, and there is a risk of obsessing over the training metadata. But if you find yourself with a limited amount of compute, or expensive compute paid per minute, it makes a lot of sense to track big training runs in real time so you can intervene. If the run crashes, you can restart, or abort the node so you aren’t paying for an expensive GPU node that’s no longer training.</p>
</section>
<section id="preparing-for-masked-language-modeling-mlm" class="level3" data-number="2.5.7">
<h3 data-number="2.5.7" class="anchored" data-anchor-id="preparing-for-masked-language-modeling-mlm"><span class="header-section-number">2.5.7</span> Preparing for Masked Language Modeling (MLM)</h3>
<p>Since we are training our DNA BERT model using <strong>masked language modeling (MLM)</strong>, we need to handle introducing masked tokens properly. This is done using the <strong><code>DataCollatorForLanguageModeling</code></strong>, which:</p>
<ul>
<li><strong>Randomly masks nucleotides</strong> in the training sequences.</li>
<li><strong>Creates <code>labels</code> automatically</strong>, meaning the model learns by trying to predict these labeled masked tokens.</li>
<li><strong>Uses a masking probability of 5%-15%</strong>, ensuring that a small but meaningful portion of the sequence is masked during training.</li>
</ul>
<p>By applying MLM, we allow the model to generalize nucleotide relationships and capture sequence dependencies, just like how BERT learns relationships between words in text.</p>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorForLanguageModeling(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>hf_tokenizer,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    mlm<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    mlm_probability<span class="op">=</span><span class="fl">0.05</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="training-the-dna-bert-model" class="level3" data-number="2.5.8">
<h3 data-number="2.5.8" class="anchored" data-anchor-id="training-the-dna-bert-model"><span class="header-section-number">2.5.8</span> Training the DNA BERT Model</h3>
<p>With our configuration and data collator in place, we now train the model. We use the Hugging Face <strong><code>Trainer</code></strong> API, which simplifies the training process by handling:</p>
<ul>
<li><strong>Dataset iteration:</strong> Automatically loads and batches training sequences.</li>
<li><strong>Gradient updates:</strong> Adjusts model weights based on training loss.</li>
<li><strong>Logging &amp; saving:</strong> Tracks training progress and stores checkpoints.</li>
</ul>
<p>Once training begins, the model will gradually learn nucleotide dependencies and improve its ability to predict missing DNA bases. <strong>Python Code:</strong></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>tokenized_dataset,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>hf_tokenizer,</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collator,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>trainer.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you have set up a free wandb account, you can track your training runs, wherever they are running, on a central dashboard. You then get a dashboard full of pretty loss vs progress plots like the one below in <strong>figure 2</strong>, which I screencapped about ± 30 minutes into training a tiny version of the model on my MacBook.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/paste-4.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure 2:</strong> a training loss curve screen capped from wandb.com during training</figcaption>
</figure>
</div>
<p>I’t valuable to consider the meaning of the loss. In this case the cross entropy loss can be transformed into a probability: <span class="math inline">\(e^{-loss} = p\)</span> the predicted masked token is the true token in the training data. So as you can see in DNA data that is absolutely dominated by the tokens for G,C,T and A (the training data has no, or very very few, ambiguous nucleotide tokens) the loss almost immediately drops to below ~ -1.4, which is very close too random guess in of those four nocleotides: <span class="math inline">\(e^{-1.4}  = 0.25\)</span>. In other words it takes the model 1 iteration to learn almost all bases are G,C,T or A in the FASTA. after half an hour the loss is near 1.2, which corresponds to a probability of predicting the masked token correctly of: <span class="math inline">\(e^{-1.2}  = 0.30\)</span>. We’ll dig deeper into the meaning of the loss in <a href="Chapter3_DNA.html" class="quarto-xref"><span>Chapter 3</span></a>.</p>
</section>
<section id="saving-the-trained-model" class="level3" data-number="2.5.9">
<h3 data-number="2.5.9" class="anchored" data-anchor-id="saving-the-trained-model"><span class="header-section-number">2.5.9</span> Saving the Trained Model</h3>
<p>After training completes, we save both the model and tokenizer so they can be used for future predictions or fine-tuning.</p>
<ul>
<li>The model weights are stored in <code>./bert-dna</code>, allowing us to reload the trained model.</li>
<li>The tokenizer is also saved, ensuring that input sequences can be processed the same way during inference.</li>
</ul>
<p>Finally, a success message is printed, confirming that the training process has been completed.</p>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the final model and tokenizer</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>trainer.save_model(<span class="st">"./bert-dna"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>hf_tokenizer.save_pretrained(<span class="st">"./bert-dna"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Training complete! Model saved to ./bert-dna"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you intend to use, and re-use your model repeatedly, on different machines, or share it with others, it’s very convenient to save it to Hugging Face. If you have an account, you can do so for free using their internal tools. You’ll need to include an API token, I have omitted mine, and so should you when sharing code, because the API token lets people write to your account!</p>
<p><strong>Python Code:</strong></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>model.push_to_hub(repo_id<span class="op">=</span><span class="st">"MichelNivard/DNABert-CDS-13Species-v0.1"</span>,use_auth_token<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>hf_tokenizer.push_to_hub(repo_id<span class="op">=</span><span class="st">"MichelNivard/DNABert-CDS-13Species-v0.1"</span>,use_auth_token<span class="op">=</span><span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Because we used a standard BERT model (<strong><code>BertModern</code></strong>), it’s super easy for others to pull the model weights from the hub into a model configured for use on their machine, using the Hugging Face <strong><code>Transformers</code></strong> library. If you want to train a state of the art model you’d obviously need to scale training well beyond what I did here. We’ll cover how to in the chapter on <a href="https://michelnivard.github.io/Biological-language-models/Scaling_training.html">scaling</a>.</p>
</section>
<section id="summary" class="level3" data-number="2.5.10">
<h3 data-number="2.5.10" class="anchored" data-anchor-id="summary"><span class="header-section-number">2.5.10</span> Summary</h3>
<p>In this section, we:</p>
<ul>
<li>Defined training hyperparameters such as batch size, learning rate, and training steps.</li>
<li>Used masked language modeling (MLM) to train the model to predict gaps in DNA sequences.</li>
<li>Leveraged the Hugging Face <strong><code>Trainer</code></strong> API to automate model training.</li>
<li>Saved the final trained model and tokenizer for future use.</li>
</ul>
<p>With this trained model, we can now fine-tune or apply it to various genomic tasks, such as predicting genetic variations or classifying functional DNA sequences. In the next chapter, we will explore how to fine-tune our DNA BERT model for specific applications.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-benegas2023" class="csl-entry" role="listitem">
Benegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. <span>“DNA Language Models Are Powerful Predictors of Genome-Wide Variant Effects.”</span> <em>Proceedings of the National Academy of Sciences</em> 120 (44). <a href="https://doi.org/10.1073/pnas.2311219120">https://doi.org/10.1073/pnas.2311219120</a>.
</div>
<div id="ref-benegas2025" class="csl-entry" role="listitem">
Benegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun S. Song. 2025. <span>“Genomic Language Models: Opportunities and Challenges.”</span> <em>Trends in Genetics</em>, January. <a href="https://doi.org/10.1016/j.tig.2024.11.013">https://doi.org/10.1016/j.tig.2024.11.013</a>.
</div>
<div id="ref-nguyen2024" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva Katrekar, David B. Li, Liam J. Bartie, et al. 2024. <span>“Sequence Modeling and Design from Molecular to Genome Scale with Evo.”</span> <em>Science</em> 386 (6723). <a href="https://doi.org/10.1126/science.ado9336">https://doi.org/10.1126/science.ado9336</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="MichelNivard/Biological-language-models" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Chapter1_DNA.html" class="pagination-link" aria-label="Preparing DNA data for training">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preparing DNA data for training</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Chapter3_DNA.html" class="pagination-link" aria-label="Evaluating DNA Language Models">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluating DNA Language Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>