<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; Integrated protein diffusion language models – Biological Language Models &amp; Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./summary.html" rel="next">
<link href="./Chapter4_Proteins.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e84559ba8659b1a571faa725acb99328.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="site_libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet">
<script src="site_libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="site_libs/viz-1.8.2/viz.js"></script>
<link href="site_libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet">
<script src="site_libs/grViz-binding-1.0.11/grViz.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./DNA.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Biological Language Models &amp; Neural Networks</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Chapter1_Proteins.html">Proteins</a></li><li class="breadcrumb-item"><a href="./Chapter5_Proteins.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Integrated protein diffusion language models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Preamble1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">What is this Book About?</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Preamble2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to Read this Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Preamble3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The software stack</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">DNA</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter1_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preparing DNA data for training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter2_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Training our first DNA Language Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter3_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluating DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter4_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evolution-Aware Encoders</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter5_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Weaving Together Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter6_DNA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">A Review of Current DNA Language Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Scaling_training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Scale up Training</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Proteins</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter1_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Proteins: from sequence to structure</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter2_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Selecting and curating protein sequences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter3_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Training our first Protein Language Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter4_Proteins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Protein contact maps from attention maps</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter5_Proteins.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Integrated protein diffusion language models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Multi-Modal-Biology</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#alphafold3" id="toc-alphafold3" class="nav-link active" data-scroll-target="#alphafold3"><span class="header-section-number">11.1</span> Alphafold3</a></li>
  <li><a href="#diffusion-models" id="toc-diffusion-models" class="nav-link" data-scroll-target="#diffusion-models"><span class="header-section-number">11.2</span> Diffusion models</a>
  <ul class="collapse">
  <li><a href="#diffusion-reverse-model-architecture" id="toc-diffusion-reverse-model-architecture" class="nav-link" data-scroll-target="#diffusion-reverse-model-architecture"><span class="header-section-number">11.2.1</span> Diffusion reverse model architecture</a></li>
  <li><a href="#diffusion-model-for-contact-maps" id="toc-diffusion-model-for-contact-maps" class="nav-link" data-scroll-target="#diffusion-model-for-contact-maps"><span class="header-section-number">11.2.2</span> Diffusion model for contact maps</a></li>
  </ul></li>
  <li><a href="#from-attention-to-cross-attention" id="toc-from-attention-to-cross-attention" class="nav-link" data-scroll-target="#from-attention-to-cross-attention"><span class="header-section-number">11.3</span> From attention, to cross-attention</a></li>
  <li><a href="#cross-attention-and-a-diffusion-model." id="toc-cross-attention-and-a-diffusion-model." class="nav-link" data-scroll-target="#cross-attention-and-a-diffusion-model."><span class="header-section-number">11.4</span> Cross attention and a diffusion model.</a></li>
  <li><a href="#training-the-imagen-model" id="toc-training-the-imagen-model" class="nav-link" data-scroll-target="#training-the-imagen-model"><span class="header-section-number">11.5</span> Training the imagen model</a></li>
  <li><a href="#conclussion" id="toc-conclussion" class="nav-link" data-scroll-target="#conclussion"><span class="header-section-number">11.6</span> Conclussion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Chapter1_Proteins.html">Proteins</a></li><li class="breadcrumb-item"><a href="./Chapter5_Proteins.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Integrated protein diffusion language models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Integrated protein diffusion language models</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Abstract
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this chapter, we’ll discuss AlphaFold3 and its joint sequence (language model like) &amp; diffusion (Image generation model like) architecture. But because AF3 is a truly complex model, we’ll use a far simpler protein contact diffusion model, guided by a protein language model.</p>
<p>In doing so we’ll cover and study three core deep learning concepts, 1. denoising diffusion, 2. cross-attentions and 3. sequence guided diffusion (the actual link between sequences and 2D or 3D representations of molecules). The code used for this chapter is found here: <a href="#0" class="uri">https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/Proteins/Chapter11</a></p>
</div>
</div>
<section id="alphafold3" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="alphafold3"><span class="header-section-number">11.1</span> Alphafold3</h2>
<p>Alphafold3<span class="citation" data-cites="abramson2024a">(<a href="references.html#ref-abramson2024a" role="doc-biblioref">Abramson et al. 2024a</a>)</span> has what initially looks like a very complex architecture (See <strong>Figure 1</strong>), at least it did to me for the longest time.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/paste-33.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure 1:</strong> (source: Figure 1d Alphafold paper<span class="citation" data-cites="abramson2024b">(<a href="references.html#ref-abramson2024b" role="doc-biblioref">Abramson et al. 2024b</a>)</span>) which abstracts the model architecture used in Alphafold3. We’ll talk through things step by step below.</figcaption>
</figure>
</div>
<p>One important new aspect to the architecture is the “Diffusion module” on the bottom right. It has 3 inputs, the results from three other model elements feed into it (blue paths) and a weird little point cloud. The actual architecture of AF3 is really remarkably complex, I feel in no way competent to teach it. One of the best overviews I saw is a <a href="https://elanapearl.github.io/blog/2024/the-illustrated-alphafold/">blog post by Elena Simon &amp; Jake Silberg</a>. Like AF2 before it, AF3 is a model that imposes evolutionarily sensible (the multiple sequence alignments) and geometric (triangle rules) constraints that mean their models are tailored/specific/supervised/slower but often better than pure protein language models (at the expense of speed and efficiency).</p>
<p>Because I can’t do the full complexity of AF3 justice, but I think I can get people up to speed on how you’d begin to think about bridging protein sequence models, and 2D and 3D representation of proteins, we are going to train a protein diffusion model, that is guided by the inputs with a generic protein language model (<code>EvolutionaryScale/esmc-300m-2024-12</code> to be precise, ESM2’s successor) and attaches a diffusion model to it.</p>
<p>Now since I don’t have the compute that Google DeepMind has (and neither do you…), we’ll train a 2D diffusion model that, guided by a pre-existing protein language model, generates protein contact maps. The reason to again work on predicting contact maps is that 1. the image-like illustrations contact maps are help me effectively convey the concept of diffusion and 2. abstracting away the highly effective MSA and physics-based architecture in AF3 lets me focus on something fairly profound (I think): <strong>cross-attention</strong>. To my simple statistics/numerically minded brain, the fact that through encoding and attention we can capture the correlations between sequences of tokens is already a huge leap. When I think about how to capture two entirely separate (but related) modalities, a sequence of tokenized amino acids, and the 3-D (or 2D) structure of the molecule, my brain breaks a little. Cross-attention is how ChatGPT can relate your written command (“make a picture of a dog strapped to the roof of Mitt Romney’s car, in the style of Studio Ghibli”) and produce an image of just that scene, to the incredible annoyance and in some cases despair of artists who’s work they pirated for that! Fortunately for us, in Genomics, the data is actually in the public domain! and so there are no moral compromises to what we are about to do in that respect!</p>
</section>
<section id="diffusion-models" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="diffusion-models"><span class="header-section-number">11.2</span> Diffusion models</h2>
<p>Gaussian Denoising Diffusion models are a very flexible, and ingenious, class of deep learning models for images. Architectures derived from diffusion models are the conceptual basis for all kinds of famous image generation models, like the one integrated into ChatGPT or other image generation AI models.</p>
<p>Diffusion models have two parts, a <em>forward</em> process, which is defined, or fixed, and a <em>reverse</em> process which takes the form of a neural network and which is learned. The forward process takes training images (in our case of protein distance maps) and adds sequentially more noise (See Figure 2).</p>
<p>The relation between the image (x) at time t, and t-1 is:</p>
<p><span class="math display">\[
X_{t-1} =  b_0 * X_{t} + b_1 * \mathcal{N}(\mu,\sigma)
\]</span></p>
<p>Where <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are parameterized in a clever way, such that they 1. depend on t alone and 2. the variance of the total image stays the same, or is controlled. This means that we don’t have to store/create all $X_{t-1} $ images but can reconstruct them from t and the input image <span class="math inline">\(X_0\)</span>. For a specific protein contact map ( <span class="math inline">\(X_0\)</span> )the “noising” sequence might look a little like <strong>figure 2</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/paste-35.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure 2</strong>: Denoising diffusion forward process</figcaption>
</figure>
</div>
<p>A diffusion model uses the, and the value of <span class="math inline">\(t\)</span> to learn to “denoise” the image. It doesn’t denoise the image all at once though, it trains to denoise from t=500 to t=499, and t=150 to t=149 etc etc. During training, the images are embedded with their specific timestep <span class="math inline">\(t\)</span> such that the <em>reverse</em> diffusion model (generally a U-Net architecture) can learn weights that are able to optimally estimate (and the subtract) the noise for. given image at time step t.</p>
<section id="diffusion-reverse-model-architecture" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="diffusion-reverse-model-architecture"><span class="header-section-number">11.2.1</span> Diffusion reverse model architecture</h3>
<p>Each “down block” in a diffusion U-Net starts with a&nbsp;normalization layer, which helps keep the model stable during training by making sure the numbers flowing through the network stay within a reasonable range. Then comes an&nbsp;activation function, like&nbsp;Swish&nbsp;or&nbsp;GELU, which adds flexibility to the model and helps it learn more complex patterns. The core part of the block is a&nbsp;<strong>Conv2D layer</strong>, which looks at small squares of pixels (like 3×3 patches) and learns to summarize what’s in them—kind of like learning to detect edges, textures, or other useful features. A special trick used in diffusion models is the&nbsp;<strong>time embedding</strong>, which tells the model what step of the denoising process it’s on. This time information is turned into numbers and added to the features in the block so the model can behave differently at each step.</p>
<p>After the main part of the down block, there’s a&nbsp;<strong>downsampling layer</strong>&nbsp;that reduces the size of the image (usually by half) so the next layer can focus on a broader view of the picture. This is often done with a&nbsp;strided convolution, which skips over pixels to shrink the height and width while keeping the most important features. Skip connections pass the feature maps from each downsampling stage directly to the matching upsampling stage on the other side of the U. This helps the model keep important details that might be lost during compression, allowing the decoder to reconstruct sharper and more accurate outputs. The goal of downsampling the image is to tend to the same image at different scales, the fine detail in the early layers, the more global structure in the later layers. In the case of protein contact maps, the early layers tend to the secondary structure: local spatial conformation of the polypeptide backbone, helices, sheets, and loops.</p>
<p>In the&nbsp;<strong>middle of the U-Net</strong>, after the deepest downsampling layer, there’s often a&nbsp;<strong>self-attention block</strong>. This layer helps the model understand&nbsp;<strong>global relationships</strong>&nbsp;in the image — for example, connecting far-apart pixels that should be related (like opposite ends of a stripe or outline). Since it operates at the most compressed resolution, it’s efficient but powerful, and it benefits from the time embedding just like the ResBlocks. In the context of protein contact diffusion models, the attention learns aspects of the tertiary protein structure.</p>
<div class="cell">
<div class="cell-output-display">
<div class="grViz html-widget html-fill-item" id="htmlwidget-70122ae9083f3c68385f" style="width:100%;height:464px;"></div>
<script type="application/json" data-for="htmlwidget-70122ae9083f3c68385f">{"x":{"diagram":"\ndigraph diffusion_unet {\n  graph [layout = dot, rankdir = LR, fontsize = 30]\n\n  // Node styles\n  node [shape=box, style=filled, fontname=Helvetica, fontsize=30]\n\n  // Input and encoder\n  input     [label=\"Input Image\\n(1×128×128)\", fillcolor=lightgray]\n  init_conv [label=\"Init Conv\\n(1 → 64)\", fillcolor=lightgray]\n  down1     [label=\"Down Block 1\\nResBlock ×2\\n(64)\", fillcolor=lightblue]\n  down2     [label=\"Down Block 2\\nResBlock ×2\\n(128)\", fillcolor=lightblue]\n  down3     [label=\"Down Block 3\\nResBlock ×2\\n(256)\", fillcolor=lightblue]\n  down4     [label=\"Down Block 4\\nResBlock ×2\\n(512)\", fillcolor=lightblue]\n\n  // Bottleneck\n  mid1      [label=\"Mid Block 1\\nResBlock\\n(512)\", fillcolor=gold]\n  mid_attn  [label=\"Mid Attention\\nSelf-Attn\\n(512)\", shape=ellipse, fillcolor=orange]\n  mid2      [label=\"Mid Block 2\\nResBlock\\n(512)\", fillcolor=gold]\n\n  // Decoder (upsampling path)\n  up4       [label=\"Up Block 4\\nResBlock ×2\\n(512 → 256)\", fillcolor=lightblue]\n  up3       [label=\"Up Block 3\\nResBlock ×2\\n(256 → 128)\", fillcolor=lightblue]\n  up2       [label=\"Up Block 2\\nResBlock ×2\\n(128 → 64)\", fillcolor=lightblue]\n  up1       [label=\"Up Block 1\\nResBlock ×2\\n(64)\", fillcolor=lightblue]\n  final_res [label=\"Final ResBlock\\n(64)\", fillcolor=lightblue]\n  final_conv[label=\"Final Conv\\n(64 → 1)\", fillcolor=lightblue]\n  output    [label=\"Output Image\\n(1×128×128)\", fillcolor=lightgray]\n\n  // Time embedding\n  time_mlp  [label=\"Time MLP\\nSinusoidal + MLP\\n(→ 64/128/256/512)\", shape=hexagon, fillcolor=pink]\n\n  // Flow (encoder → bottleneck → decoder)\n  input     -> init_conv -> down1 -> down2 -> down3 -> down4\n  down4     -> mid1 -> mid_attn -> mid2 -> up4 -> up3 -> up2 -> up1 -> final_res -> final_conv -> output\n\n  // Time embedding connections to all ResBlocks\n  time_mlp -> down1\n  time_mlp -> down2\n  time_mlp -> down3\n  time_mlp -> down4\n  time_mlp -> mid1\n  time_mlp -> mid2\n  time_mlp -> up4\n  time_mlp -> up3\n  time_mlp -> up2\n  time_mlp -> up1\n  time_mlp -> final_res\n\n  // Skip connections across the U\n  down4 -> up4 [style=dashed, color=darkgray]\n  down3 -> up3 [style=dashed, color=darkgray]\n  down2 -> up2 [style=dashed, color=darkgray]\n  down1 -> up1 [style=dashed, color=darkgray]\n}\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<p><strong>Up blocks</strong> in a diffusion U-Net are the mirror of the down blocks—they take the compressed features and gradually rebuild the image to its original size. Each up block typically starts by&nbsp;<strong>upsampling</strong>&nbsp;the feature map, usually with a&nbsp;transpose convolution&nbsp;or&nbsp;nearest-neighbor interpolation followed by a Conv2D, which increases the height and width (often doubling them). After upsampling, the block&nbsp;combines the upsampled features with the skip connection&nbsp;from the matching down block, so it has both high-level context and fine details. It then passes this combined input through one or more&nbsp;ResBlocks, just like in the encoder, using normalization, activation, convolutions, and time embedding again to refine the reconstruction.</p>
<p>At the start, the scalar timestep&nbsp;<code>t</code>&nbsp;is turned into a&nbsp;<strong>high-dimensional vector</strong>&nbsp;using a&nbsp;sinusoidal position embedding, and then passed through a small&nbsp;MLP (multi-layer perceptron)&nbsp;to create a learned time embedding. This embedding is then&nbsp;<strong>injected into nearly every ResBlock</strong>&nbsp;in the model — both in the&nbsp;down blocks,&nbsp;up blocks, and&nbsp;middle blocks. Inside each ResBlock, the time embedding is&nbsp;added (broadcasted)&nbsp;to the feature map after a linear layer transforms it to match the number of channels. This allows every part of the network to be conditioned on how much noise it should expect and how aggressively to denoise.</p>
</section>
<section id="diffusion-model-for-contact-maps" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="diffusion-model-for-contact-maps"><span class="header-section-number">11.2.2</span> Diffusion model for contact maps</h3>
<p>As a training set, I build 7000 contact maps, based on experimentally validated proteins in the CASP12 set I obtained from the SideChainNet dataset <span class="citation" data-cites="king2021">(<a href="references.html#ref-king2021" role="doc-biblioref">King and Koes 2021</a>)</span>. I do not build strictly binary contact maps, but build images that reflect 9 distances thresholds (see <strong>Figure 3</strong> for an example entry from the training set).</p>
<p>Here are 2 functions I used to make contact/distance maps and then crop those to 128x 128 pixels.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># === FUNCTIONS ===</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_contact_map(coords, binary<span class="op">=</span><span class="va">False</span>, threshold<span class="op">=</span><span class="fl">8.0</span>):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Computes a contact or distance map from 3D Cα coordinates.</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">        coords (np.ndarray): Shape (L, 3) or (L, A, 3), 3D coordinates.</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">        binary (bool): Whether to return binary contact map.</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">        threshold (float): Å distance cutoff for binary maps.</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">        clip_dist (float): Max distance for clipping and scaling.</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">        np.ndarray: (L, L) contact or scaled distance map in uint8.</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># extract only Cα coordinates (atom 0 for each residue)</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    ca_coords <span class="op">=</span> coords[:, <span class="dv">0</span>, :]  <span class="co"># shape (L, 3)</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    n_residues <span class="op">=</span> ca_coords.shape[<span class="dv">0</span>]</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    n_missing <span class="op">=</span> np.isnan(ca_coords).<span class="bu">any</span>(axis<span class="op">=</span><span class="dv">1</span>).<span class="bu">sum</span>()</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    frac_missing <span class="op">=</span> n_missing <span class="op">/</span> n_residues</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> frac_missing <span class="op">&gt;</span> <span class="fl">0.10</span>:</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>protein_id<span class="sc">}</span><span class="ss"> skipped: </span><span class="sc">{</span>n_missing<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n_residues<span class="sc">}</span><span class="ss"> Cα coords missing (</span><span class="sc">{</span>frac_missing<span class="sc">:.1%}</span><span class="ss">)"</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.unique(ca_coords, axis<span class="op">=</span><span class="dv">0</span>).shape[<span class="dv">0</span>] <span class="op">&lt;=</span> <span class="dv">1</span>:</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>protein_id<span class="sc">}</span><span class="ss"> skipped: collapsed structure (identical Cα)"</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n_residues <span class="op">&lt;</span> <span class="dv">10</span>:</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>protein_id<span class="sc">}</span><span class="ss"> skipped: too short (</span><span class="sc">{</span>n_residues<span class="sc">}</span><span class="ss"> residues)"</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># normalize based on first Cα</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    ca_coords <span class="op">-=</span> ca_coords[<span class="dv">0</span>]  <span class="co"># shift so residue 0 is at (0, 0, 0)</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute pairwise distance matrix</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    dists <span class="op">=</span> np.linalg.norm(ca_coords[:, <span class="va">None</span>, :] <span class="op">-</span> ca_coords[<span class="va">None</span>, :, :], axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> binary:</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        contact_map <span class="op">=</span> (dists <span class="op">&lt;</span> threshold).astype(np.uint8) <span class="op">*</span> <span class="dv">255</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> contact_map</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        levels <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">20</span>,<span class="dv">40</span>,<span class="dv">60</span>,<span class="dv">80</span>,<span class="dv">100</span>,<span class="dv">120</span>,<span class="dv">140</span>,<span class="dv">160</span>,<span class="dv">180</span>]</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        contact_map <span class="op">=</span> np.zeros_like(dists, dtype<span class="op">=</span>np.uint8) <span class="op">+</span> <span class="dv">255</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Bin 0: d &lt; threshold - 3</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> dists <span class="op">&lt;</span> (threshold <span class="op">-</span> <span class="dv">3</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        contact_map[mask] <span class="op">=</span> levels[<span class="dv">0</span>]</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Bins 1 to 9: 1Å slices</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>):</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>            lower <span class="op">=</span> threshold <span class="op">-</span> <span class="dv">5</span> <span class="op">+</span> (i) </span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>            upper <span class="op">=</span> lower <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> (dists <span class="op">&gt;=</span> lower) <span class="op">&amp;</span> (dists <span class="op">&lt;</span> upper)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>            contact_map[mask] <span class="op">=</span> levels[i]</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> contact_map</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Or simply crop the first 128 amino-acids (Ca atoms associated with those amino-acids)</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> crop_contact_map(contact_map, crop_size<span class="op">=</span><span class="dv">128</span>, pad_value<span class="op">=</span><span class="dv">255</span>, min_size<span class="op">=</span><span class="dv">20</span>):</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="co">    Crop the top-left corner of the contact map to (crop_size, crop_size).</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="co">    Pads with `pad_value` if the map is smaller.</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="co">    Skips maps smaller than `min_size`.</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>    h, w <span class="op">=</span> contact_map.shape</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> h <span class="op">&lt;</span> min_size <span class="kw">or</span> w <span class="op">&lt;</span> min_size:</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Contact map too small: </span><span class="sc">{</span>h<span class="sc">}</span><span class="ss">x</span><span class="sc">{</span>w<span class="sc">}</span><span class="ss"> (min required: </span><span class="sc">{</span>min_size<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>    canvas <span class="op">=</span> np.full((crop_size, crop_size), pad_value, dtype<span class="op">=</span>np.uint8)</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    crop_h <span class="op">=</span> <span class="bu">min</span>(h, crop_size)</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>    crop_w <span class="op">=</span> <span class="bu">min</span>(w, crop_size)</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>    canvas[:crop_h, :crop_w] <span class="op">=</span> contact_map[:crop_h, :crop_w]</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Image.fromarray(canvas)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Then using the <code>sidechainnet</code> package, I download protein information, save the contact maps as images using the following function:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === MAIN ===</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading SideChainNet CASP12 dataset..."</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> scn.load(casp_version<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generating contact maps..."</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, sample <span class="kw">in</span> <span class="bu">enumerate</span>(data):</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        protein_id <span class="op">=</span> sample.<span class="bu">id</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        coords <span class="op">=</span> sample.coords  <span class="co"># shape: (L, A, 3)</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> coords <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> coords.shape[<span class="dv">0</span>] <span class="op">&lt;</span> <span class="dv">2</span>:</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute distance matrix and preprocess</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        distance_map <span class="op">=</span> make_contact_map(coords, binary<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Skip if it's completely empty</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.<span class="bu">all</span>(distance_map <span class="op">==</span> <span class="dv">255</span>):</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Skipping </span><span class="sc">{</span>protein_id<span class="sc">}</span><span class="ss">: distance map is all white"</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Make the final image</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> crop_contact_map(distance_map, crop_size<span class="op">=</span><span class="dv">128</span>,pad_value<span class="op">=</span><span class="dv">255</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        img.save(os.path.join(output_dir, <span class="ss">f"</span><span class="sc">{</span>protein_id<span class="sc">}</span><span class="ss">.jpg"</span>), <span class="bu">format</span><span class="op">=</span><span class="st">'JPEG'</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"[</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">] Saved: </span><span class="sc">{</span>protein_id<span class="sc">}</span><span class="ss">.jpg"</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Skipping </span><span class="sc">{</span>sample<span class="sc">.</span><span class="bu">id</span><span class="sc">}</span><span class="ss"> due to error: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Done."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This generates a folder with thousands of images of contact maps, where the specific shade of gray of the pixel corresponds to &lt;4 (black), 5, 6, 7, 8, 9, 10, 11, 12, or &gt;13 (white) Angstrom distances.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/1KZQ_d1kzqb1.jpg" class="img-fluid figure-img" width="444"></p>
<figcaption><strong>Figure 3</strong>: A protein distance map based on experimental CASP12 data.</figcaption>
</figure>
</div>
<section id="detailed-parameter-breakdown-by-module-for-our-model" class="level4" data-number="11.2.2.1">
<h4 data-number="11.2.2.1" class="anchored" data-anchor-id="detailed-parameter-breakdown-by-module-for-our-model"><span class="header-section-number">11.2.2.1</span> <strong>Detailed Parameter Breakdown by Module for our model</strong></h4>
<p>The table below breaks down the different elements of the U-Net architecture of a tiny diffusion model I trained. The table describes how many free parameters each part of the model contains. As you can see, the decoder <code>ups</code> part of the model has almost half of the parameters. This is where the encoded information, and the skip connections are brought together. You’ll also note the most number of parameters in the <code>time_mlp</code>, which encodes the influence of the specific timestep <code>t</code>. It’s important to realize that the ResBlocks (both in the encoder and decoder) also contain parameters that map the time-specific embedding to the image information. These regulate <em>how</em> the time information influences image generation at each specific stage.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Module</th>
<th>Parameters</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>ups</code></td>
<td>19,832,768</td>
<td><strong>Upsampling path (decoder):</strong>&nbsp;progressively reconstructs the denoised image from compressed features. Includes ResBlocks, upsampling (e.g.&nbsp;transposed convolutions), and skip connections from encoder layers.</td>
</tr>
<tr class="even">
<td><code>downs</code></td>
<td>5,402,816</td>
<td><strong>Downsampling path (encoder):</strong>&nbsp;extracts hierarchical features from the noisy input image using stacked ResBlocks and downsampling layers.</td>
</tr>
<tr class="odd">
<td><code>mid_block1</code></td>
<td>4,983,808</td>
<td><strong>First bottleneck ResBlock:</strong>&nbsp;processes the most compressed latent representation of the input, directly before/after the attention block.</td>
</tr>
<tr class="even">
<td><code>mid_block2</code></td>
<td>4,983,808</td>
<td><strong>Second bottleneck ResBlock:</strong>&nbsp;further transforms latent features after attention at the bottleneck. Acts as a transition before decoding.</td>
</tr>
<tr class="odd">
<td><code>mid_attn</code></td>
<td>264,192</td>
<td><strong>Self-attention at bottleneck:</strong>&nbsp;captures global spatial dependencies in the most compressed feature map, enabling long-range interactions.</td>
</tr>
<tr class="even">
<td><code>final_res_block</code></td>
<td>152,000</td>
<td><strong>Final ResBlock before output:</strong>&nbsp;fuses decoder output and prepares it for the final convolution. Often used to refine the final image prediction.</td>
</tr>
<tr class="odd">
<td><code>time_mlp</code></td>
<td>82,432</td>
<td><strong>Timestep embedding network:</strong>&nbsp;converts scalar timestep into a vector that conditions all ResBlocks, allowing the model to denoise appropriately for each diffusion step.</td>
</tr>
<tr class="even">
<td><code>init_conv</code></td>
<td>3,200</td>
<td><strong>Initial input convolution:</strong>&nbsp;expands the input image from 1 channel to base feature dimension (<code>dim=64</code>), preparing it for downstream processing.</td>
</tr>
<tr class="odd">
<td><code>final_conv</code></td>
<td>65</td>
<td><strong>Final output convolution:</strong>&nbsp;projects the final hidden features back to 1 channel to match the original image shape. Predicts either noise (<code>ε_t</code>) or clean image (<code>x₀</code>).</td>
</tr>
</tbody>
</table>
<p>Once we’ve trained the diffusion model for about 40/60 minutes (though ideally longer and on way more data!) we can sample from it. Sampling from it involves sampling pure Gaussian noise (“<code>t - 999</code>”) and passing the noise through the diffusion model a few hundred times (each time updating <code>t</code>). There are speedups and tricks that mean you won’t have to pass it through the model 1000 times to get a solid result, though more passes (“steps”) generally means a better result. Because protein contact maps are symmetric, but the model doesn’t know that, I generate a protein but then copy over one of the two halves. In <strong>Figure 3</strong> I show a particularly convincing-looking sample at 5 steps from noise to final protein contact map.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/paste-36.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure 3:</strong> interim and final stages of a protein contact map sampled from pure noise, after training for a few epochs on 7000 real proteins.</figcaption>
</figure>
</div>
<p>The protein contact map sampled in <strong>Figure 3</strong> is a fiction that, according to the model, fits the distribution of the training data, which <strong>while fascinating isn’t very useful yet</strong>. To make things useful, we must condition the model on not just timestep, but also on condensed, or embedded sequence information.</p>
<p>The next step is to condition the generation of a diffusion model on the amino-acid sequence. This is a very specific multi-modal problem, we want to create an image, based on a sequence of tokens. In order to wrap our head around.</p>
</section>
</section>
</section>
<section id="from-attention-to-cross-attention" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="from-attention-to-cross-attention"><span class="header-section-number">11.3</span> From attention, to cross-attention</h2>
<p>In <a href="Chapter4_Proteins.html" class="quarto-xref"><span>Chapter 10</span></a> we discussed the attention mechanism, where each token in a sequence is embedded in <span class="math inline">\(X\)</span>, looks at all other tokens in the same sequence—we can explore something more general:&nbsp;<strong>cross-attention</strong>. Cross-attention (See Figure below) is what allows models to link&nbsp;<em>different modalities</em>&nbsp;together. It is how an image can learn to attend to words, or how a protein contact map can align with an amino acid sequence.</p>
<p>In technical terms, self-attention happens when the Query, Key, and Value matrices—<span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span>—are all derived from the same input. Cross-attention, on the other hand, involves two different sources. One stream generates the queries, <span class="math inline">\(Q_{nxd}\)</span>, where n is the number of tokens in the query source (e.g., the contact map image), and <span class="math inline">\(d_q\)</span> is the learned query dimensionality. The other stream produces the keys <span class="math inline">\(K_{mxd}\)</span> and values <span class="math inline">\(K_{mxd}\)</span>, where <span class="math inline">\(m\)</span> is the number of tokens in the key/value source (e.g., the sequence). In this setup, the queries are essentially asking, “Where in the other modality is the most relevant information for me?”</p>
<p>Here’s where it gets interesting: even though the inputs—say, a 2D contact map versus a 1D amino acid sequence—might have vastly different shapes and lengths (n ≠ m), attention still works because we project both inputs into a shared space using learned linear transformations. The only requirement is that the inner dimensions match appropriately: the dot product QKᵗ yields an attention score matrix <span class="math inline">\(A_{nxm}\)</span>, which tells each query vector how much to attend to each key. After applying softmax to these scores, the attention weights are used to compute a weighted sum of the values <span class="math inline">\(V\)</span>, resulting in the final output <span class="math inline">\(Z_{n*d}\)</span>.</p>
<p>So, while the original modalities might differ wildly in format and token count, cross-attention does not require the inputs to be the same size—only that their projections align where it matters: in the shared attention dimensions <span class="math inline">\(d_q\)</span> and <span class="math inline">\(d_v\)</span>.</p>
<p>In our case, when we say the&nbsp;contact <strong>map diffusion model attends to the sequence</strong>, we mean the image features are used to generate queries, and the sequence provides the keys and values, the final output dimensions are consistent with those of the diffusion model. This lets the contact map image learn where in the sequence there are features that are most relevant to its shape. That might sound abstract, but think of it as one set of data (the image) trying to find the most useful context from another (the sequence).</p>
<p>This mechanism is central in multimodal transformers like text to image, speech to text, or many others. It is increasingly being used in biological modeling too. It enables not just learning within a sequence, but learning&nbsp;<em>between representations</em>—and that’s a big leap.</p>
<div class="cell">
<div class="cell-output-display">
<div class="grViz html-widget html-fill-item" id="htmlwidget-1fa2def3ae8da061943d" style="width:100%;height:464px;"></div>
<script type="application/json" data-for="htmlwidget-1fa2def3ae8da061943d">{"x":{"diagram":"\ndigraph cross_attention_parallel {\n  graph [rankdir=LR, nodesep=1.0, ranksep=0.6, fontsize=12]\n\n  node [shape=box, style=filled, fontname=Helvetica]\n\n  // Inputs\n  img [label=\"Contact map X\", fillcolor=lightblue]\n  txt [label=\"Seq X\", fillcolor=lightblue]\n\n  // Weights\n  Wq [label=\"W_q\", fillcolor=gray]\n  Wk [label=\"W_k\", fillcolor=gray]\n  Wv [label=\"W_v\", fillcolor=gray]\n\n  // QKV blocks\n  Q [label=\"Q\", fillcolor=lavender]\n  K [label=\"K\", fillcolor=lavender]\n  V [label=\"V\", fillcolor=lavender]\n\n  // Attention ops\n  QK [label=\"QKᵀ\", fillcolor=lightcyan]\n  Softmax [label=\"Softmax\", fillcolor=black, fontcolor=white]\n  A [label=\"Cross attention\n(Attn Weights)\", fillcolor=lightcyan]\n  Z [label=\"Z\n(Output)\", fillcolor=pink]\n\n  // Layout helpers\n  { rank = same; img; txt }\n  { rank = same; Q; K; V }\n  { rank = same; QK; Softmax; A }\n\n  // Connections\n  img -> Wq -> Q\n  txt -> Wk -> K\n  txt -> Wv -> V\n\n  Q -> QK\n  K -> QK\n  QK -> Softmax -> A\n\n  A -> Z\n  V -> Z\n}\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
</section>
<section id="cross-attention-and-a-diffusion-model." class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="cross-attention-and-a-diffusion-model."><span class="header-section-number">11.4</span> Cross attention and a diffusion model.</h2>
<p>Google’s ‘imagen’ architecture implements a diffusion model, which uses cross attention to attend to the final embeddings of a text model. The great news to us is that the ‘imagen’ paper actually showed you don’t really need to train the “text” part of the model with this specific goal in mind, you can just use an existing model! And that is what we are going to do. We are going to use <code>facebook/esm2_t30_150M_UR50D</code> which is an evolution of the Facebook-funded ESM-2 models, to compute embeddings for all CASP 12 proteins, and train a diffusion model <em>conditioned</em> on the image embedding. Recall how U-net diffusion models are already conditioned on the time <code>t1</code> embedding, we are simply going to also condition specific steps in the U-net on the sequence model embedding. As we know from <a href="Chapter4_Proteins.html" class="quarto-xref"><span>Chapter 10</span></a> sequence models do learn latent representations of contact maps, we are going to leverage that information when running the diffusion process.</p>
<p>I use the script below to download the amino-acid sequences for proteins in the <code>sidechainnet</code> dataset, run the <code>facebook/esm2_t30_150M_UR50D</code> model on these proteins, and save the per amino-acid embeddings generated by the last layer. These embeddings, unlike the input embeddings, are the contextualized (their passed through attention blocks across many layers) embedding of each amino-acid in its context. The output is a 128 by 640 (embedding dimension for this model) matrix that is a numerical representation of the protein according to the <code>facebook/esm2_t30_150M_UR50D</code> model. This is a quite resource-intensive process, so I added code to make sure it only computes embeddings for proteins for which we are able to generate a solid contact map (one without too many missing data points for example).</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a> <span class="op">===</span> embedding_extraction.py <span class="op">===</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sidechainnet <span class="im">as</span> scn</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModel</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># === PARAMETERS ===</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"facebook/esm2_t30_150M_UR50D"</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>output_dir <span class="op">=</span> <span class="st">"embeddings_all"</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>valid_ids_path <span class="op">=</span> <span class="st">"valid_proteins.txt"</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>os.makedirs(output_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># === Load Model ===</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"mps"</span> <span class="cf">if</span> torch.backends.mps.is_available() <span class="cf">else</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModel.from_pretrained(model_name).to(device).<span class="bu">eval</span>()</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co"># === embedding_extraction.py ===</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>os.makedirs(output_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="co"># === Load valid protein IDs from contact map step ===</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(valid_ids_path, <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    valid_ids <span class="op">=</span> <span class="bu">set</span>(line.strip() <span class="cf">for</span> line <span class="kw">in</span> f <span class="cf">if</span> line.strip())</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_embeddings(sequence):</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> tokenizer(sequence, return_tensors<span class="op">=</span><span class="st">"pt"</span>, truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> {k: v.to(device) <span class="cf">for</span> k, v <span class="kw">in</span> inputs.items()}</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        seq_emb <span class="op">=</span> outputs.last_hidden_state[<span class="dv">0</span>].cpu().numpy()  <span class="co"># (L, D)</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>        seq_len, emb_dim <span class="op">=</span> seq_emb.shape</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pad to 128 if needed</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> seq_len <span class="op">&lt;</span> <span class="dv">128</span>:</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>            pad <span class="op">=</span> np.zeros((<span class="dv">128</span> <span class="op">-</span> seq_len, emb_dim), dtype<span class="op">=</span>np.float32)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>            seq_emb <span class="op">=</span> np.vstack([seq_emb, pad])</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>            seq_emb <span class="op">=</span> seq_emb[:<span class="dv">128</span>]</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> seq_emb  <span class="co"># shape: (128, D)</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>    casp_versions <span class="op">=</span> [<span class="dv">12</span>]</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" Loading SideChainNet CASP datasets: </span><span class="sc">{</span>casp_versions<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine all samples</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>    all_samples <span class="op">=</span> []</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> casp_version <span class="kw">in</span> casp_versions:</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> scn.load(casp_version<span class="op">=</span>casp_version, casp_thinning<span class="op">=</span><span class="dv">70</span>)</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>        all_samples.extend(data)</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" Extracting embeddings only for valid proteins..."</span>)</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sample <span class="kw">in</span> tqdm(all_samples):</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>        protein_id <span class="op">=</span> sample.<span class="bu">id</span></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> protein_id <span class="kw">not</span> <span class="kw">in</span> valid_ids:</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>            sequence <span class="op">=</span> sample.sequence</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> sequence <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> <span class="bu">len</span>(sequence) <span class="op">&lt;</span> <span class="dv">10</span>:</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>            cropped_sequence <span class="op">=</span> sequence[:<span class="dv">128</span>]</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>            emb <span class="op">=</span> get_embeddings(cropped_sequence)</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>            np.save(os.path.join(output_dir, <span class="ss">f"</span><span class="sc">{</span>protein_id<span class="sc">}</span><span class="ss">.npy"</span>), emb)</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f" Skipping </span><span class="sc">{</span>protein_id<span class="sc">}</span><span class="ss"> due to error: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" Embedding extraction complete."</span>)</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>    main()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="training-the-imagen-model" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="training-the-imagen-model"><span class="header-section-number">11.5</span> Training the imagen model</h2>
<p>The high-level library <code>huggingface</code> provides for image models, called <code>diffusion</code>, isn’t as fully developed as their <code>transformers</code> library. Therefore, we’re going to use a library maintained by GitHub user <code>lucidrains</code> who I guess is a scientist in biomedical science but also someone who builds these amazing implementations of AI models from papers. The repo we’ll use is <a href="https://github.com/lucidrains/imagen-pytorch"><code>imagen-pythorch</code></a> which you can just install using pip: <code>pip install imagen-pytorch</code>.</p>
<p>What we’ll effectively end up doing to predict protein contact maps is run a two-model sequence: 1. run a protein language model to transform sequences into embeddings, encoding a lot of information using a pre-trained 150m parameter language model, and 2. use those embeddings, and the contact maps of the same protein to train a ~31 million parameter Unet image generation model, where every so often there is a cross-attention layer where the image attends to the sequence embedding. You can find the full script here, but below we can have a look at the core components, a U-net diffusion model, and an <code>imagen</code> model which contains it.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># === Define U-Nets for Imagen Cascade ===</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>unet1 <span class="op">=</span> Unet(</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    dim<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    cond_dim<span class="op">=</span>embedding_dim,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    dim_mults<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>),</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    channels<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    num_resnet_blocks<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    layer_attns<span class="op">=</span>(<span class="va">False</span>, <span class="va">False</span>, <span class="va">True</span>),</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    layer_cross_attns<span class="op">=</span>(<span class="va">False</span>, <span class="va">True</span>, <span class="va">True</span>),</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># === Imagen ===</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>imagen <span class="op">=</span> Imagen(    </span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    unets<span class="op">=</span>(unet1),</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    image_sizes<span class="op">=</span>(<span class="dv">128</span>),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    timesteps<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    cond_drop_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    text_embed_dim<span class="op">=</span><span class="dv">640</span>,</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    channels<span class="op">=</span><span class="dv">1</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We’ll have to do a little model work, because we can use Hugging Face’s <code>Trainer</code> library here. Though the repo we use here is still fairly high-level, so it provides a trainer, etc. the data loading process is abstracted away here, but of course, all scripts are available in full on GitHub.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === ImagenTrainer ===</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the trainer</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> ImagenTrainer(imagen,fp16<span class="op">=</span><span class="va">True</span>).to(device)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># FIX: Wrap with cond_images_dataset so (cond, image) is handled correctly</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>trainer.add_train_dataset(train_subset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>trainer.add_valid_dataset(eval_subset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># set up eval...</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>eval_losses <span class="op">=</span> []  <span class="co"># store losses for plotting later</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>eval_every <span class="op">=</span> <span class="dv">100</span>  <span class="co"># steps</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>steps_per_epoch <span class="op">=</span> <span class="bu">len</span>(train_subset) <span class="op">//</span> batch_size</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co"># === Training Loop ===</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Starting training..."</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> tqdm(<span class="bu">range</span>(steps_per_epoch)):</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        loss1 <span class="op">=</span> trainer.train_step(unet_number<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">+=</span> loss1</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> step <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss"> | Loss: </span><span class="sc">{</span>loss1<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> step <span class="op">%</span> eval_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>            eval_loss <span class="op">=</span> trainer.valid_step(unet_number<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>            eval_losses.append((epoch, step, eval_loss))</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f" Eval Loss @ Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>eval_loss<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"[Epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss">]  Loss: </span><span class="sc">{</span>total_loss <span class="op">/</span> steps_per_epoch<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (epoch<span class="op">+</span><span class="dv">1</span>) <span class="op">%</span> <span class="dv">5</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        trainer.save(<span class="ss">f"imagen_cont/imagen_protein_epoch</span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">.pt"</span>)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training complete."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>A training loop can be fairly simple, especially with great out-of-the-box pre-built models like the one provided by <code>lucidrains</code>. It’s essentially two Python loops, the outer loop loops over epochs, the inner loop over steps. In each inner loop, we’ll take a single training step, computing the derivatives of all parameters wrt the loss over a batch of data, and updating all parameters:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> tqdm(<span class="bu">range</span>(steps_per_epoch)):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>        loss1 <span class="op">=</span> trainer.train_step(unet_number<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This particular training loop still abstracts a fair bit away from the user, in later chapters we’ll likely have to open up that training loop even further. We train the imagen model on 16,000 contact maps and embedded sequences, for 80 epochs (image models need way more epochs than sequence models). After training, the model is able to fairly accurately reconstruct protein contact maps, see <strong>Figure 5</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imagen_contact_maps.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure 5</strong>: true vs generated protein contact maps generated by diffusion conditional on protein language model embeddings.</figcaption>
</figure>
</div>
<p>To understand the pros and cons of this model relative to the attention-based protein contact map prediction we performed in <a href="Chapter4_Proteins.html" class="quarto-xref"><span>Chapter 10</span></a>, we can do some visual inspection. While these generations on the right are sharp, crisp, and often very accurate, Generation 10889 has some clear “<strong>hallucinations</strong>”, or contacts that seem very confident and real, but aren’t in the ground truth at all! While if you go back to Figure 2 of <a href="Chapter4_Proteins.html" class="quarto-xref"><span>Chapter 10</span></a> you’d see far more “noisy” error in predictions. Hallucinations are a very pernicious kind of noise, they look clean, convincing, plausible, because the diffusion model just turned some early noise into a very slick processed convincing-looking result, which noise in statistical predictions in the last chapter are far more recognizable as noise!</p>
<p>Alphafold3, and AlphaFold2 before it, had a specific feature to guard against convincing-looking hallucinations. Both models had confidence modules, a neural network that predicts the model’s confidence in each predicted structure/residue. But it also used double conditional diffusion, where the diffusion was conditioned on both the sequence embeddings, and on the embedding of the proteins pair representations (similar to the attention maps we discussed in <a href="Chapter4_Proteins.html" class="quarto-xref"><span>Chapter 10</span></a>, a 2D representation of which amino-acids attend to each other).</p>
</section>
<section id="conclussion" class="level2" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="conclussion"><span class="header-section-number">11.6</span> Conclussion</h2>
<p>So in this chapter we trained, and discussed, a model that is abstraction fo AF3, it is like AF3 int he sense that we use diffusion, conditional on sequence information to estimate a representation of a protein. Obviosluyit is missing all kind of keye part of the AF3 model. in the next chapter we’ll slightly expand our abstraction of AF3. We’ll train diffusion model hat is conditioned on the amino-acid sequence embedding AND on the protein map as predicted based on the 2D presentation of the attention maps.</p>
<p>To prepare, you can try the two self-study assignments below, they pull in things we learned in the last two chapters to basically make protein predictions that adhere to the noisy, but mostly accurate outlines we could generate based on attention maps, and then use diffusion modeling to predict the local details, preventing strong hallucinations. If you’re more of a reader, skip to the next chapter, where I’ll take a stab at these two assignments myself.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In order from beginner to advanced project, consider these self-study exercises, all of which share a common goal, to see if we can further condition the diffusion model on the noisy but hallucination-free attention-based predictions we made in the last chapter, the hallucinations reduce.</p>
<p><strong>1. (easy)</strong> Run contact map prediction like in <a href="Chapter4_Proteins.html" class="quarto-xref"><span>Chapter 10</span></a>. That is, extract the attention maps when running <code>facebook/esm2_t30_150M_UR50D</code> for ~20 genes, predict their contact maps with those, store and average the prediction weights and use those to predict contact maps for further proteins in the <code>imagen</code> validation set. Then when sampling from your <code>imagen</code> model, give the attention-based prediction the argument <code>init_image</code>, along with the embeddings. This will start the denoising from the attention-based prediction, you can experiment with <code>skip_steps = xxx</code> because as the model now starts from a prediction, not pure noise, you might want to a portion of the 1000 steps for optimal results</p>
<p><strong>2. (medium)</strong> Run contact map prediction like in <a href="Chapter4_Proteins.html" class="quarto-xref"><span>Chapter 10</span></a>. That is, extract the attention maps when running <code>facebook/esm2_t30_150M_UR50D</code> for ~20 genes, predict their contact maps with those, store and average the prediction weights and use those to predict contact maps for further proteins in the <code>imagen</code> <strong>training</strong> set. Then when training your <code>imagen</code> model, give the attention-based prediction the argument <code>cond_images</code>, along with the text embeddings. This will train an <code>imagen</code> model that is conditioned on the faint outlines of the protein structure generated based on the attention maps, which are less likely to be hallucinations.</p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-abramson2024a" class="csl-entry" role="listitem">
Abramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, et al. 2024a. <span>“Accurate Structure Prediction of Biomolecular Interactions with AlphaFold 3.”</span> <em>Nature</em> 630 (8016): 493–500. <a href="https://doi.org/10.1038/s41586-024-07487-w">https://doi.org/10.1038/s41586-024-07487-w</a>.
</div>
<div id="ref-abramson2024b" class="csl-entry" role="listitem">
———, et al. 2024b. <span>“Accurate Structure Prediction of Biomolecular Interactions with AlphaFold 3.”</span> <em>Nature</em> 630 (8016): 493–500. <a href="https://doi.org/10.1038/s41586-024-07487-w">https://doi.org/10.1038/s41586-024-07487-w</a>.
</div>
<div id="ref-king2021" class="csl-entry" role="listitem">
King, Jonathan Edward, and David Ryan Koes. 2021. <span>“SidechainNet: An All<span>-</span>Atom Protein Structure Dataset for Machine Learning.”</span> <em>Proteins: Structure, Function, and Bioinformatics</em> 89 (11): 1489–96. <a href="https://doi.org/10.1002/prot.26169">https://doi.org/10.1002/prot.26169</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="MichelNivard/Biological-language-models" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Chapter4_Proteins.html" class="pagination-link" aria-label="Protein contact maps from attention maps">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Protein contact maps from attention maps</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./summary.html" class="pagination-link" aria-label="Summary">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Summary</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>