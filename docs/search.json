[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Biological language models & Neural Networks",
    "section": "",
    "text": "Preface\nThese are my study notes on training DNA/RNA/Protein and other biomedical language models. The text/book is intended for people who want to casually explore the field before they on-ramp to actually training large DNA/Biological language models, or for their PIs, an anxious aging millennial or bitter but wise gen-X‚Äôers who want to be able to understand the next generation of computational genomics that is about to wash over us all.\nAt all times I‚Äôll try to add minimal biological context (though I am no biologist!) for people who have an ML background but no college bio experience and ill try to add context on ML concepts for those with a bio background but limited experience with language models.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#structure",
    "href": "index.html#structure",
    "title": "Biological language models & Neural Networks",
    "section": "Structure",
    "text": "Structure\nThe book is divided up into sections that deal with a specific modality or data type.\n\nDNA language models\nChapter 1 covers downloading (DNA) sequences data from ensembl and uploading it to Huggingface. Chapter 2 covers training a first small DNA sequence language model. in Chapter 3 we explore how you‚Äôd evaluate whether a DNA model is any good, is our model learning anything at all?\n\n\nRNA language models\n\n\nprotein language models\n\n\nEpidemiological language models",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#practicalities",
    "href": "index.html#practicalities",
    "title": "Biological language models & Neural Networks",
    "section": "Practicalities",
    "text": "Practicalities\nThe book is accompanied by scripts in both the R and Python programming languages. IF you want to code along, and aren‚Äôt training any models that need to out compete the state of the art you can run most of this on a macbook. Maybe you‚Äôll need to run a training run overnight. If you want a bit more performance you can use google colab for access to A100 GPUs training the DNABert we outline in Chapter 2 on 500k coding sequences from 13 species took ¬±6 hours on an A100 on Colab, which means that cost me ¬±4$ in colab credit.\nI can highly recommend using positron as an IDE when following along with this book (https://positron.posit.co), its a vscode fork, and a great alternative to RStudio. it has tooling integrated for datascience in both python and R and I can switch between python and R sessions instantly!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Biological language models & Neural Networks",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese notes are written by me, Michel Nivard, a professor of Epidemiology at the University of Bristol, and as this book is not a core outputs for my job, I rely heavily on LLMs to help me with spelling and formatting.\nThese study notes are influences by discussion with Robbee Wedow and Seyedeh Zahra Paylakhi with whom I work on relate projects.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Chapter1.html",
    "href": "Chapter1.html",
    "title": "1¬† Preparing DNA data for training",
    "section": "",
    "text": "1.1 Garbage in garbage out\nHigh-quality data is essential for training effective machine learning models. In natural language processing (NLP), researchers invest significant effort in curating clean datasets from raw internet text. For example a dataset like ‚Äòfineweb-edu‚Äô contains English text that is of very high quality. Models trained on fineweb-edu (and similar high quality datasets) will improve MUCH faster then the equivalent model trained on other less carefully processed and evaluated datasets.\nThose with experience with genetics will know most of what I am about to explain when it comes to the sources of genetic data and the file formats these come in. Those with an ML background will be very familiar with Huggingface and the amazing data/model/training integration their libraries offer. Assembing high quality genomics datasets for language modeling requires familiarity with both. When working with DNA, RNA, and protein sequences, the authoritative sources of data are specialized databases and infrastructures rather than data scraped from the internet. If you want to learn to train DNA/RNA/Protein-based language model, we must learn how, and where, to retrieve data and convert it into a structured format.\nIn this tutorial, we will begin by collecting coding DNA sequences (CDS) from the human genome using the Ensembl database via BioMart. The goal is to store these sequences in a format suitable for training a DNA-based language model. We will then upload the processed data to Huggingface, a platform for hosting datasets and models for machine learning tasks.\nThis chapter will introduce key biological and computational concepts, ensuring that both biology newcomers and those unfamiliar with language modeling tools can follow along.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#garbage-in-garbage-out",
    "href": "Chapter1.html#garbage-in-garbage-out",
    "title": "1¬† Preparing DNA data for training",
    "section": "",
    "text": "Relative training efficiency using a high quality dataset like fineweb-edu and other massive datasets of English language texts. Image obtained from: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#understanding-ensembl-and-biomart",
    "href": "Chapter1.html#understanding-ensembl-and-biomart",
    "title": "1¬† Preparing DNA data for training",
    "section": "1.2 Understanding Ensembl and BioMart",
    "text": "1.2 Understanding Ensembl and BioMart\nFortunately for us, there is decades of work cleaning up genomic data and we can just go and get it from US government funded websites, where it is deposited by the global scientific community. Ensembl is a genomic database that provides extensive annotations for various species, including humans. It offers access to gene sequences, transcript annotations, and protein-coding information. One of its most powerful tools is BioMart, a flexible data retrieval system that allows users to download specific genomic datasets easily.\nIf we want t work with the data in a language model its efficient to store it in a format that is tailored for machine learning libraries. Unlike text-based NLP datasets, genomic data is structured differently and must be properly formatted before use in a model.\n\n1.2.1 What Are FASTA Files?\nA FASTA file is a simple text-based format used for storing biological sequences. Each entry in a FASTA file consists of: 1. A header line (starting with &gt;), which contains metadata such as gene IDs and chromosome locations. 2. A sequence line, which contains the nucleotide or protein sequence.\nThere is a very comprehensive Wikipedia entry on the FASTA format.\n‚ÄúSequences may be¬†protein sequences¬†or¬†nucleic acid¬†sequences, and they can contain gaps or alignment characters (see¬†sequence alignment). Sequences are expected to be represented in the standard IUB/IUPAC¬†amino acid¬†and¬†nucleic acid¬†codes, with these exceptions: lower-case letters are accepted and are mapped into upper-case; a single hyphen or dash can be used to represent a gap character; and in amino acid sequences, U and * are acceptable letters (see below). Numerical digits are not allowed but are used in some databases to indicate the position in the sequence.‚Äù ((source: https://en.wikipedia.org/wiki/FASTA_format))\n\n\n\nNucleic Acid Code\nMeaning\nMnemonic\n\n\n\n\nA\nA\nAdenine\n\n\nC\nC\nCytosine\n\n\nG\nG\nGuanine\n\n\nT\nT\nThymine\n\n\nU\nU\nUracil\n\n\n(i)\ni\ninosine¬†(non-standard)\n\n\nR\nA or G (I)\npuRine\n\n\nY\nC, T or U\npYrimidines\n\n\nK\nG, T or U\nbases which are¬†Ketones\n\n\nM\nA or C\nbases with¬†aMino groups\n\n\nS\nC or G\nStrong interaction\n\n\nW\nA, T or U\nWeak interaction\n\n\nB\nnot A (i.e.¬†C, G, T or U)\nB¬†comes after A\n\n\nD\nnot C (i.e.¬†A, G, T or U)\nD¬†comes after C\n\n\nH\nnot G (i.e., A, C, T or U)\nH¬†comes after G\n\n\nV\nneither T nor U (i.e.¬†A, C or G)\nV¬†comes after U\n\n\nN\nA C G T U\nNucleic acid\n\n\n-\ngap of indeterminate length\n\n\n\n\nThe amino acid codes supported (22 amino acids and 3 special codes) are:\n\n\n\nAmino Acid Code\nMeaning\n\n\n\n\nA\nAlanine\n\n\nB\nAspartic acid¬†(D) or¬†Asparagine¬†(N)\n\n\nC\nCysteine\n\n\nD\nAspartic acid\n\n\nE\nGlutamic acid\n\n\nF\nPhenylalanine\n\n\nG\nGlycine\n\n\nH\nHistidine\n\n\nI\nIsoleucine\n\n\nJ\nLeucine¬†(L) or¬†Isoleucine¬†(I)\n\n\nK\nLysine\n\n\nL\nLeucine\n\n\nM\nMethionine/Start codon\n\n\nN\nAsparagine\n\n\nO\nPyrrolysine¬†(rare)\n\n\nP\nProline\n\n\nQ\nGlutamine\n\n\nR\nArginine\n\n\nS\nSerine\n\n\nT\nThreonine\n\n\nU\nSelenocysteine¬†(rare)\n\n\nV\nValine\n\n\nW\nTryptophan\n\n\nY\nTyrosine\n\n\nZ\nGlutamic acid¬†(E) or¬†Glutamine¬†(Q)\n\n\nX\nany\n\n\n*\ntranslation stop\n\n\n-\ngap of indeterminate length\n\n\n\nYou‚Äôll notice the FASTA format has a well defined structure, and it could be leveraged to build a complete tokenizer, for now though our 4 character (+6 special characters) tokenizer will have to do.\n\n\n1.2.2 Why Focus on Coding DNA Sequences (CDS)?\nIn the example, we retrieve the human coding DNA sequences (CDS), which represent the DNA sequence of protein-coding regions of genes.\nWhile our ultimate goal is to model the entire human genome‚Äîand potentially multiple genomes across species or individuals‚Äîsuch tasks require significant computational resources. The tutorials in this book are designed to be accessible, running efficiently on a MacBook or Google Colab. Therefore, we focus on CDS, which are highly structured DNA sequences within genes, that directly transcribed into RNA which is in turn translate into proteins. the Table below contains the direct translation from 3 letter DNA sequence to amino-acid (which are the building blocks of proteins).\n\n\n\nThe Genetic code to translate codins (3 leter DNA sequences) to amino-acids that are in turn the building blocks of proteins (source: https://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/_images/P7_image1.png)\n\n\nIn contrast, much of the rest of the genome consists of regulatory regions, which are more complex and less structured. CDS sequences provide a strong foundation for a small DNA-based language model because they contain well-defined biological structure, making them a logical starting point before expanding to more complex genomic elements.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#why-upload-dna-data-to-hugging-face",
    "href": "Chapter1.html#why-upload-dna-data-to-hugging-face",
    "title": "1¬† Preparing DNA data for training",
    "section": "1.3 Why Upload DNA Data to Hugging Face?",
    "text": "1.3 Why Upload DNA Data to Hugging Face?\nHugging Face provides a robust ecosystem for hosting and sharing datasets, particularly for machine learning applications. Some key advantages include: - Easy accessibility: Researchers and models can easily retrieve datasets. - Standardized format: Datasets are structured for seamless integration with deep learning frameworks. - Direct integration with Hugging Face tools: The data on the Hugging Face Hub integrates seamlessly with their Transformers and Trainer Python libraries, making it easy to load datasets and train models. - Version control and updates: Data can be refined and expanded over time.\nBy storing our dataset on Hugging Face, we enable efficient training and collaboration for DNA language modeling.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#the-script-downloading-and-formatting-human-cds-data",
    "href": "Chapter1.html#the-script-downloading-and-formatting-human-cds-data",
    "title": "1¬† Preparing DNA data for training",
    "section": "1.4 The Script: Downloading and Formatting Human CDS Data",
    "text": "1.4 The Script: Downloading and Formatting Human CDS Data\nBelow is the R script that downloads human CDS from Ensembl using BioMart, extracts metadata, and saves the processed data into a CSV file. the package we use, biomartr isn‚Äôt the official R package but its a great option! it has very extensive documentation, so if you want to download other sequences in the future make sure to start here: https://docs.ropensci.org/biomartr/\n# Install necessary packages\ninstall.packages(\"biomartr\", dependencies = TRUE)\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n  install.packages(\"BiocManager\")\n}\n\n# Load required libraries\nlibrary(Biostrings)\nlibrary(biomartr)\n\n# Download the human CDS dataset from Ensembl\nHS.cds.ensembl &lt;- getCDS(db = \"ensembl\", organism = \"Homo sapiens\", path = file.path(\"_ncbi_downloads\",\"CDS\"))\n\n# Read CDS data as a Biostrings object\nHuman_CDS &lt;- read_cds(file = HS.cds.ensembl, obj.type = \"Biostrings\")\n\n# Extract headers and sequences\nheaders &lt;- names(Human_CDS)\nsequences &lt;- as.character(Human_CDS)\n\n# Function to extract metadata from headers\nextract_metadata &lt;- function(header) {\n  transcript_id &lt;- sub(\"^&gt;([^ ]+).*\", \"\\\\1\", header)\n  chromosome &lt;- sub(\".*chromosome:([^ ]+).*\", \"\\\\1\", header)\n  start &lt;- sub(\".*chromosome:[^:]+:([^:]+).*\", \"\\\\1\", header)\n  end &lt;- sub(\".*chromosome:[^:]+:[^:]+:([^:]+).*\", \"\\\\1\", header)\n  strand &lt;- sub(\".*chromosome:[^:]+:[^:]+:[^:]+:([^ ]+).*\", \"\\\\1\", header)\n  gene_id &lt;- sub(\".*gene:([^ ]+).*\", \"\\\\1\", header)\n  gene_biotype &lt;- sub(\".*gene_biotype:([^ ]+).*\", \"\\\\1\", header)\n  transcript_biotype &lt;- sub(\".*transcript_biotype:([^ ]+).*\", \"\\\\1\", header)\n  gene_symbol &lt;- sub(\".*gene_symbol:([^ ]+).*\", \"\\\\1\", header)\n  description &lt;- sub(\".*description:(.*)\", \"\\\\1\", header)\n\n  list(\n    transcript_id = transcript_id,\n    chromosome = chromosome,\n    start = start,\n    end = end,\n    strand = strand,\n    gene_id = gene_id,\n    gene_biotype = gene_biotype,\n    transcript_biotype = transcript_biotype,\n    gene_symbol = gene_symbol,\n    description = description\n  )\n}\n\n# Extract metadata from headers\nmetadata_list &lt;- lapply(headers, extract_metadata)\nmetadata_df &lt;- do.call(rbind, lapply(metadata_list, as.data.frame))\nmetadata_df$sequence &lt;- sequences\n\n# Save data to CSV\nwrite.csv(metadata_df, \"genome_sequences.csv\", row.names = FALSE, quote = TRUE)\n\n# Print sample data\nhead(metadata_df)\nYou can run the script yourself, but I have also gone ahead and uploaded it to huggingface: https://huggingface.co/datasets/MichelNivard/Human-genome-CDS-GRCh38",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#summary",
    "href": "Chapter1.html#summary",
    "title": "1¬† Preparing DNA data for training",
    "section": "1.5 Summary",
    "text": "1.5 Summary\nIn this chapter, we: - Introduced Ensembl and BioMart as tools for retrieving genomic data. - Explained FASTA files and human CDS, which form the core of our dataset. - Discussed the advantages of uploading datasets to Hugging Face, emphasizing its integration with Transformers and Trainer libraries. - Provided an R script to download, process, and store human CDS in a structured format.\nIn the next chapter, we will explore preprocessing techniques like tokenization and strategies for encoding DNA sequences into a format suitable for training a deep learning language model, and we use Huggingface Transformers and Trainer library to train our first little DNA language model!",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter2.html",
    "href": "Chapter2.html",
    "title": "2¬† Training our first DNA Language Model",
    "section": "",
    "text": "2.1 Introduction\nNow that we have collected and formatted a DNA dataset in Chapter 1, we can proceed to train a language model using the (Modern)BERT model architecture. In this chapter, we will walk through the process of tokenizing DNA sequences, configuring a BERT model, and training it using the Masked Language Modeling (MLM) objective.\nWe will cover the utility and rational behing DNA language models, and the key concepts behind tokenization, the BERT model, and the idea of masked language modeling (MLM) before diving into the Python script that trains the actual model.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#why-would-we-train-dna-language-models",
    "href": "Chapter2.html#why-would-we-train-dna-language-models",
    "title": "2¬† Training our first DNA Language Model",
    "section": "2.2 Why would we train DNA language models?",
    "text": "2.2 Why would we train DNA language models?\nFor a full review of the utility of language models you should dig into the literature. I can recommend (Benegas et al. 2025) for example. Genomic language models (gLMs) apply AI techniques to DNA sequences, enabling breakthroughs in variant effect prediction, sequence design, and genomic analysis.\nLike larger language models like chatGPT DNA language models have ‚Äúemergent properties:. If you train a gnomic Language models (gLM) on the reference genome sequence of humans, and various other species then the model that emerges is able to detect damaging mutations, without ever being trained on mutations (as mutations are defined as deviations from the reference)(Benegas, Batra, and Song 2023). To assess functional constraints, a widely used metric is the¬†log-likelihood ratio (LLR)¬†between two alleles. This measures the probability of a nucleotide variant appearing in a given context, with lower probabilities indicating potential deleterious effects. This application will be one of the examples I use throughout, simply because my experience in genetics align with it.\nAnother key application is transfer learning, where pretrained gLMs improve predictions in tasks like gene expression and chromatin accessibility. However, training effective models is difficult due to the vast, complex, and often non-functional nature of genomes. Unlike protein models, gLMs struggle with limited genomic diversity in training data and require more sophisticated benchmarks for evaluation.\nFuture advancements will focus on improving long-range genomic interactions, integrating multimodal biological data, and refining sequence design for practical applications. Despite challenges, gLMs hold great promise for revolutionizing genome research, advancing genetic disease understanding, and enabling synthetic biology innovations.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#understanding-tokenization",
    "href": "Chapter2.html#understanding-tokenization",
    "title": "2¬† Training our first DNA Language Model",
    "section": "2.3 Understanding Tokenization",
    "text": "2.3 Understanding Tokenization\n\n2.3.1 What is a Tokenizer?\nA tokenizer is a fundamental component of any language model. Language models are essentially large multinomial models that predict the next token in a sequence based on previous tokens, or predict a masked token using the surrounding tokens. Since machine learning models operate on numerical representations, tokenized text must be converted into integers, which serve as indices for lookup in a vocabulary.\nThese integers, however, have no inherent numeric value‚Äîthey simply act as categorical labels that the model learns to associate with semantic meaning. For example, if we tokenize the sentence:\n\n‚ÄúThe quick brown fox jumps over the lazy dog‚Äù\n\nat the word level, we might obtain a numerical sequence like:\n\n[4, 123, 678, 89, 245, 983, 56, 4564]\n\nwhere each number corresponds to a word based on a pre-defined tokenization dictionary, such as:\n{\"the\": 4, \"quick\": 123, \"brown\": 678, \"fox\": 89, \"jumps\": 245, \"over\": 983, \"lazy\": 56, \"dog\": 4564}\nSimilarly, for DNA sequences, each nucleotide (A, T, C, G) is assigned a unique integer ID.\n\n\n2.3.2 Our DNA Tokenizer\nOur tokenizer uses a character-level approach, where each nucleotide is assigned a unique integer ID. Special tokens are also included for various purposes:\n\n[UNK] (unknown token)\n[PAD] (padding token for equal-length sequences)\n[CLS] (classification token, useful for downstream tasks)\n[SEP] (separator token, used in tasks like sequence-pair classification)\n[MASK] (used for masked language modeling training)\n\nPython Code:\nimport torch\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.pre_tokenizers import Split\nfrom transformers import PreTrainedTokenizerFast\n\n# --------------------------------\n# 1. DNA Tokenizer with Full FASTA Nucleic Acid Code\n# --------------------------------\n\n# Define vocabulary to include all FASTA nucleotides and symbols\ndna_vocab = {\n    \"A\": 0, \"T\": 1, \"C\": 2, \"G\": 3, \"N\": 4, \"U\": 5, \"i\": 6,  # Standard bases + Inosine\n    \"R\": 7, \"Y\": 8, \"K\": 9, \"M\": 10, \"S\": 11, \"W\": 12,  # Ambiguous bases\n    \"B\": 13, \"D\": 14, \"H\": 15, \"V\": 16,  # More ambiguity codes\n    \"-\": 17,  # Gap character\n    \"[UNK]\": 18, \"[PAD]\": 19, \"[CLS]\": 20, \"[SEP]\": 21, \"[MASK]\": 22\n}\n\n# Create tokenizer\ntokenizer = Tokenizer(WordLevel(vocab=dna_vocab, unk_token=\"[UNK]\"))\ntokenizer.pre_tokenizer = Split(\"\", \"isolated\")  # Character-level splitting\n\n# Convert to Hugging Face-compatible tokenizer\nhf_tokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\"\n)\n\n\n2.3.3 Other Tokenization Strategies for DNA, RNA, and Proteins\nWhile character-level tokenization is effective, other tokenization approaches can offer different tradeoffs:\n\n2.3.3.1 Byte Pair Encoding (BPE)\nBPE is widely used in NLP and allows frequently occurring patterns to be merged into subword units. While BPE could be useful for repetitive genome sequences, it may not capture biologically meaningful units.\n\n\n2.3.3.2 K-mer Tokenization\nK-mer tokenization groups nucleotides into fixed-length substrings (e.g., 3-mers like ‚ÄúATG‚Äù). This approach retains local sequence structure but can lead to a large vocabulary size.\n\n\n2.3.3.3 Tiktoken and Similar Models\nSome modern tokenization methods, such as Tiktoken, optimize speed and efficiency by precomputing merges. These are often optimized for large-scale NLP tasks but could be adapted for biological sequences.\nChoosing the best tokenizer depends on the specific modeling task. For example, RNA secondary structures or protein folding models might benefit from more complex tokenization strategies.\nSource: RPubs Tokenization Review",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#loading-and-tokenizing-the-dna-dataset",
    "href": "Chapter2.html#loading-and-tokenizing-the-dna-dataset",
    "title": "2¬† Training our first DNA Language Model",
    "section": "2.4 Loading and Tokenizing the DNA Dataset",
    "text": "2.4 Loading and Tokenizing the DNA Dataset\n\n2.4.1 Understanding the Dataset\nWe will use a pre-existing dataset, Human-genome-CDS-GRCh38, which contains coding sequences from the human genome.\n\n\n2.4.2 Tokenizing the Dataset\nTo prepare the dataset for training, we must apply the tokenizer to each sequence while ensuring:\n\nSequences are truncated or padded to a fixed length (512 tokens)\nUnwanted columns are removed\n\nPython Code:\nfrom datasets import load_dataset\n\ndataset_name = \"MichelNivard/Human-genome-CDS-GRCh38\"\ndataset = load_dataset(dataset_name)\n\ncolumn_name = \"sequence\"\n\ndef tokenize_function(examples):\n    return hf_tokenizer(examples[column_name], truncation=True, padding=\"max_length\", max_length=512)\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[column_name])\n\n\n2.4.3 Saving and Preparing the Dataset for Training\nOnce tokenized, we save the dataset for efficient access during training.\nPython Code:\ntokenized_dataset.save_to_disk(\"tokenized_dna_dataset\")",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#understanding-bert-and-masked-language-modeling-mlm",
    "href": "Chapter2.html#understanding-bert-and-masked-language-modeling-mlm",
    "title": "2¬† Training our first DNA Language Model",
    "section": "2.5 Understanding BERT and Masked Language Modeling (MLM)",
    "text": "2.5 Understanding BERT and Masked Language Modeling (MLM)\n\n2.5.1 What is BERT?\nBERT (Bidirectional Encoder Representations from Transformers) is a powerful transformer-based language model. Unlike traditional left-to-right models, BERT learns bidirectional context, allowing it to understand sequences more effectively.\nReturning to our earlier example sentence:\n\n‚ÄúThe quick brown fox jumps over the lazy dog‚Äù\n\nBERT does not process words one at a time but instead considers the entire sequence simultaneously. This bidirectional approach allows BERT to infer missing words based on context from both directions.\n\n\n2.5.2 What is Masked Language Modeling (MLM)?\nMLM is a self-supervised learning objective where the model learns by predicting missing tokens in a sequence. During training:\n\nSome tokens are randomly replaced with [MASK]\nThe model must predict the original token based on surrounding context\n\nFor example, if we mask the word ‚Äúfox‚Äù in our sentence:\n\n‚ÄúThe quick brown [MASK] jumps over the lazy dog‚Äù\n\nBERT will analyze the remaining words and attempt to predict ‚Äúfox.‚Äù\nThis technique enables BERT to learn useful representations without requiring labeled data.\n\n\n2.5.3 Understanding Transformer Layers, Attention Heads, and Hidden Size\nA transformer layer consists of self-attention and feed-forward layers that help the model learn relationships between tokens. The number of transformer layers determines how deep the model is.\nAn attention head is a component of the self-attention mechanism that learns different types of relationships within the data. Having multiple attention heads allows the model to capture various dependencies between tokens.\nReturning to our example:\n\nOne attention head might focus on subject-verb relationships, recognizing that ‚Äúfox‚Äù is the subject of ‚Äújumps.‚Äù\nAnother head might capture adjective-noun relationships, linking ‚Äúbrown‚Äù to ‚Äúfox.‚Äù\n\nThe hidden size defines the dimensionality of the model‚Äôs internal representations. A larger hidden size enables the model to capture more complex patterns, but also increases computational cost.\nBy stacking multiple transformer layers and attention heads, BERT gains a deep understanding of sentence structures.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#defining-the-bert-model-for-dna-sequences",
    "href": "Chapter2.html#defining-the-bert-model-for-dna-sequences",
    "title": "2¬† Training our first DNA Language Model",
    "section": "2.6 Defining the BERT Model for DNA Sequences",
    "text": "2.6 Defining the BERT Model for DNA Sequences\nWhile the ‚Äúquick brown fox‚Äù example helps us understand how BERT processes natural language, our goal is to apply the same principles to¬†DNA sequences. Instead of predicting missing words in a sentence, we want our model to learn¬†biological patterns¬†and¬†genomic structure¬†by predicting masked nucleotides within DNA sequences.\nIn¬†DNA modeling, understanding sequence context is just as critical as in language modeling. Just as BERT learns that ‚Äúfox‚Äù fits within a given sentence structure, our model should learn that¬†specific nucleotide sequences appear in biologically meaningful patterns. This could involve recognizing¬†gene coding regions, regulatory motifs, or conserved sequence elements¬†across different genomes.\nTo accomplish this, we define a¬†custom BERT model¬†designed specifically for processing DNA sequences. Unlike traditional text-based models, our DNA BERT model uses a¬†character-level vocabulary¬†of nucleotides (A, T, C, G) and special tokens to represent sequence structure. By leveraging¬†masked language modeling (MLM), the model will learn to predict missing nucleotides based on surrounding context, allowing it to capture meaningful genomic features.\nWith this in mind, let‚Äôs move forward and define our BERT architecture for DNA sequences.\nPython Code:\nfrom transformers import ModernBertConfig, ModernBertForMaskedLM\n\nconfig = ModernBertConfig(\n    vocab_size=len(dna_vocab),\n    hidden_size=256,\n    num_hidden_layers=8,\n    num_attention_heads=8,\n    intermediate_size=512,\n    max_position_embeddings=512,\n    type_vocab_size=1,\n)\nconfig.pad_token_id = dna_vocab[\"[PAD]\"]\nmodel = ModernBertForMaskedLM(config)\n\n2.6.1 Configuring Training for DNA BERT\nNow that we have defined our BERT model for DNA sequences, we need to set up the training process. This involves specifying various training hyperparameters, handling masked language modeling (MLM) data, and preparing for efficient learning.\nUnlike traditional NLP tasks where models are trained on massive text corpora, training on DNA sequences presents unique challenges. The structure of genomic data, sequence length, and biological patterns all influence how the model learns. Our configuration ensures that the training process is efficient while maintaining biological relevance.\n\n\n\n2.6.2 Setting Training Parameters\nTo train our DNA BERT model, we use the Hugging Face TrainingArguments class, which allows us to define key training settings. These include:\n\nBatch size: We set a batch size of 16 for both training and evaluation. This determines how many sequences are processed at once.\nLogging & Saving: We log loss every 50 steps and save model checkpoints every 100 steps to monitor training progress.\nLearning Rate: We use a learning rate of 5e-5, a common choice for transformer models that balances learning speed and stability.\nWeight Decay: A value of 0.01 is used to prevent overfitting by applying L2 regularization to model weights.\nTraining Steps: The model is trained for 4000 steps. This ensures sufficient learning without excessive computation.\nModel Saving: The model checkpoints are stored in ./bert-dna, allowing us to resume training if needed.\n\nPython Code:\ntraining_args = TrainingArguments(\n    output_dir=\"./bert-dna\",\n    overwrite_output_dir=True,\n    logging_steps=50,  # Log loss every step\n    save_steps=100,\n    save_total_limit=2,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    max_steps=4000,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    push_to_hub=False,\n    report_to=\"none\",  # Disables wandb logging\n)\n\n\n\n2.6.3 Preparing for Masked Language Modeling (MLM)\nSince we are training our DNA BERT model using masked language modeling (MLM), we need to handle masked tokens properly. This is done using the DataCollatorForLanguageModeling, which:\n\nRandomly masks nucleotides in the training sequences.\nCreates labels automatically, meaning the model learns by trying to predict these masked tokens.\nUses a masking probability of 5%, ensuring that a small but meaningful portion of the sequence is masked during training.\n\nBy applying MLM, we allow the model to generalize nucleotide relationships and capture sequence dependencies, just like how BERT learns relationships between words in text.\nPython Code:\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=hf_tokenizer,\n    mlm=True,\n    mlm_probability=0.05\n)\n\n\n\n2.6.4 Training the DNA BERT Model\nWith our configuration and data collator in place, we now train the model. We use the Hugging Face Trainer API, which simplifies the training process by handling:\n\nDataset iteration: Automatically loads and batches training sequences.\nGradient updates: Adjusts model weights based on training loss.\nLogging & saving: Tracks training progress and stores checkpoints.\n\nOnce training begins, the model will gradually learn nucleotide dependencies and improve its ability to predict missing DNA bases.\nPython Code:\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=hf_tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()\nyou set up free wandb logging (go to https://wandb.ai/site for more info) you can track your training runs online, wherever they are running. You then get a dashboard full of pretty loss vs progress plots like the one below which I screencapped about ¬± 30 minutes into training on my macbook.\n\n\n\n2.6.5 Saving the Trained Model\nAfter training completes, we save both the model and tokenizer so they can be used for future predictions or fine-tuning.\n\nThe model weights are stored in ./bert-dna, allowing us to reload the trained model.\nThe tokenizer is also saved, ensuring that input sequences can be processed the same way during inference.\n\nFinally, a success message is printed, confirming that the training process has been completed.\nPython Code:\n# Save the final model and tokenizer\ntrainer.save_model(\"./bert-dna\")\nhf_tokenizer.save_pretrained(\"./bert-dna\")\n\nprint(\"üéâ Training complete! Model saved to ./bert-dna\")\n\n\n2.6.6 Summary\nIn this section, we:\n\nDefined training hyperparameters such as batch size, learning rate, and training steps.\nUsed masked language modeling (MLM) to train the model on DNA sequences.\nLeveraged the Hugging Face Trainer API to automate model training.\nSaved the final trained model and tokenizer for future use.\n\nWith this trained model, we can now fine-tune or apply it to various genomic tasks, such as predicting genetic variations or classifying functional DNA sequences. In the next chapter, we will explore how to fine-tune our DNA BERT model for specific applications.\n\n\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. ‚ÄúDNA Language Models Are Powerful Predictors of Genome-Wide Variant Effects.‚Äù Proceedings of the National Academy of Sciences 120 (44). https://doi.org/10.1073/pnas.2311219120.\n\n\nBenegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun S. Song. 2025. ‚ÄúGenomic Language Models: Opportunities and Challenges.‚Äù Trends in Genetics, January. https://doi.org/10.1016/j.tig.2024.11.013.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter3.html",
    "href": "Chapter3.html",
    "title": "3¬† Evaluating DNA Language Models",
    "section": "",
    "text": "3.1 Introduction\nIn Chapters 1 and 2, we introduced the process of preparing DNA sequences and training a BERT language model for genomic data. In this chapter, we will turn our attention to how single nucleotide mutations can be systematically generated and evaluated using the trained DNA language model.\nThese synethetic mutations can form the basis for an evaluation of our DNA language model.\nThis chapter has two goals:",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#introduction",
    "href": "Chapter3.html#introduction",
    "title": "3¬† Evaluating DNA Language Models",
    "section": "",
    "text": "To explain to machine learning readers how to enumerate synonymous and missense mutations based on the standard genetic code, and why this is biologically meaningful. Consider the Kahn Academy ‚Äúap bio‚Äù course if this chapter doesn‚Äôt really offer enough for you https://www.khanacademy.org/science/ap-biology\nTo explain to bioinformatics and genetics readers how the Masked Language Modeling (MLM) objective provides a way to compute the pseudo-log-likelihood (PLL) of entire sequences and to score mutations in terms of their ‚Äúnaturalness‚Äù under the trained model.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#biological-background-the-genetic-code-and-mutation-types",
    "href": "Chapter3.html#biological-background-the-genetic-code-and-mutation-types",
    "title": "3¬† Evaluating DNA Language Models",
    "section": "3.2 Biological Background: The Genetic Code and Mutation Types",
    "text": "3.2 Biological Background: The Genetic Code and Mutation Types\nBefore diving into code, it‚Äôs useful to recall the basics of how DNA encodes proteins. DNA is transcribed into RNA, and RNA is translated into proteins using¬†codons, groups of three nucleotides. Each codon corresponds to a specific amino acid ‚Äî this mapping is called the¬†genetic code.\n\n\n\nFigure 3.1 DNA is translated to RNA then transcribed to amino-acids which form proteins. Source: https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/intro-to-gene-expression-central-dogma\n\n\nCrucially, some amino acids can be encoded by multiple codons, a property called¬†degeneracy. This degeneracy is why¬†synonymous mutations¬†exist ‚Äî changes in the DNA sequence that do not alter the resulting amino acid. In contrast,¬†missense mutations¬†alter the encoded amino acid, which may change protein function.\nThis distinction between¬†synonymous¬†and¬†missense¬†mutations will allow us to systematically categorize the impact of each possible single nucleotide substitution. Below is the standard genetic code table it contains a full translation from DNA to protein.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodon\nAmino Acid\nCodon\nAmino Acid\nCodon\nAmino Acid\nCodon\nAmino Acid\n\n\n\n\nTTT\nF\nTTC\nF\nTTA\nL\nTTG\nL\n\n\nTCT\nS\nTCC\nS\nTCA\nS\nTCG\nS\n\n\nTAT\nY\nTAC\nY\nTAA\nStop\nTAG\nStop\n\n\nTGT\nC\nTGC\nC\nTGA\nStop\nTGG\nW\n\n\nCTT\nL\nCTC\nL\nCTA\nL\nCTG\nL\n\n\nCCT\nP\nCCC\nP\nCCA\nP\nCCG\nP\n\n\nCAT\nH\nCAC\nH\nCAA\nQ\nCAG\nQ\n\n\nCGT\nR\nCGC\nR\nCGA\nR\nCGG\nR\n\n\nATT\nI\nATC\nI\nATA\nI\nATG\nM\n\n\nACT\nT\nACC\nT\nACA\nT\nACG\nT\n\n\nAAT\nN\nAAC\nN\nAAA\nK\nAAG\nK\n\n\nAGT\nS\nAGC\nS\nAGA\nR\nAGG\nR\n\n\nGTT\nV\nGTC\nV\nGTA\nV\nGTG\nV\n\n\nGCT\nA\nGCC\nA\nGCA\nA\nGCG\nA\n\n\nGAT\nD\nGAC\nD\nGAA\nE\nGAG\nE\n\n\nGGT\nG\nGGC\nG\nGGA\nG\nGGG\nG\n\n\n\nA single nucleotide mutation can cause:\n\nA synonymous mutation: The amino acid does not change, meaning the mutation is ‚Äúsilent‚Äù in terms of protein sequence.\n\nFor example in row 1 pf the table we see that if we mutate the codon TTT to TTC both before and after the mutation the amino-acid F (phe) is produced. While its not guaranteed by any means that a synonymous mutation is entirely harmless their very likely to be harmless.\n\nA missense mutation: The amino acid changes, potentially altering protein structure and function.\n\nFor example in row 1 pf the table we see that if we mutate the codon TTT to TTA the amino-acid F (phe) is replaced by L (leu) in the protein, potentially changing the function. While missense mutations aren‚Äôt always damaging, they are fare more likely to be damaging.\n\n\nEarlier we trained a DNA language model on coding sequences for humans, ad I actually expanded that to a training run of 2 epochs (the data was all used twice) on 500k sequences from 13 vertebrae species. This model should, with probabilities slightly above chance,",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#enumerating-all-single-nucleotide-mutants",
    "href": "Chapter3.html#enumerating-all-single-nucleotide-mutants",
    "title": "3¬† Evaluating DNA Language Models",
    "section": "3.3 Enumerating All Single-Nucleotide Mutants",
    "text": "3.3 Enumerating All Single-Nucleotide Mutants\nThe code in this section systematically generates¬†every possible single nucleotide substitution¬†across the input sequence. Since each codon consists of three nucleotides, and each nucleotide can mutate into three alternatives, there are up to 9 potential codon variants for each original codon.\n\n\n\n\n\n\nTip\n\n\n\nThe data generated by applying this ‚Äúmutator‚Äù to the DRD2 (Dopamine receptor D2) gene is on huggingface: https://huggingface.co/datasets/MichelNivard/DRD2-mutations\n\n\nFor each mutation, we check the¬†original amino acid¬†and the¬†new amino acid¬†using the standard genetic code table. This allows us to classify each mutation as either:\n\nSynonymous¬†‚Äî Same amino acid, no apparent change to the protein.\nMissense¬†‚Äî Different amino acid, potential change to protein function.\n\nThis step is crucial in genomics, where we often want to prioritize¬†functional variants¬†‚Äî mutations that actually change protein products, rather than silent changes that do not.\nI have a preference for R myself, so I wrote this specific job in R.We provide the gene sequence, starting at the start codon, I use the dopamine receptor gene DRD2. Based on the genetic code, which translates DNA to the amino-acids that eventually are produced, we then write code to mutate each codon in a gene.\nDRD2 &lt;- \"ATGGATCCACTGAATCTGTCCTGGTATGATGATGATCTGGAGAGGCAGAACTGGAGCCGGCCCTTCAACGGGTCAGACGGGAAGGCGGACAGACCCCACTACAACTACTATGCCACACTGCTCACCCTGCTCATCGCTGTCATCGTCTTCGGCAACGTGCTGGTGTGCATGGCTGTGTCCCGCGAGAAGGCGCTGCAGACCACCACCAACTACCTGATCGTCAGCCTCGCAGTGGCCGACCTCCTCGTCGCCACACTGGTCATGCCCTGGGTTGTCTACCTGGAGGTGGTAGGTGAGTGGAAATTCAGCAGGATTCACTGTGACATCTTCGTCACTCTGGACGTCATGATGTGCACGGCGAGCATCCTGAACTTGTGTGCCATCAGCATCGACAGGTACACAGCTGTGGCCATGCCCATGCTGTACAATACGCGCTACAGCTCCAAGCGCCGGGTCACCGTCATGATCTCCATCGTCTGGGTCCTGTCCTTCACCATCTCCTGCCCACTCCTCTTCGGACTCAATAACGCAGACCAGAACGAGTGCATCATTGCCAACCCGGCCTTCGTGGTCTACTCCTCCATCGTCTCCTTCTACGTGCCCTTCATTGTCACCCTGCTGGTCTACATCAAGATCTACATTGTCCTCCGCAGACGCCGCAAGCGAGTCAACACCAAACGCAGCAGCCGAGCTTTCAGGGCCCACCTGAGGGCTCCACTAAAGGAGGCTGCCCGGCGAGCCCAGGAGCTGGAGATGGAGATGCTCTCCAGCACCAGCCCACCCGAGAGGACCCGGTACAGCCCCATCCCACCCAGCCACCACCAGCTGACTCTCCCCGACCCGTCCCACCATGGTCTCCACAGCACTCCCGACAGCCCCGCCAAACCAGAGAAGAATGGGCATGCCAAAGACCACCCCAAGATTGCCAAGATCTTTGAGATCCAGACCATGCCCAATGGCAAAACCCGGACCTCCCTCAAGACCATGAGCCGTAGGAAGCTCTCCCAGCAGAAGGAGAAGAAAGCCACTCAGATGCTCGCCATTGTTCTCGGCGTGTTCATCATCTGCTGGCTGCCCTTCTTCATCACACACATCCTGAACATACACTGTGACTGCAACATCCCGCCTGTCCTGTACAGCGCCTTCACGTGGCTGGGCTATGTCAACAGCGCCGTGAACCCCATCATCTACACCACCTTCAACATTGAGTTCCGCAAGGCCTTCCTGAAGATCCTCCACTGCTGA\"}\nnchar(DRD2)/3\n\n# Genetic code table (Standard Code)\ngenetic_code &lt;- c(\n  \"TTT\"=\"F\", \"TTC\"=\"F\", \"TTA\"=\"L\", \"TTG\"=\"L\",\n  \"TCT\"=\"S\", \"TCC\"=\"S\", \"TCA\"=\"S\", \"TCG\"=\"S\",\n  \"TAT\"=\"Y\", \"TAC\"=\"Y\", \"TAA\"=\"Stop\", \"TAG\"=\"Stop\",\n  \"TGT\"=\"C\", \"TGC\"=\"C\", \"TGA\"=\"Stop\", \"TGG\"=\"W\",\n  \"CTT\"=\"L\", \"CTC\"=\"L\", \"CTA\"=\"L\", \"CTG\"=\"L\",\n  \"CCT\"=\"P\", \"CCC\"=\"P\", \"CCA\"=\"P\", \"CCG\"=\"P\",\n  \"CAT\"=\"H\", \"CAC\"=\"H\", \"CAA\"=\"Q\", \"CAG\"=\"Q\",\n  \"CGT\"=\"R\", \"CGC\"=\"R\", \"CGA\"=\"R\", \"CGG\"=\"R\",\n  \"ATT\"=\"I\", \"ATC\"=\"I\", \"ATA\"=\"I\", \"ATG\"=\"M\",\n  \"ACT\"=\"T\", \"ACC\"=\"T\", \"ACA\"=\"T\", \"ACG\"=\"T\",\n  \"AAT\"=\"N\", \"AAC\"=\"N\", \"AAA\"=\"K\", \"AAG\"=\"K\",\n  \"AGT\"=\"S\", \"AGC\"=\"S\", \"AGA\"=\"R\", \"AGG\"=\"R\",\n  \"GTT\"=\"V\", \"GTC\"=\"V\", \"GTA\"=\"V\", \"GTG\"=\"V\",\n  \"GCT\"=\"A\", \"GCC\"=\"A\", \"GCA\"=\"A\", \"GCG\"=\"A\",\n  \"GAT\"=\"D\", \"GAC\"=\"D\", \"GAA\"=\"E\", \"GAG\"=\"E\",\n  \"GGT\"=\"G\", \"GGC\"=\"G\", \"GGA\"=\"G\", \"GGG\"=\"G\"\n)\n\n# Function to get all mutations for a codon\nmutate_codon &lt;- function(codon, codon_index, full_sequence) {\n  nucleotides &lt;- c(\"A\", \"T\", \"C\", \"G\")\n  mutations &lt;- data.frame()\n  \n  original_aa &lt;- genetic_code[[codon]]\n  \n  for (pos in 1:3) {\n      original_base &lt;- substr(codon, pos, pos)\n      for (nuc in nucleotides) {\n          if (nuc != original_base) {\n              # Mutate the codon at this position\n              mutated_codon &lt;- codon\n              substr(mutated_codon, pos, pos) &lt;- nuc\n              mutated_aa &lt;- genetic_code[[mutated_codon]]\n              \n              # Create the mutated sequence\n              mutated_sequence &lt;- full_sequence\n              start &lt;- (codon_index - 1) * 3 + 1\n              substr(mutated_sequence, start, start+2) &lt;- mutated_codon\n              \n              mutation_type &lt;- if (mutated_aa == original_aa) \"synonymous\" else \"missense\"\n              \n              mutations &lt;- rbind(mutations, data.frame(\n                  codon_index = codon_index,\n                  position = pos,\n                  original_codon = codon,\n                  mutated_codon = mutated_codon,\n                  original_aa = original_aa,\n                  mutated_aa = mutated_aa,\n                  mutation_position = (codon_index -1)*3 + pos,\n                  mutation_type = mutation_type,\n                  sequence = mutated_sequence\n              ))\n          }\n      }\n  }\n  return(mutations)\n}\nTHen we write code to mutate all codons within the gene and save all the mutations, and add a column that indicates whether their missense or synonymous mutations.\n# Main function to process the whole sequence\nmutate_sequence &lt;- function(dna_sequence) {\n  codons &lt;- strsplit(dna_sequence, \"\")[[1]]\n  codons &lt;- sapply(seq(1, length(codons), by=3), function(i) paste(codons[i:(i+2)], collapse=\"\"))\n  all_mutations &lt;- data.frame()\n  \n  for (i in seq_along(codons)) {\n      codon &lt;- codons[i]\n      mutations &lt;- mutate_codon(codon, i, dna_sequence)\n      all_mutations &lt;- rbind(all_mutations, mutations)\n  }\n  return(all_mutations)\n}\n\n# Example usage\nsequence &lt;- DRD2\nmutations &lt;- mutate_sequence(sequence)\n\n\n# Filter synonymous and missense if needed\nsynonymous_mutations &lt;- subset(mutations, mutation_type == \"synonymous\")\nmissense_mutations &lt;- subset(mutations, mutation_type == \"missense\")\n\nsource &lt;- c(NA,\"wildtype\",DRD2)\n\noutput &lt;- rbind(source,mutations[,7:9])\n\n\nwrite.csv(file=\"DRD2_mutations.csv\",output)\nThe code can be used to generate massive amounts of mutations for validation, but missense vs synonymous is a very simple and crude distinction to make! to fully validate a DNA model we‚Äôll want to obtain additional external information and well do so in later chapters!",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#evaluating-base-position-likelihoods-with-a-bert-model",
    "href": "Chapter3.html#evaluating-base-position-likelihoods-with-a-bert-model",
    "title": "3¬† Evaluating DNA Language Models",
    "section": "3.4 Evaluating base position likelihoods with a BERT Model",
    "text": "3.4 Evaluating base position likelihoods with a BERT Model\nIn machine learning terms, the¬†MLM loss¬†is the¬†negative log likelihood (NLL)¬†of the correct nucleotide. For example, if the correct nucleotide is ‚ÄúA‚Äù at a given position, and the model assigns ‚ÄúA‚Äù a probability of 0.8, then the contribution to the loss is:\n\\[loss = ‚àíln(0.8) = 0.22\\]\nThe lower this value, the better the model‚Äôs confidence matches reality ‚Äî indicating that the nucleotide was expected. Near the end of training our models loss hovered around 1.09, meaning that the average true base had a predicted probability of ¬±34%. The loss is highly dependent on the tokenizer, for example if we would have used a more complex tokenizer with say 100 options for each next token (encoding for example all 3-mer combinations of bases: A,C,T,G,AA,AC,AT,AG etc etc until GGA,GGG) the the probability of geting the one correct token is way lower as the base rate is way lower!\nWhen we compute the¬†pseudo-log-likelihood (PLL)¬†for an entire sequence, we mask and score each position, adding up these log probabilities:\n\\[log‚Å°P(nucleotide_1)+log‚Å°P(nucleotide_2)+‚ãØ+log‚Å°P(nucleotide_n)‚Äã\\]\nThis sum is the¬†total log likelihood¬†of the sequence under the model ‚Äî it quantifies how natural the model thinks the sequence is.\nFirst we load the model I trained in Chapter 2, if you trained your own on more data, or for longer, or want to evaluate a different model you can load those yourself easily.\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport torch\nimport pandas as pd\n\n# Load model & tokenizer\nmodel_name = \"MichelNivard/DNABert-CDS-13Species-v0.1\"  # Replace if needed\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\nmodel.eval()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Maximum context length ‚Äî BERT's trained context window\nMAX_CONTEXT_LENGTH = 512\nThen we define 2 functions to compute the pseudo likelihood of the whole sequence up to 512 bases as that is the sequence length we trained DNABert for (in full scale applications you‚Äôd use a longer sequence length) and 2. the log likelihood ratio of the mutation vs the wildtype (original DRD2 sequence).\ndef compute_log_likelihood(sequence, tokenizer, model):\n    \"\"\"Compute pseudo-log-likelihood (PLL) for the first 512 bases.\"\"\"\n    tokens = tokenizer(sequence, return_tensors='pt', add_special_tokens=True)\n    input_ids = tokens['input_ids'].to(device)\n    attention_mask = tokens['attention_mask'].to(device)\n\n    log_likelihood = 0.0\n    seq_len = input_ids.shape[1] - 2  # Exclude [CLS] and [SEP]\n\n    with torch.no_grad():\n        for i in range(1, seq_len + 1):\n            masked_input = input_ids.clone()\n            masked_input[0, i] = tokenizer.mask_token_id\n\n            outputs = model(masked_input, attention_mask=attention_mask)\n            logits = outputs.logits\n\n            true_token_id = input_ids[0, i]\n            log_probs = torch.log_softmax(logits[0, i], dim=-1)\n            log_likelihood += log_probs[true_token_id].item()\n\n    return log_likelihood\n\n\ndef compute_mutant_log_likelihood_ratio(wild_type, mutant, position, tokenizer, model):\n    \"\"\"Compare wild type and mutant likelihood at a single position (within 512 bases).\"\"\"\n    assert len(wild_type) == len(mutant), \"Wild type and mutant must have the same length\"\n    assert wild_type[position] != mutant[position], f\"No mutation detected at position {position + 1}\"\n\n    tokens = tokenizer(wild_type[:MAX_CONTEXT_LENGTH], return_tensors='pt', add_special_tokens=True)\n    input_ids = tokens['input_ids'].to(device)\n    attention_mask = tokens['attention_mask'].to(device)\n\n    mask_position = position + 1  # Shift for [CLS] token\n\n    masked_input = input_ids.clone()\n    masked_input[0, mask_position] = tokenizer.mask_token_id\n\n    with torch.no_grad():\n        outputs = model(masked_input, attention_mask=attention_mask)\n        logits = outputs.logits\n\n        log_probs = torch.log_softmax(logits[0, mask_position], dim=-1)\n\n    wild_base_id = tokenizer.convert_tokens_to_ids(wild_type[position])\n    mutant_base_id = tokenizer.convert_tokens_to_ids(mutant[position])\n\n    log_prob_wild = log_probs[wild_base_id].item()\n    log_prob_mutant = log_probs[mutant_base_id].item()\n\n    return log_prob_wild - log_prob_mutant\n\n3.4.1 The Likelihood Ratio to evaluate mutations\nThe log-likelihood ratio (LLR) compares how much more (or less) likely the wild-type sequence is compared to a mutant sequence, given the DNA language model. Specifically, we compare the log likelihood of the correct wild-type nucleotide to the log likelihood of the mutant nucleotide at the mutated position only.\n\\[LLR = log ‚Å° P ( wild-type¬†nucleotide ‚à£ context ) ‚àí log ‚Å° P ( mutant¬†nucleotide ‚à£ context )\\]\nThis metric is widely used in bioinformatics because it focuses on the exact site of the mutation, instead of comparing entire sequences. A positive LLR indicates the wild-type is favored by the model (the mutation is unlikely and therefore possibly deliterious), while a negative LLR means the mutant is more likely (the mutation is neural or maybe even protectivr).\nWe then apply these functions to all the synthetic DRD2 mutations we generated (in the first 512 bases) to evaluate whether the DNABert we trained thinks the missense mutations are generally less likely, and therefore possibly damaging, given the model.\n# Load dataset directly from Hugging Face dataset repo\ndataset_url = \"https://huggingface.co/datasets/MichelNivard/DRD2-mutations/raw/main/DRD2_mutations.csv\"\ndf = pd.read_csv(dataset_url)\n\n# Find wild-type sequence\nwild_type_row = df[df['mutation_type'] == 'wildtype'].iloc[0]\nwild_type_sequence = wild_type_row['sequence'][:MAX_CONTEXT_LENGTH]\n\nresults = []\n\n# Process all sequences\nfor idx, row in df.iterrows():\n    sequence = row['sequence'][:MAX_CONTEXT_LENGTH]\n    mutation_type = row['mutation_type']\n    mutation_position = row['mutation_position'] - 1  # Convert 1-based to 0-based\n\n    # Skip mutants where the mutation position is beyond 512 bases\n    if mutation_type != 'wildtype' and mutation_position &gt;= MAX_CONTEXT_LENGTH:\n        continue\n\n    print(idx)\n\n    llr = None\n    log_prob_wild = None\n    prob_wild = None\n\n    if mutation_type != 'wildtype':\n        llr, log_prob_wild, prob_wild = compute_mutant_log_likelihood_ratio(\n            wild_type_sequence, sequence, int(mutation_position), tokenizer, model\n        )\n\n    # append results for each mutation:\n    results.append({\n        'sequence': sequence,\n        'mutation_type': mutation_type,\n        'pll': 0,\n        'llr': llr,\n        'wildtype_log_prob': log_prob_wild,\n        'wildtype_prob': prob_wild,\n        'mutation_position': mutation_position + 1\n    })\n\n\n# Convert to DataFrame for saving or inspection\nresults_df = pd.DataFrame(results)\n\n# Save or print results\nprint(results_df)\n\n# Optionally, save to CSV\nresults_df.to_csv(\"sequence_log_likelihoods.csv\", index=False)\n\n\n3.4.2 Language Models Provide Biological Insight trough the likelihood ratio\nWhy do we care about these log likelihoods and log likelihood ratios? Because they provide a direct, data-driven estimate of how plausible or ‚Äúnatural‚Äù each mutated sequence looks to the model compared to the wild type sequence. Since the model was trained on real DNA sequences, sequences with high likelihoods resemble biological reality, while sequences with low likelihoods deviate from patterns the model has learned. A high ‚Äúmutation log likelihood ratio‚Äù corresponds tot he model strongly favoring the reference sequences over the mutation. This test lets us flag potentially deleterious mutations (those with sharp likelihood drops), prioritize candidate variants for functional follow-up, or even explore adaptive evolution by identifying mutations that DNA BERT ‚Äúlikes‚Äù more than the wild-type\nTo explore our result here, we can plot the LLR versus the position within the DRD2 gene, this can give us insight into the location within the coding sequence where we find unlikely (and therefore potentially damaging) mutations. in the lot below a LOW LLR means the variant is unlikely. Most variants cluster around a neural LLR, consistent with some statistical noise.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter to only mutations (skip wildtype which has no llr)\nplot_df = results_df[results_df['mutation_type'].isin(['synonymous', 'missense'])].copy()\n\n\n# Optional: Clip LLR to avoid excessive sizes\nplot_df['size'] = plot_df['llr'].clip(-5, 5)  # LLRs smaller than -5 get maximum size\n\n# Scatter plot with enhanced size scaling\nplt.figure(figsize=(14, 5))\nsns.scatterplot(\n    x='mutation_position', \n    y='llr', \n    hue='mutation_type', \n    size='size',  # Use clipped size column\n    sizes=(20, 200),  # Bigger range for better visibility\n    alpha=0.7, \n    palette={'synonymous': 'green', 'missense': 'orange'},\n    data=plot_df\n)\nplt.axhline(0, color='gray', linestyle='--', label='Neutral LLR')\nplt.title('Mutation Log Likelihood Ratio (LLR) Along DRD2 Gene')\nplt.xlabel('Position in Gene')\nplt.ylabel('Log Likelihood Ratio (LLR)')\nplt.legend(title='Mutation Type', bbox_to_anchor=(1.02, 1), loc='upper left')\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\nFigure 3.2 The LLR for the mutation (y-axis) give the position in the DRD2 gne (x-axis). High values indicate the reference, or wild, type sequence is far more likely then the mutation. Very high LLR values are almost exclusively missense mutations, consistent with a DNA model able to pick up dewleterious variants based on its training\n\n\nIts obvious from Figure 3.2 that 1. really almost all very unlikely mutations (positive LLR) are missense mutations and 2. There are potentially certain locations within this particular coding sequence where there is an abundance of unlikely mutations packed closely together, these could be regions that are intolerant to deleterious mutations.\nIts important to not get overconfident in our predictions! Rember this is a relatively tiny DNA sequence model (¬±5m parameters) we trained on sequences for 13 fairly randomly picked vertebrae. Lets look at the likelihood of the true base in the references (wild-type) sequence given the model. The mean probability is 40% (Figure 3.3), given the model essentially is trying to pick between 4 tokens (G,C,T & A) 40% is considerably better then chance! Its also clear the probability is not even across the gene, the first few based are almost certain (almost all coding sequences in these species start with the start codon ATG, the model obviously learned this). After that there is quite a spread, which is logical I think, in many places across the sequence the specific base might be very obvious, as all three alternates might be missense mutations, but in other spot one, two or even all three alternate tokens might be synonymous, and perhaps even present int he analog gene int he other 12 species we trained our model on! This would make the model FAR less certain about the true base at that location.\n\n\n\nFigure 3.3 The probability of the base in the reference, or wild type, sequence given the DNABert model we trained. The model clearly performed above random (random guessing would be 1 in 4, or 25%).",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#summary",
    "href": "Chapter3.html#summary",
    "title": "3¬† Evaluating DNA Language Models",
    "section": "3.5 Summary",
    "text": "3.5 Summary\nThis chapter introduced key pieces that are esential for those who want to train, or just understand, DNA Language models.\n\nWe explored how to systematically generate all synonymous and missense mutations in a gene, these simuated mutations then form an important part in imnnitial evaluation of our model.\nWe discussed how to compute the log-likelihood. of a sequences, and log-likelihood ratio of a single mutation using DNA BERT. These metrics are a proxy for how natural the model considers each sequence.\nWe finally used these simulated mutation, some knowledge of biology (whether the mutations are synonymous or missense) to validate our language model actually did do some learning.\n\nThe analyses outlined in this chapter form the foundation for variant effect prediction using genomic language models.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4¬† Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Benegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. ‚ÄúDNA\nLanguage Models Are Powerful Predictors of Genome-Wide Variant\nEffects.‚Äù Proceedings of the National Academy of\nSciences 120 (44). https://doi.org/10.1073/pnas.2311219120.\n\n\nBenegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun\nS. Song. 2025. ‚ÄúGenomic Language Models: Opportunities and\nChallenges.‚Äù Trends in Genetics, January. https://doi.org/10.1016/j.tig.2024.11.013.",
    "crumbs": [
      "References"
    ]
  }
]