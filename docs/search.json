[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Biological Language Models & Neural Networks",
    "section": "",
    "text": "Preface\nThese are my study notes on training DNA/RNA/Protein and other biomedical language models. The text/book is intended for people who want to casually explore the field before they on-ramp to actually training large DNA/Biological language models, or for their PIs—anxious aging millennials or wise Gen-X’ers who want to be able to understand the next generation of computational genomics that is about to wash over us all.\nAt all times, I’ll try to add minimal biological context (though I am no biologist!) for people who have an ML background but no college bio experience, and I’ll try to add context on ML concepts for those with a bio background but limited experience with language models.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-times-we-live-in",
    "href": "index.html#the-times-we-live-in",
    "title": "Biological Language Models & Neural Networks",
    "section": "The Times We Live In",
    "text": "The Times We Live In\nMore than natural language models, biological/sequence language models rely heavily on NIH-funded databases, datasets, resources, and scientists. The data we train on was bought and paid for by taxpayers all over the globe. The Human Genome Project was to a great extent funded, directed, and conceived under the auspices of the US federal government, under both Democratic and Republican presidents. Had they not, pharmaceutical companies might have done it, and while those can be highly innovative, there would have been no space for startups, no space for Google DeepMind to come in and iterate, revolutionize, or grow biological modeling. There is no uproar over training data in biology because under the firm guidance of US federal policy, all the data sequencers generate is generally in the public domain, or accessible for those willing and able to meet ethical standards. All scientists reading this know this—should you find yourself as someone from Silicon Valley, from a well-funded startup even, take a beat and think through whether you’d stand a snowball’s chance in hell to compete if the next wave of sequence data isn’t public but generated inside Google/Microsoft/pharma. Then adjust your politics accordingly.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Biological Language Models & Neural Networks",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese notes are written by me, Michel Nivard, a professor of Epidemiology at the University of Bristol, and as this book is not a core output for my job, I rely on LLMs to help me with drafting, spelling, and formatting.\nThese study notes are influenced by discussions with Robbee Wedow and Seyedeh Zahra Paylakhi, with whom I work on related projects.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Preamble1.html",
    "href": "Preamble1.html",
    "title": "What is this Book About?",
    "section": "",
    "text": "A Brief Glossary of ML Model Types\nThis book is me trying to keep up with the current state of the art in ML for biology, with an initial focus on language models. When studying the latest hyped tools, it’s good to resist the temptation to be awed by the models. Some are great, and it can feel magical to see an unsupervised model pick up important biological signals and be very predictive by only processing sequence data! However, the current state of the art in biology, genetics especially, is remarkable—we know a lot about the genome, about how DNA is transcribed into RNA and then proteins, which proteins are conserved across evolution (i.e., essential for all life). So throughout, we have to keep in mind that in some domains, while it may feel (and actually be) remarkable that a language model picks up fundamental biology just from processing data, it might not be state of the art or even close to it.\nSupervised machine learning is a type of machine learning where a model learns to make predictions based on examples that come with known answers, or “labels.” In biology, this could mean training a model to predict whether a DNA sequence comes from a healthy or diseased tissue, or identifying which species a DNA sample belongs to. The model sees many examples where the input (the DNA sequence) is paired with the correct output (the label, like “healthy” or “diseased”), and learns to find patterns that link the two. Supervised learning is very powerful when we have lots of high-quality labeled data, but in biology, obtaining these labels can be expensive, time-consuming, and sometimes even impossible if we don’t know the “right answer” in advance.\nUnsupervised machine learning, in contrast, is used when we don’t have labels—the model only sees the raw data and has to find patterns on its own. This is especially useful in biology when exploring large datasets where the structure isn’t fully understood, such as grouping similar cells in single-cell RNA sequencing or discovering new subtypes of proteins. In the case of biological language models, the “language” is made up of sequences like DNA, RNA, or proteins. Unsupervised models, such as transformers trained on genome sequences, learn the “grammar” and “vocabulary” of these biological molecules just by seeing lots of sequences, without being told what they mean. This allows them to uncover hidden rules of biology, like which sequences are likely to code for stable proteins or which mutations might disrupt function.\nBiological language models have become particularly important because DNA, RNA, and proteins all follow sequential, language-like patterns—just as words form sentences, nucleotides form genes, and amino acids form proteins. By training on vast amounts of biological data in an unsupervised way, these models can learn useful representations of biological sequences, even without human-provided labels. Researchers can then use these pretrained models for many downstream tasks, such as predicting gene function, identifying regulatory regions, or studying how genetic variation might affect disease—combining the power of unsupervised learning to understand biology’s “language” with supervised learning for more targeted, disease-specific predictions.\nIn some cases, the boundary between supervised and unsupervised learning is blurry—for example, in protein language models trained to predict 3D structure from amino acid sequences. These models are not given simple “labels” like “healthy” or “diseased,” but they are provided with 3D structural information that acts as an open-ended example rather than a strict classification label. The model isn’t being asked to sort sequences into a few categories, but rather to learn a very rich and flexible relationship between sequence and structure. This kind of learning—where the system uses biological context to guide its training without explicit classification tasks—occupies a middle ground between supervised and unsupervised methods, illustrating how biological complexity often resists fitting into neat ML categories. A key difference between learning labels and learning open-ended structures is that learning labels is data reduction (from complex 1D sequence to two, or a few, labels) while structure prediction is data expansion (from 1D protein sequence to 3D spatial molecular map).\nIn sequence analysis, there are also biologically-driven models that sit outside traditional machine learning entirely, or only use minimal regression or statistical modeling. For example, methods to predict whether a missense mutation (a single amino acid change) is deleterious often rely on biological theory, such as identifying mutation-depleted regions—parts of the genome or protein where harmful mutations rarely appear in healthy populations. These models leverage evolutionary conservation, functional annotations, and biochemical properties to prioritize mutations for further study, sometimes incorporating simple regression to combine different biological signals into a final score. These biologically-informed approaches are critical in genomic medicine and show how biology itself can provide a strong prior for prediction, even without extensive machine learning.",
    "crumbs": [
      "What is this Book About?"
    ]
  },
  {
    "objectID": "Preamble2.html",
    "href": "Preamble2.html",
    "title": "How to Read this Book",
    "section": "",
    "text": "Practicalities\nThe book is accompanied by scripts in both the R and Python programming languages. I had to make some choices—some of the biological data repositories have great integrated Perl and R packages. I wouldn’t want to force people into Perl (especially not myself!). I am more comfortable wrangling the initial data in R than in Python, so here we are.\nIf you want to code along, rest assured you can run most of this on a MacBook. Maybe you’ll need to run a training run overnight a few times. If you want a bit more performance, or not have your MacBook turn into a space heater for 24 hours, you can use Google Colab for access to A100 GPUs. Training the DNABERT model we outline in Chapter 2 on 500k coding sequences from 13 species took ±6 hours on an A100 on Colab, which means that cost me ±$4 in Colab credit.\nThe GitHub repo that hosts the book will be populated with all the scripts I discuss and use. The data used to train the models, and some of the models themselves, will be hosted on Hugging Face (a repository of ML training data). I will try to make Jupyter notebooks available, though given my R background, I usually run Python in a REPL because that is what R people do…\nIf you come at this from an R background, and Python isn’t your native language, I can highly recommend using Positron as an IDE when following along with this book. It’s a VSCode derivative developed by Posit, the company that developed RStudio. Positron has tooling integrated for data science in both Python and R, and I can switch between Python and R sessions instantly!",
    "crumbs": [
      "How to Read this Book"
    ]
  },
  {
    "objectID": "Preamble2.html#structure",
    "href": "Preamble2.html#structure",
    "title": "How to Read this Book",
    "section": "Structure",
    "text": "Structure\nThe book is divided into sections that deal with a specific biological modality or data type. The last chapter in each section is a review of current models. It’s more common to begin a chapter reviewing what’s available out there, but given the novelty of these models, it makes sense to learn how they work before reading a review of what’s out there. There is a risk of the reader attributing insights to me simply because I describe it to you first. I’ll always cite my sources as if this is a peer-reviewed article, and you should assume most models we build together are directly, or indirectly, influenced by the literature. I also ask you do not cite this book other than for novel content, or to refer to it as teaching material—please, for models, architectures, and insights, cite the underlying empirical literature.\n\nDNA Language Models\nChapter 1 covers downloading and processing (DNA) sequence data from Ensembl and uploading it to Hugging Face. Chapter 2 covers training a first small DNA sequence language model; the model is a bog-standard language model meant for natural languages, simply applied to DNA. In Chapter 3, we explore how you’d evaluate whether a DNA model is any good—is our model learning anything at all? Then in Chapter 4, we explore evolutionary-aware encoders and how they relate to DNA language models. In Chapter 5 we compare the two model we trained on a number of tasks, getting a feel for comparatie evaluation. If you stuck with it and get to Chapter 6 you are ready for a brief review of existing DNA language models.",
    "crumbs": [
      "How to Read this Book"
    ]
  },
  {
    "objectID": "Preamble2.html#scaling-training",
    "href": "Preamble2.html#scaling-training",
    "title": "How to Read this Book",
    "section": "Scaling Training",
    "text": "Scaling Training\nAfter the book section on DNA models, we step back and consider scaling up model training. To train a full “production” model you’d need to scale from running things interactively on a MacBook, to a GPU in the cloud to 8 GPUs in a server. Conditional on me getting some funds and/or arranging HPC compute access I might even write about/run training on a whole rack of servers, each with 1-8 GPUs. When scaling we are confronted with a whole host of new issues around training stability and parallel compute.\n\nProtein Language Models\n\n\nMulti-modal Models (DNA Meets Proteins)",
    "crumbs": [
      "How to Read this Book"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html",
    "href": "Chapter1_DNA.html",
    "title": "1  Preparing DNA data for training",
    "section": "",
    "text": "1.1 Garbage in garbage out\nHigh-quality data is essential for training effective machine learning models. In natural language processing (NLP), researchers invest significant effort in curating clean datasets from raw internet text. For example, a dataset like ‘fineweb-edu’ contains English text that is of very high quality. Models trained on fineweb-edu (and similar high-quality datasets) will improve much faster than the equivalent model trained on other less carefully processed and evaluated datasets.\nThose with experience in genetics will know most of what I am about to explain when it comes to the sources of genetic data and the file formats these come in. Those with an ML background will be very familiar with Huggingface and the amazing data/model/training integration their libraries offer. Assembling high-quality genomics datasets for language modeling requires familiarity with both. When working with DNA, RNA, and protein sequences, the authoritative sources of data are specialized databases and infrastructures rather than data scraped from the internet. If you want to learn to train DNA/RNA/Protein-based language models, we must learn how, and where, to retrieve data and convert it into a structured format.\nIn this tutorial, we will begin by collecting coding DNA sequences (CDS) from the human genome using the Ensembl database via BioMart. The goal is to store these sequences in a format suitable for training a DNA-based language model. We will then upload the processed data to Huggingface, a platform for hosting datasets and models for machine learning tasks.\nThis chapter will introduce key biological and computational concepts, ensuring that both biology newcomers and those unfamiliar with language modeling tools can follow along.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#garbage-in-garbage-out",
    "href": "Chapter1_DNA.html#garbage-in-garbage-out",
    "title": "1  Preparing DNA data for training",
    "section": "",
    "text": "Relative training efficiency using a high-quality dataset like fineweb-edu and other massive datasets of English language texts. Image obtained from: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#understanding-ensembl-and-biomart",
    "href": "Chapter1_DNA.html#understanding-ensembl-and-biomart",
    "title": "1  Preparing DNA data for training",
    "section": "1.2 Understanding Ensembl and BioMart",
    "text": "1.2 Understanding Ensembl and BioMart\nFortunately for us, there is decades of work cleaning up genomic data, and we can just go and get it from US government-funded websites, where it is deposited by the global scientific community. Ensembl is a genomic database that provides extensive annotations for various species, including humans. It offers access to gene sequences, transcript annotations, and protein-coding information. One of its most powerful tools is BioMart, a flexible data retrieval system that allows users to download specific genomic datasets easily.\nIf we want to work with the data in a language model, it’s efficient to store it in a format that is tailored for machine learning libraries. Unlike text-based NLP datasets, genomic data is structured differently and must be properly formatted before use in a model.\n\n1.2.1 What Are FASTA Files?\nA FASTA file is a simple text-based format used for storing biological sequences. Each entry in a FASTA file consists of: 1. A header line (starting with &gt;), which contains metadata such as gene IDs and chromosome locations. 2. A sequence line, which contains the nucleotide or protein sequence.\nThere is a very comprehensive Wikipedia entry on the FASTA format.\n“Sequences may be protein sequences or nucleic acid sequences, and they can contain gaps or alignment characters (see sequence alignment). Sequences are expected to be represented in the standard IUB/IUPAC amino acid and nucleic acid codes, with these exceptions: lower-case letters are accepted and are mapped into upper-case; a single hyphen or dash can be used to represent a gap character; and in amino acid sequences, U and * are acceptable letters (see below). Numerical digits are not allowed but are used in some databases to indicate the position in the sequence.” ((source: https://en.wikipedia.org/wiki/FASTA_format))\n\n\n\nNucleic Acid Code\nMeaning\nMnemonic\n\n\n\n\nA\nA\nAdenine\n\n\nC\nC\nCytosine\n\n\nG\nG\nGuanine\n\n\nT\nT\nThymine\n\n\nU\nU\nUracil\n\n\n(i)\ni\ninosine (non-standard)\n\n\nR\nA or G (I)\npuRine\n\n\nY\nC, T or U\npYrimidines\n\n\nK\nG, T or U\nbases which are Ketones\n\n\nM\nA or C\nbases with aMino groups\n\n\nS\nC or G\nStrong interaction\n\n\nW\nA, T or U\nWeak interaction\n\n\nB\nnot A (i.e. C, G, T or U)\nB comes after A\n\n\nD\nnot C (i.e. A, G, T or U)\nD comes after C\n\n\nH\nnot G (i.e., A, C, T or U)\nH comes after G\n\n\nV\nneither T nor U (i.e. A, C or G)\nV comes after U\n\n\nN\nA C G T U\nNucleic acid\n\n\n-\ngap of indeterminate length\n\n\n\n\nThe amino acid codes supported (22 amino acids and 3 special codes) are:\n\n\n\nAmino Acid Code\nMeaning\n\n\n\n\nA\nAlanine\n\n\nB\nAspartic acid (D) or Asparagine (N)\n\n\nC\nCysteine\n\n\nD\nAspartic acid\n\n\nE\nGlutamic acid\n\n\nF\nPhenylalanine\n\n\nG\nGlycine\n\n\nH\nHistidine\n\n\nI\nIsoleucine\n\n\nJ\nLeucine (L) or Isoleucine (I)\n\n\nK\nLysine\n\n\nL\nLeucine\n\n\nM\nMethionine/Start codon\n\n\nN\nAsparagine\n\n\nO\nPyrrolysine (rare)\n\n\nP\nProline\n\n\nQ\nGlutamine\n\n\nR\nArginine\n\n\nS\nSerine\n\n\nT\nThreonine\n\n\nU\nSelenocysteine (rare)\n\n\nV\nValine\n\n\nW\nTryptophan\n\n\nY\nTyrosine\n\n\nZ\nGlutamic acid (E) or Glutamine (Q)\n\n\nX\nany\n\n\n*\ntranslation stop\n\n\n-\ngap of indeterminate length\n\n\n\nYou’ll notice the FASTA format has a well-defined structure, and it could be leveraged to build a complete tokenizer. For now, though, our 4-character (+6 special characters) tokenizer will have to do.\n\n\n1.2.2 Why Focus on Coding DNA Sequences (CDS)?\nIn the example, we retrieve the human coding DNA sequences (CDS), which represent the DNA sequence of protein-coding regions of genes.\nWhile our ultimate goal is to model the entire human genome—and potentially multiple genomes across species or individuals—such tasks require significant computational resources. The tutorials in this book are designed to be accessible, running efficiently on a MacBook or Google Colab. Therefore, we focus on CDS, which are highly structured DNA sequences within genes, that are directly transcribed into RNA which is in turn translated into proteins. The table below contains the direct translation from 3-letter DNA sequences to amino acids (which are the building blocks of proteins).\n\n\n\nThe Genetic code to translate codins (3-letter DNA sequences) to amino-acids that are in turn the building blocks of proteins (source: https://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/_images/P7_image1.png)\n\n\nIn contrast, much of the rest of the genome consists of regulatory regions, which are more complex and less structured. CDS sequences provide a strong foundation for a small DNA-based language model because they contain well-defined biological structure, making them a logical starting point before expanding to more complex genomic elements.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#why-upload-dna-data-to-hugging-face",
    "href": "Chapter1_DNA.html#why-upload-dna-data-to-hugging-face",
    "title": "1  Preparing DNA data for training",
    "section": "1.3 Why Upload DNA Data to Hugging Face?",
    "text": "1.3 Why Upload DNA Data to Hugging Face?\nHugging Face provides a robust ecosystem for hosting and sharing datasets, particularly for machine learning applications. Some key advantages include: - Easy accessibility: Researchers and models can easily retrieve datasets. - Standardized format: Datasets are structured for seamless integration with deep learning frameworks. - Direct integration with Hugging Face tools: The data on the Hugging Face Hub integrates seamlessly with their Transformers and Trainer Python libraries, making it easy to load datasets and train models. - Version control and updates: Data can be refined and expanded over time.\nBy storing our dataset on Hugging Face, we enable efficient training and collaboration for DNA language modeling.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#the-script-downloading-and-formatting-human-cds-data",
    "href": "Chapter1_DNA.html#the-script-downloading-and-formatting-human-cds-data",
    "title": "1  Preparing DNA data for training",
    "section": "1.4 The Script: Downloading and Formatting Human CDS Data",
    "text": "1.4 The Script: Downloading and Formatting Human CDS Data\nBelow is the R script that downloads human CDS from Ensembl using BioMart, extracts metadata, and saves the processed data into a CSV file. The package we use, biomartr, isn’t the official R package, but it’s a great option! It has very extensive documentation, so if you want to download other sequences in the future, make sure to start here: https://docs.ropensci.org/biomartr/\n# Install necessary packages\ninstall.packages(\"biomartr\", dependencies = TRUE)\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n  install.packages(\"BiocManager\")\n}\n\n# Load required libraries\nlibrary(Biostrings)\nlibrary(biomartr)\n\n# Download the human CDS dataset from Ensembl\nHS.cds.ensembl &lt;- getCDS(db = \"ensembl\", organism = \"Homo sapiens\", path = file.path(\"_ncbi_downloads\",\"CDS\"))\n\n# Read CDS data as a Biostrings object\nHuman_CDS &lt;- read_cds(file = HS.cds.ensembl, obj.type = \"Biostrings\")\n\n# Extract headers and sequences\nheaders &lt;- names(Human_CDS)\nsequences &lt;- as.character(Human_CDS)\n\n# Function to extract metadata from headers\nextract_metadata &lt;- function(header) {\n  transcript_id &lt;- sub(\"^&gt;([^ ]+).*\", \"\\\\1\", header)\n  chromosome &lt;- sub(\".*chromosome:([^ ]+).*\", \"\\\\1\", header)\n  start &lt;- sub(\".*chromosome:[^:]+:([^:]+).*\", \"\\\\1\", header)\n  end &lt;- sub(\".*chromosome:[^:]+:[^:]+:([^:]+).*\", \"\\\\1\", header)\n  strand &lt;- sub(\".*chromosome:[^:]+:[^:]+:[^:]+:([^ ]+).*\", \"\\\\1\", header)\n  gene_id &lt;- sub(\".*gene:([^ ]+).*\", \"\\\\1\", header)\n  gene_biotype &lt;- sub(\".*gene_biotype:([^ ]+).*\", \"\\\\1\", header)\n  transcript_biotype &lt;- sub(\".*transcript_biotype:([^ ]+).*\", \"\\\\1\", header)\n  gene_symbol &lt;- sub(\".*gene_symbol:([^ ]+).*\", \"\\\\1\", header)\n  description &lt;- sub(\".*description:(.*)\", \"\\\\1\", header)\n\n  list(\n    transcript_id = transcript_id,\n    chromosome = chromosome,\n    start = start,\n    end = end,\n    strand = strand,\n    gene_id = gene_id,\n    gene_biotype = gene_biotype,\n    transcript_biotype = transcript_biotype,\n    gene_symbol = gene_symbol,\n    description = description\n  )\n}\n\n# Extract metadata from headers\nmetadata_list &lt;- lapply(headers, extract_metadata)\nmetadata_df &lt;- do.call(rbind, lapply(metadata_list, as.data.frame))\nmetadata_df$sequence &lt;- sequences\n\n# Save data to CSV\nwrite.csv(metadata_df, \"genome_sequences.csv\", row.names = FALSE, quote = TRUE)\n\n# Print sample data\nhead(metadata_df)\nYou can run the script yourself, but I have also gone ahead and uploaded it to Huggingface: https://huggingface.co/datasets/MichelNivard/Human-genome-CDS-GRCh38",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#summary",
    "href": "Chapter1_DNA.html#summary",
    "title": "1  Preparing DNA data for training",
    "section": "1.5 Summary",
    "text": "1.5 Summary\nIn this chapter, we: - Introduced Ensembl and BioMart as tools for retrieving genomic data. - Explained FASTA files and human CDS, which form the core of our dataset. - Discussed the advantages of uploading datasets to Hugging Face, emphasizing its integration with Transformers and Trainer libraries. - Provided an R script to download, process, and store human CDS in a structured format.\nIn the next chapter, we will explore preprocessing techniques like tokenization and strategies for encoding DNA sequences into a format suitable for training a deep learning language model, and we use Huggingface Transformers and Trainer library to train our first little DNA language model!",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html",
    "href": "Chapter2_DNA.html",
    "title": "2  Training our first DNA Language Model",
    "section": "",
    "text": "2.1 Introduction\nNow that we have collected and formatted a DNA dataset in Chapter 1, we can proceed to train a language model using the (Modern)BERT model architecture. In this chapter, we will walk through the process of tokenizing DNA sequences, configuring a BERT model, and training it using the Masked Language Modeling (MLM) objective.\nWe will cover the utility and rationale behind DNA language models, and the key concepts behind tokenization, the BERT model, and the idea of masked language modeling (MLM) before diving into the Python script that trains the actual model.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html#why-would-we-train-dna-language-models",
    "href": "Chapter2_DNA.html#why-would-we-train-dna-language-models",
    "title": "2  Training our first DNA Language Model",
    "section": "2.2 Why would we train DNA language models?",
    "text": "2.2 Why would we train DNA language models?\nFor a full review of the utility of language models, you should dig into the literature. I can recommend (Benegas et al. 2025) for example. Genomic language models (gLMs) apply AI techniques to DNA sequences, enabling breakthroughs in variant effect prediction, sequence design, and genomic analysis.\nLike larger language models like ChatGPT, DNA language models (DNA-LM) have “emergent properties”. If you train genomic Language models (gLM, strictly speaking a slightly more general class of models than DNA-LMs, so also RNA or protein LMs) on the reference genome sequence of humans and various other species, then the model that emerges is able to detect damaging mutations, without ever being trained on mutations (as mutations are defined as deviations from the reference)(Benegas, Batra, and Song 2023). To assess functional constraints, a widely used metric is the log-likelihood ratio (LLR) between two alleles. This measures the probability of a nucleotide variant appearing in a given context, with lower probabilities indicating potential deleterious effects. This application will be one of the examples I use throughout, simply because my experience in genetics aligns with it.\nAnother key application is transfer learning, where pretrained DNA-LMs improve predictions in tasks like gene expression and chromatin accessibility. However, training effective models is difficult due to the vast, complex, and often non-functional nature of genomes. Unlike protein models, DNA-LMs struggle with limited genomic diversity in training data and require more sophisticated benchmarks for evaluation.\nFuture advancements will focus on improving long-range genomic interactions, integrating multimodal biological data, and refining sequence design for practical applications. Despite challenges, gLMs hold great promise for revolutionizing genome research, advancing genetic disease understanding, and enabling synthetic biology innovations.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html#understanding-tokenization",
    "href": "Chapter2_DNA.html#understanding-tokenization",
    "title": "2  Training our first DNA Language Model",
    "section": "2.3 Understanding Tokenization",
    "text": "2.3 Understanding Tokenization\n\n2.3.1 What is a Tokenizer?\nA tokenizer is a fundamental component of any language model. Language models are essentially large multinomial models that predict the next token in a sequence based on previous tokens, or predict a masked token using the surrounding tokens. Since machine learning models operate on numerical representations, tokenized text must be converted into integers, which serve as indices for lookup in a vocabulary.\nThese integers, however, have no inherent numeric value—they simply act as categorical labels that the model learns to associate with semantic meaning. For example, if we tokenize the sentence:\n\n“The quick brown fox jumps over the lazy dog”\n\nat the word level, we might obtain a numerical sequence like:\n\n[4, 123, 678, 89, 245, 983, 56, 4564]\n\nwhere each number corresponds to a word based on a pre-defined tokenization dictionary, such as:\n{\"the\": 4, \"quick\": 123, \"brown\": 678, \"fox\": 89, \"jumps\": 245, \"over\": 983, \"lazy\": 56, \"dog\": 4564}\nSimilarly, for DNA sequences, each nucleotide (A, T, C, G) is assigned a unique integer ID.\n\n\n2.3.2 Our DNA Tokenizer\nOur tokenizer uses a character-level approach, where each nucleotide is assigned a unique integer ID. Special tokens are also included for various purposes:\n\n[UNK] (unknown token)\n[PAD] (padding token for equal-length sequences)\n[CLS] (classification token, useful for downstream tasks)\n[SEP] (separator token, used in tasks like sequence-pair classification)\n[MASK] (used for masked language modeling training)\n\nPython Code:\nimport torch\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.pre_tokenizers import Split\nfrom transformers import PreTrainedTokenizerFast\n\n# --------------------------------\n# 1. DNA Tokenizer with Full FASTA Nucleic Acid Code\n# --------------------------------\n\n# Define vocabulary to include all FASTA nucleotides and symbols\ndna_vocab = {\n    \"A\": 0, \"T\": 1, \"C\": 2, \"G\": 3, \"N\": 4, \"U\": 5, \"i\": 6,  # Standard bases + Inosine\n    \"R\": 7, \"Y\": 8, \"K\": 9, \"M\": 10, \"S\": 11, \"W\": 12,  # Ambiguous bases\n    \"B\": 13, \"D\": 14, \"H\": 15, \"V\": 16,  # More ambiguity codes\n    \"-\": 17,  # Gap character\n    \"[UNK]\": 18, \"[PAD]\": 19, \"[CLS]\": 20, \"[SEP]\": 21, \"[MASK]\": 22\n}\n\n# Create tokenizer\ntokenizer = Tokenizer(WordLevel(vocab=dna_vocab, unk_token=\"[UNK]\"))\ntokenizer.pre_tokenizer = Split(\"\", \"isolated\")  # Character-level splitting\n\n# Convert to Hugging Face-compatible tokenizer\nhf_tokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\"\n)\n\n\n2.3.3 Other Tokenization Strategies for DNA, RNA, and Proteins\nWhile character-level tokenization is effective, other tokenization approaches can offer different tradeoffs:\n\n2.3.3.1 Byte Pair Encoding (BPE)\nBPE is widely used in NLP and allows frequently occurring patterns to be merged into subword units. While BPE could be useful for repetitive genome sequences, it may not capture biologically meaningful units.\n\n\n2.3.3.2 K-mer Tokenization\nK-mer tokenization groups nucleotides into fixed-length substrings (e.g., 3-mers like “ATG”). This approach retains local sequence structure but can lead to a large vocabulary size.\n\n\n2.3.3.3 Tiktoken and Similar Models\nSome modern tokenization methods, such as Tiktoken, optimize speed and efficiency by precomputing merges. These are often optimized for large-scale NLP tasks but could be adapted for biological sequences.\nChoosing the best tokenizer depends on the specific modeling task. For example, RNA secondary structures or protein folding models might benefit from more complex tokenization strategies.\nSource: RPubs Tokenization Review",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html#loading-and-tokenizing-the-dna-dataset",
    "href": "Chapter2_DNA.html#loading-and-tokenizing-the-dna-dataset",
    "title": "2  Training our first DNA Language Model",
    "section": "2.4 Loading and Tokenizing the DNA Dataset",
    "text": "2.4 Loading and Tokenizing the DNA Dataset\n\n2.4.1 Understanding the Dataset\nWe will use a pre-existing dataset, Human-genome-CDS-GRCh38, which contains coding sequences from the human genome.\n\n\n2.4.2 Tokenizing the Dataset\nTo prepare the dataset for training, we must apply the tokenizer to each sequence while ensuring:\n\nSequences are truncated or padded to a fixed length (512 tokens)\nUnwanted columns are removed\n\nPython Code:\nfrom datasets import load_dataset\n\ndataset_name = \"MichelNivard/Human-genome-CDS-GRCh38\"\ndataset = load_dataset(dataset_name)\n\ncolumn_name = \"sequence\"\n\ndef tokenize_function(examples):\n    return hf_tokenizer(examples[column_name], truncation=True, padding=\"max_length\", max_length=512)\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[column_name])\n\n\n2.4.3 Saving and Preparing the Dataset for Training\nOnce tokenized, we save the dataset for efficient access during training.\nPython Code:\ntokenized_dataset.save_to_disk(\"tokenized_dna_dataset\")",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html#understanding-bert-and-masked-language-modeling-mlm",
    "href": "Chapter2_DNA.html#understanding-bert-and-masked-language-modeling-mlm",
    "title": "2  Training our first DNA Language Model",
    "section": "2.5 Understanding BERT and Masked Language Modeling (MLM)",
    "text": "2.5 Understanding BERT and Masked Language Modeling (MLM)\n\n2.5.1 What is BERT?\nBERT (Bidirectional Encoder Representations from Transformers) is a powerful transformer-based language model. Unlike traditional left-to-right models, BERT learns bidirectional context, allowing it to understand sequences more effectively.\nReturning to our earlier example sentence:\n\n“The quick brown fox jumps over the lazy dog”\n\nBERT does not process words one at a time but instead considers the entire sequence simultaneously. This bidirectional approach allows BERT to infer missing words based on context from both directions.\n\n\n2.5.2 What is Masked Language Modeling (MLM)?\nMLM is a self-supervised learning objective where the model learns by predicting missing tokens in a sequence. During training:\n\nSome tokens are randomly replaced with [MASK]\nThe model must predict the original token based on surrounding context\n\nFor example, if we mask the word “fox” in our sentence:\n\n“The quick brown [MASK] jumps over the lazy dog”\n\nBERT will analyze the remaining words and attempt to predict “fox.”\nThis technique enables BERT to learn useful representations without requiring labeled data.\n\n\n2.5.3 Understanding the model: Transformer Layers, Attention Heads, and Hidden Size\nWe’ll briefly have to discuss the general architecture of the model, in bold the key elements of the model, in bold and italic parameters we get to set to determine the size of the model.\nA transformer model, like those used for DNA, RNA, or protein sequences, starts with an embedding layer, which plays a critical role in converting raw tokens — such as individual nucleotides (A, T, C, G) or amino acids — into numerical vectors that the model can process. Each token is mapped to a high-dimensional vector that captures some initial information about its identity and, in more advanced models, even its biochemical properties. This embedding layer acts as the interface between the raw sequence data and the deeper transformer layers, ensuring that the model works with continuous mathematical representations rather than raw symbolic letters.\n\n\n\n\n\n\nImportant\n\n\n\nThe embedding layers design is intimately related to the nature of the data, here we simply use a standard BERT embedding layer but in Chapter 4 & 5 we’ll dive deep into researchers efforts to design embedding layers for DNA specifically.\n\n\nA transformer layer consists of two key components: a self-attention mechanism and a feed-forward neural network. The self-attention mechanism allows the model to dynamically weigh the importance of every other token in the sequence when processing a given token, helping it learn relationships across different parts of the sequence — whether those are between neighboring amino acids or between distant regulatory elements in a long DNA strand. After the self-attention step, the feed-forward neural network processes each token’s representation independently, applying a small multi-layer perceptron (MLP) to transform the token’s internal representation. This helps the model refine and enrich the learned features, capturing nonlinear combinations of the attention-derived information. While self-attention captures interactions across the sequence, the feed-forward layer focuses on how to represent each token itself in a biologically meaningful way, helping the model identify local biochemical properties, sequence motifs, or structural preferences. The number of transformer layers (num_hidden_layers) determines how deep the model is, with more layers giving the model more capacity to learn complex biological relationships, but also increasing training time and data requirements.\nWithin each transformer layer, there are multiple attention heads (num_attention_heads), each focusing on different types of relationships within the data. In a natural language example, one attention head might capture subject-verb relationships, while another tracks adjective-noun pairs. In biological sequences, one attention head might learn to link binding motifs in promoters to transcription start sites, while another might focus on co-evolving residues in proteins that contribute to structural stability. The hidden size (hidden_size) refers to the dimensionality of these internal vector representations, defining how much information the model can store at each position. Larger hidden sizes allow the model to capture richer biological context, but they also increase computational cost. By combining deep transformer stacks, multiple attention heads, and flexible embeddings, biological language models can develop a powerful and nuanced understanding of biological sequences, helping researchers uncover new regulatory elements, predict protein folding, or study the effects of mutations.\nDefining the BERT Model for DNA Sequences\nWhile the “quick brown fox” example helps us understand how BERT processes natural language, our goal is to apply the same principles to DNA sequences. Instead of predicting missing words in a sentence, we want our model to learn biological patterns and genomic structure by predicting masked nucleotides within DNA sequences.\nIn DNA modeling, understanding sequence context is just as critical as in language modeling. Just as BERT learns that “fox” fits within a given sentence structure, our model should learn that specific nucleotide sequences appear in biologically meaningful patterns. This could involve recognizing gene coding regions, regulatory motifs, or conserved sequence elements across different genomes.\nTo accomplish this, we define a custom BERT model designed specifically for processing DNA sequences. Unlike traditional text-based models, our DNA BERT model uses a character-level vocabulary of nucleotides (A, T, C, G) and special tokens to represent sequence structure. By leveraging masked language modeling (MLM), the model will learn to predict missing nucleotides based on surrounding context, allowing it to capture meaningful genomic features.\nThe max_position_embeddings defines the longest sequence the model can process at once, which is crucial for biological sequences like genomes or proteins that can vary widely in length. To help the model understand where each token appears in the sequence, position embeddings are added to the token embeddings, giving the model a sense of order and distance, which is especially important when analyzing long-range interactions, like regulatory elements controlling distant genes.\nWith this in mind, let’s move forward and define a standard BERT architecture, which we’ll apply to DNA sequences. Because we’ll train a standard model, we can basically get away with defining the dimensions of certain aspects of the model.\nPython Code:\nfrom transformers import ModernBertConfig, ModernBertForMaskedLM\n\nconfig = ModernBertConfig(\n    vocab_size=len(dna_vocab),\n    hidden_size=256,\n    num_hidden_layers=8,\n    num_attention_heads=8,\n    intermediate_size=512,\n    max_position_embeddings=512,\n    type_vocab_size=1,\n)\nconfig.pad_token_id = dna_vocab[\"[PAD]\"]\nmodel = ModernBertForMaskedLM(config)\nThe keenly eyed among you see a lot of powers of 2, 8 is 2^3, 256, 512 are also powers of two, etc. Computer memory encodes in bits and bytes, and is designed around powers of two. Building matrices that are powers of 2, 16, 32, 64, etc. makes them fit in memory more efficiently, and this can have serious consequences for training efficiency (see Figure 2.1).\n\n\n\nFigure 2.1 the power of powers of two.\n\n\n\n\n2.5.4 Configuring Training for DNA BERT\nNow that we have defined our BERT model for DNA sequences, we need to set up the training process. This involves specifying various training hyperparameters, handling masked language modeling (MLM) data, and preparing for efficient learning.\nUnlike traditional NLP tasks where models are trained on massive text corpora, training on DNA sequences presents unique challenges. The structure of genomic data, sequence length, and biological patterns all influence how the model learns. Our configuration is again rather general and not yet tuned to DNA or biological data. If you would scale this model, you’d likely have to drop the learning rate down, for example. In “production” hyperparameter optimization becomes super important when you train a large model on all your data; each run might be costly, and setting optimal hyperparameters can lead to serious gains in training results.\n\n\n2.5.5 Setting Training Parameters\nTo train our DNA BERT model, we use the Hugging Face TrainingArguments class, which allows us to define key training settings. These include:\n\nBatch size: We set a batch size (per_device_train_batch_size) of 16 for both training and evaluation. This determines how many sequences are processed at once.\nLogging & Saving: We log loss every 50 steps (logging_steps) and save model checkpoints every 100 steps to monitor training progress.\nLearning Rate: We use a learning rate of 5e-5 (learning_rate), a common choice for transformer models that balances learning speed and stability.\nWeight Decay: A value of 0.01 is used to prevent overfitting by applying L2 regularization to model weights.\nTraining Steps: The model is trained for 4000 steps (max_steps), though on the Google Colab code I ran in the cloud, I trained for 2 whole epochs over all data (num_train_epochs = 2), which is a more precise way to ensure the model sees all the data twice.\nModel Saving: The model checkpoints are stored in ./bert-dna, allowing us to resume training from a checkpoint if needed (after a computer crash, or after the model going off the rails, etc.).\n\nPython Code:\ntraining_args = TrainingArguments(\n    output_dir=\"./bert-dna\",\n    overwrite_output_dir=True,\n    logging_steps=50,  # Log loss every step\n    save_steps=100,\n    save_total_limit=2,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    max_steps=4000,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    push_to_hub=False,\n    report_to=\"none\",  # Disables wandb logging, can enable if you have a wandb account so you can track your training\n)\nWhile I have enabled it here, I can recommend tracking your training runs on wandb. Go to wandb.ai (w and b meaning weights and biases, the core parameters in AI models) to make a free account. Now to some extent, this is like Strava but for AI, and there is a risk of obsessing over the training metadata. But if you find yourself with a limited amount of compute, or expensive compute paid per minute, it makes a lot of sense to track big training runs in real time so you can intervene. If the run crashes, you can restart, or abort the node so you aren’t paying for an expensive GPU node that’s no longer training.\n\n\n2.5.6 Preparing for Masked Language Modeling (MLM)\nSince we are training our DNA BERT model using masked language modeling (MLM), we need to handle introducing masked tokens properly. This is done using the DataCollatorForLanguageModeling, which:\n\nRandomly masks nucleotides in the training sequences.\nCreates labels automatically, meaning the model learns by trying to predict these masked tokens.\nUses a masking probability of 5%-15%, ensuring that a small but meaningful portion of the sequence is masked during training.\n\nBy applying MLM, we allow the model to generalize nucleotide relationships and capture sequence dependencies, just like how BERT learns relationships between words in text.\nPython Code:\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=hf_tokenizer,\n    mlm=True,\n    mlm_probability=0.05\n)\n\n\n2.5.7 Training the DNA BERT Model\nWith our configuration and data collator in place, we now train the model. We use the Hugging Face Trainer API, which simplifies the training process by handling:\n\nDataset iteration: Automatically loads and batches training sequences.\nGradient updates: Adjusts model weights based on training loss.\nLogging & saving: Tracks training progress and stores checkpoints.\n\nOnce training begins, the model will gradually learn nucleotide dependencies and improve its ability to predict missing DNA bases.\nPython Code:\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=hf_tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()\nIf you have set up a free wandb account, you can track your training runs, wherever they are running, on a central dashboard. You then get a dashboard full of pretty loss vs progress plots like the one below, which I screencapped about ± 30 minutes into training a tiny version of the model on my MacBook.\n\n\n\n2.5.8 Saving the Trained Model\nAfter training completes, we save both the model and tokenizer so they can be used for future predictions or fine-tuning.\n\nThe model weights are stored in ./bert-dna, allowing us to reload the trained model.\nThe tokenizer is also saved, ensuring that input sequences can be processed the same way during inference.\n\nFinally, a success message is printed, confirming that the training process has been completed.\nPython Code:\n# Save the final model and tokenizer\ntrainer.save_model(\"./bert-dna\")\nhf_tokenizer.save_pretrained(\"./bert-dna\")\n\nprint(\" Training complete! Model saved to ./bert-dna\")\nIf you intend to use, and re-use your model repeatedly, on different machines, or share it with others, it’s very convenient to save it to Hugging Face. If you have an account, you can do so for free using their internal tools. You’ll need to include an API token, I have omitted mine, and so should you when sharing code, because the API token lets people write to your account!\nmodel.push_to_hub(repo_id=\"MichelNivard/DNABert-CDS-13Species-v0.1\",use_auth_token=\"\")\nhf_tokenizer.push_to_hub(repo_id=\"MichelNivard/DNABert-CDS-13Species-v0.1\",use_auth_token=\"\")\nBecause we used a standard BERT model (BertModern), it’s super easy for others to pull the model weights from the hub into a model configured for use on their machine, using the Hugging Face Transformers library.\n\n\n2.5.9 Summary\nIn this section, we:\n\nDefined training hyperparameters such as batch size, learning rate, and training steps.\nUsed masked language modeling (MLM) to train the model to predict gaps in DNA sequences.\nLeveraged the Hugging Face Trainer API to automate model training.\nSaved the final trained model and tokenizer for future use.\n\nWith this trained model, we can now fine-tune or apply it to various genomic tasks, such as predicting genetic variations or classifying functional DNA sequences. In the next chapter, we will explore how to fine-tune our DNA BERT model for specific applications.\n\n\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. “DNA Language Models Are Powerful Predictors of Genome-Wide Variant Effects.” Proceedings of the National Academy of Sciences 120 (44). https://doi.org/10.1073/pnas.2311219120.\n\n\nBenegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun S. Song. 2025. “Genomic Language Models: Opportunities and Challenges.” Trends in Genetics, January. https://doi.org/10.1016/j.tig.2024.11.013.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html",
    "href": "Chapter3_DNA.html",
    "title": "3  Evaluating DNA Language Models",
    "section": "",
    "text": "3.1 Introduction\nIn Chapters 1 and 2, we introduced the process of preparing DNA sequences and training a BERT language model for genomic data. In this chapter, we will turn our attention to how single nucleotide mutations can be systematically generated and evaluated using the trained DNA language model.\nThese synthetic mutations can form the basis for an evaluation of our DNA language model.\nThis chapter has two goals:",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#introduction",
    "href": "Chapter3_DNA.html#introduction",
    "title": "3  Evaluating DNA Language Models",
    "section": "",
    "text": "To explain to machine learning readers how to enumerate synonymous and missense mutations based on the standard genetic code, and why this is biologically meaningful. Consider the Khan Academy “AP Bio” course if this chapter doesn’t really offer enough for you https://www.khanacademy.org/science/ap-biology\nTo explain to bioinformatics and genetics readers how the Masked Language Modeling (MLM) objective provides a way to compute the pseudo-log-likelihood (PLL) of entire sequences and to score mutations in terms of their “naturalness” under the trained model.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#biological-background-the-genetic-code-and-mutation-types",
    "href": "Chapter3_DNA.html#biological-background-the-genetic-code-and-mutation-types",
    "title": "3  Evaluating DNA Language Models",
    "section": "3.2 Biological Background: The Genetic Code and Mutation Types",
    "text": "3.2 Biological Background: The Genetic Code and Mutation Types\nBefore diving into code, it’s useful to recall the basics of how DNA encodes proteins. DNA is transcribed into RNA, and RNA is translated into proteins using codons, groups of three nucleotides. Each codon corresponds to a specific amino acid — this mapping is called the genetic code.\n\n\n\nFigure 3.1 DNA is translated to RNA then transcribed to amino acids which form proteins. Source: https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/intro-to-gene-expression-central-dogma\n\n\nCrucially, some amino acids can be encoded by multiple codons, a property called degeneracy. This degeneracy is why synonymous mutations exist — changes in the DNA sequence that do not alter the resulting amino acid. In contrast, missense mutations alter the encoded amino acid, which may change protein function.\nThis distinction between synonymous and missense mutations will allow us to systematically categorize the impact of each possible single nucleotide substitution. Below is the standard genetic code table; it contains a full translation from DNA to protein.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodon\nAmino Acid\nCodon\nAmino Acid\nCodon\nAmino Acid\nCodon\nAmino Acid\n\n\n\n\nTTT\nF\nTTC\nF\nTTA\nL\nTTG\nL\n\n\nTCT\nS\nTCC\nS\nTCA\nS\nTCG\nS\n\n\nTAT\nY\nTAC\nY\nTAA\nStop\nTAG\nStop\n\n\nTGT\nC\nTGC\nC\nTGA\nStop\nTGG\nW\n\n\nCTT\nL\nCTC\nL\nCTA\nL\nCTG\nL\n\n\nCCT\nP\nCCC\nP\nCCA\nP\nCCG\nP\n\n\nCAT\nH\nCAC\nH\nCAA\nQ\nCAG\nQ\n\n\nCGT\nR\nCGC\nR\nCGA\nR\nCGG\nR\n\n\nATT\nI\nATC\nI\nATA\nI\nATG\nM\n\n\nACT\nT\nACC\nT\nACA\nT\nACG\nT\n\n\nAAT\nN\nAAC\nN\nAAA\nK\nAAG\nK\n\n\nAGT\nS\nAGC\nS\nAGA\nR\nAGG\nR\n\n\nGTT\nV\nGTC\nV\nGTA\nV\nGTG\nV\n\n\nGCT\nA\nGCC\nA\nGCA\nA\nGCG\nA\n\n\nGAT\nD\nGAC\nD\nGAA\nE\nGAG\nE\n\n\nGGT\nG\nGGC\nG\nGGA\nG\nGGG\nG\n\n\n\nA single nucleotide mutation can cause:\n\nA synonymous mutation: The amino acid does not change, meaning the mutation is “silent” in terms of protein sequence.\n\nFor example, in row 1 of the table, we see that if we mutate the codon TTT to TTC, both before and after the mutation the amino acid F (phe) is produced. While it’s not guaranteed by any means that a synonymous mutation is entirely harmless, they’re very likely to be harmless.\n\nA missense mutation: The amino acid changes, potentially altering protein structure and function.\n\nFor example, in row 1 of the table, we see that if we mutate the codon TTT to TTA, the amino acid F (phe) is replaced by L (leu) in the protein, potentially changing the function. While missense mutations aren’t always damaging, they are far more likely to be damaging.\n\n\nEarlier, we trained a DNA language model on coding sequences for humans, and I actually expanded that to a training run of 2 epochs (the data was all used twice) on 500k sequences from 13 vertebrate species. This model should, with probabilities slightly above chance,",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#enumerating-all-single-nucleotide-mutants",
    "href": "Chapter3_DNA.html#enumerating-all-single-nucleotide-mutants",
    "title": "3  Evaluating DNA Language Models",
    "section": "3.3 Enumerating All Single-Nucleotide Mutants",
    "text": "3.3 Enumerating All Single-Nucleotide Mutants\nThe code in this section systematically generates every possible single nucleotide substitution across the input sequence. Since each codon consists of three nucleotides, and each nucleotide can mutate into three alternatives, there are up to 9 potential codon variants for each original codon.\n\n\n\n\n\n\nTip\n\n\n\nThe data generated by applying this “mutator” to the DRD2 (Dopamine receptor D2) gene is on Hugging Face: https://huggingface.co/datasets/MichelNivard/DRD2-mutations\n\n\nFor each mutation, we check the original amino acid and the new amino acid using the standard genetic code table. This allows us to classify each mutation as either:\n\nSynonymous — Same amino acid, no apparent change to the protein.\nMissense — Different amino acid, potential change to protein function.\n\nThis step is crucial in genomics, where we often want to prioritize functional variants — mutations that actually change protein products, rather than silent changes that do not.\nI have a preference for R myself, so I wrote this specific job in R. We provide the gene sequence, starting at the start codon; I use the dopamine receptor gene DRD2. Based on the genetic code, which translates DNA to the amino acids that eventually are produced, we then write code to mutate each codon in a gene.\nDRD2 &lt;- \"ATGGATCCACTGAATCTGTCCTGGTATGATGATGATCTGGAGAGGCAGAACTGGAGCCGGCCCTTCAACGGGTCAGACGGGAAGGCGGACAGACCCCACTACAACTACTATGCCACACTGCTCACCCTGCTCATCGCTGTCATCGTCTTCGGCAACGTGCTGGTGTGCATGGCTGTGTCCCGCGAGAAGGCGCTGCAGACCACCACCAACTACCTGATCGTCAGCCTCGCAGTGGCCGACCTCCTCGTCGCCACACTGGTCATGCCCTGGGTTGTCTACCTGGAGGTGGTAGGTGAGTGGAAATTCAGCAGGATTCACTGTGACATCTTCGTCACTCTGGACGTCATGATGTGCACGGCGAGCATCCTGAACTTGTGTGCCATCAGCATCGACAGGTACACAGCTGTGGCCATGCCCATGCTGTACAATACGCGCTACAGCTCCAAGCGCCGGGTCACCGTCATGATCTCCATCGTCTGGGTCCTGTCCTTCACCATCTCCTGCCCACTCCTCTTCGGACTCAATAACGCAGACCAGAACGAGTGCATCATTGCCAACCCGGCCTTCGTGGTCTACTCCTCCATCGTCTCCTTCTACGTGCCCTTCATTGTCACCCTGCTGGTCTACATCAAGATCTACATTGTCCTCCGCAGACGCCGCAAGCGAGTCAACACCAAACGCAGCAGCCGAGCTTTCAGGGCCCACCTGAGGGCTCCACTAAAGGAGGCTGCCCGGCGAGCCCAGGAGCTGGAGATGGAGATGCTCTCCAGCACCAGCCCACCCGAGAGGACCCGGTACAGCCCCATCCCACCCAGCCACCACCAGCTGACTCTCCCCGACCCGTCCCACCATGGTCTCCACAGCACTCCCGACAGCCCCGCCAAACCAGAGAAGAATGGGCATGCCAAAGACCACCCCAAGATTGCCAAGATCTTTGAGATCCAGACCATGCCCAATGGCAAAACCCGGACCTCCCTCAAGACCATGAGCCGTAGGAAGCTCTCCCAGCAGAAGGAGAAGAAAGCCACTCAGATGCTCGCCATTGTTCTCGGCGTGTTCATCATCTGCTGGCTGCCCTTCTTCATCACACACATCCTGAACATACACTGTGACTGCAACATCCCGCCTGTCCTGTACAGCGCCTTCACGTGGCTGGGCTATGTCAACAGCGCCGTGAACCCCATCATCTACACCACCTTCAACATTGAGTTCCGCAAGGCCTTCCTGAAGATCCTCCACTGCTGA\"\nnchar(DRD2)/3\n\n# Genetic code table (Standard Code)\ngenetic_code &lt;- c(\n  \"TTT\"=\"F\", \"TTC\"=\"F\", \"TTA\"=\"L\", \"TTG\"=\"L\",\n  \"TCT\"=\"S\", \"TCC\"=\"S\", \"TCA\"=\"S\", \"TCG\"=\"S\",\n  \"TAT\"=\"Y\", \"TAC\"=\"Y\", \"TAA\"=\"Stop\", \"TAG\"=\"Stop\",\n  \"TGT\"=\"C\", \"TGC\"=\"C\", \"TGA\"=\"Stop\", \"TGG\"=\"W\",\n  \"CTT\"=\"L\", \"CTC\"=\"L\", \"CTA\"=\"L\", \"CTG\"=\"L\",\n  \"CCT\"=\"P\", \"CCC\"=\"P\", \"CCA\"=\"P\", \"CCG\"=\"P\",\n  \"CAT\"=\"H\", \"CAC\"=\"H\", \"CAA\"=\"Q\", \"CAG\"=\"Q\",\n  \"CGT\"=\"R\", \"CGC\"=\"R\", \"CGA\"=\"R\", \"CGG\"=\"R\",\n  \"ATT\"=\"I\", \"ATC\"=\"I\", \"ATA\"=\"I\", \"ATG\"=\"M\",\n  \"ACT\"=\"T\", \"ACC\"=\"T\", \"ACA\"=\"T\", \"ACG\"=\"T\",\n  \"AAT\"=\"N\", \"AAC\"=\"N\", \"AAA\"=\"K\", \"AAG\"=\"K\",\n  \"AGT\"=\"S\", \"AGC\"=\"S\", \"AGA\"=\"R\", \"AGG\"=\"R\",\n  \"GTT\"=\"V\", \"GTC\"=\"V\", \"GTA\"=\"V\", \"GTG\"=\"V\",\n  \"GCT\"=\"A\", \"GCC\"=\"A\", \"GCA\"=\"A\", \"GCG\"=\"A\",\n  \"GAT\"=\"D\", \"GAC\"=\"D\", \"GAA\"=\"E\", \"GAG\"=\"E\",\n  \"GGT\"=\"G\", \"GGC\"=\"G\", \"GGA\"=\"G\", \"GGG\"=\"G\"\n)\n\n# Function to get all mutations for a codon\nmutate_codon &lt;- function(codon, codon_index, full_sequence) {\n  nucleotides &lt;- c(\"A\", \"T\", \"C\", \"G\")\n  mutations &lt;- data.frame()\n  \n  original_aa &lt;- genetic_code[[codon]]\n  \n  for (pos in 1:3) {\n      original_base &lt;- substr(codon, pos, pos)\n      for (nuc in nucleotides) {\n          if (nuc != original_base) {\n              # Mutate the codon at this position\n              mutated_codon &lt;- codon\n              substr(mutated_codon, pos, pos) &lt;- nuc\n              mutated_aa &lt;- genetic_code[[mutated_codon]]\n              \n              # Create the mutated sequence\n              mutated_sequence &lt;- full_sequence\n              start &lt;- (codon_index - 1) * 3 + 1\n              substr(mutated_sequence, start, start+2) &lt;- mutated_codon\n              \n              mutation_type &lt;- if (mutated_aa == original_aa) \"synonymous\" else \"missense\"\n              \n              mutations &lt;- rbind(mutations, data.frame(\n                  codon_index = codon_index,\n                  position = pos,\n                  original_codon = codon,\n                  mutated_codon = mutated_codon,\n                  original_aa = original_aa,\n                  mutated_aa = mutated_aa,\n                  mutation_position = (codon_index -1)*3 + pos,\n                  mutation_type = mutation_type,\n                  sequence = mutated_sequence\n              ))\n          }\n      }\n  }\n  return(mutations)\n}\n\n# Main function to process the whole sequence\nmutate_sequence &lt;- function(dna_sequence) {\n  codons &lt;- strsplit(dna_sequence, \"\")[[1]]\n  codons &lt;- sapply(seq(1, length(codons), by=3), function(i) paste(codons[i:(i+2)], collapse=\"\"))\n  all_mutations &lt;- data.frame()\n  \n  for (i in seq_along(codons)) {\n      codon &lt;- codons[i]\n      mutations &lt;- mutate_codon(codon, i, dna_sequence)\n      all_mutations &lt;- rbind(all_mutations, mutations)\n  }\n  return(all_mutations)\n}\n\n# Example usage\nsequence &lt;- DRD2\nmutations &lt;- mutate_sequence(sequence)\n\n# Filter synonymous and missense if needed\nsynonymous_mutations &lt;- subset(mutations, mutation_type == \"synonymous\")\nmissense_mutations &lt;- subset(mutations, mutation_type == \"missense\")\n\nsource &lt;- c(NA,\"wildtype\",DRD2)\n\noutput &lt;- rbind(source,mutations[,7:9])\n\nwrite.csv(file=\"DRD2_mutations.csv\",output)\n\n## Evaluating base position likelihoods with a BERT Model\n\nIn machine learning terms, the **MLM loss** is the **negative log likelihood (NLL)** of the correct nucleotide. For example, if the correct nucleotide is \"A\" at a given position, and the model assigns \"A\" a probability of 0.8, then the contribution to the loss is:\n\n$$loss = −ln(0.8) = 0.22$$\n\nThe lower this value, the better the model’s confidence matches reality — indicating that the nucleotide was expected. Near the end of training our models loss hovered around 1.09, meaning that the average true base had a predicted probability of ±34%. The loss is highly dependent on the tokenizer, for example if we would have used a more complex tokenizer with say 100 options for each next token (encoding for example all 3-mer combinations of bases: A,C,T,G,AA,AC,AT,AG etc etc until GGA,GGG) the the probability of geting the one correct token is way lower as the base rate is way lower!\n\nWhen we compute the **pseudo-log-likelihood (PLL)** for an entire sequence, we mask and score each position, adding up these log probabilities:\n\n$$log⁡P(nucleotide_1)+log⁡P(nucleotide_2)+⋯+log⁡P(nucleotide_n)​$$\n\nThis sum is the **total log likelihood** of the sequence under the model — it quantifies how natural the model thinks the sequence is.\n\nFirst we load the model I trained in Chapter 2, if you trained your own on more data, or for longer, or want to evaluate a different model you can load those yourself easily.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport torch\nimport pandas as pd\n\n# Load model & tokenizer\nmodel_name = \"MichelNivard/DNABert-CDS-13Species-v0.1\"  # Replace if needed\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\nmodel.eval()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Maximum context length — BERT's trained context window\nMAX_CONTEXT_LENGTH = 512\nThen we define two functions to compute: 1) the pseudo-likelihood of the whole sequence up to 512 bases, as that is the sequence length we trained DNABert for (in full-scale applications, you’d use a longer sequence length), and 2) the log-likelihood ratio of the mutation vs. the wildtype (original DRD2 sequence).\ndef compute_log_likelihood(sequence, tokenizer, model):\n    \"\"\"Compute pseudo-log-likelihood (PLL) for the first 512 bases.\"\"\"\n    tokens = tokenizer(sequence, return_tensors='pt', add_special_tokens=True)\n    input_ids = tokens['input_ids'].to(device)\n    attention_mask = tokens['attention_mask'].to(device)\n\n    log_likelihood = 0.0\n    seq_len = input_ids.shape[1] - 2  # Exclude [CLS] and [SEP]\n\n    with torch.no_grad():\n        for i in range(1, seq_len + 1):\n            masked_input = input_ids.clone()\n            masked_input[0, i] = tokenizer.mask_token_id\n\n            outputs = model(masked_input, attention_mask=attention_mask)\n            logits = outputs.logits\n\n            true_token_id = input_ids[0, i]\n            log_probs = torch.log_softmax(logits[0, i], dim=-1)\n            log_likelihood += log_probs[true_token_id].item()\n\n    return log_likelihood\n\n\ndef compute_mutant_log_likelihood_ratio(wild_type, mutant, position, tokenizer, model):\n    \"\"\"Compare wild type and mutant likelihood at a single position (within 512 bases).\"\"\"\n    assert len(wild_type) == len(mutant), \"Wild type and mutant must have the same length\"\n    assert wild_type[position] != mutant[position], f\"No mutation detected at position {position + 1}\"\n\n    tokens = tokenizer(wild_type[:MAX_CONTEXT_LENGTH], return_tensors='pt', add_special_tokens=True)\n    input_ids = tokens['input_ids'].to(device)\n    attention_mask = tokens['attention_mask'].to(device)\n\n    mask_position = position + 1  # Shift for [CLS] token\n\n    masked_input = input_ids.clone()\n    masked_input[0, mask_position] = tokenizer.mask_token_id\n\n    with torch.no_grad():\n        outputs = model(masked_input, attention_mask=attention_mask)\n        logits = outputs.logits\n\n        log_probs = torch.log_softmax(logits[0, mask_position], dim=-1)\n\n    wild_base_id = tokenizer.convert_tokens_to_ids(wild_type[position])\n    mutant_base_id = tokenizer.convert_tokens_to_ids(mutant[position])\n\n    log_prob_wild = log_probs[wild_base_id].item()\n    log_prob_mutant = log_probs[mutant_base_id].item()\n\n    return log_prob_wild - log_prob_mutant\n\n3.3.1 The Likelihood Ratio to evaluate mutations\nThe log-likelihood ratio (LLR) compares how much more (or less) likely the wild-type sequence is compared to a mutant sequence, given the DNA language model. Specifically, we compare the log likelihood of the correct wild-type nucleotide to the log likelihood of the mutant nucleotide at the mutated position only.\n\\[LLR = log ⁡ P ( wild-type nucleotide ∣ context ) − log ⁡ P ( mutant nucleotide ∣ context )\\]\nThis metric is widely used in bioinformatics because it focuses on the exact site of the mutation, instead of comparing entire sequences. A positive LLR indicates the wild-type is favored by the model (the mutation is unlikely and therefore possibly deleterious), while a negative LLR means the mutant is more likely (the mutation is neutral or maybe even protective).\nWe then apply these functions to all the synthetic DRD2 mutations we generated (in the first 512 bases) to evaluate whether the DNABert we trained thinks the missense mutations are generally less likely, and therefore possibly damaging, given the model.\n# Load dataset directly from Hugging Face dataset repo\ndataset_url = \"https://huggingface.co/datasets/MichelNivard/DRD2-mutations/raw/main/DRD2_mutations.csv\"\ndf = pd.read_csv(dataset_url)\n\n# Find wild-type sequence\nwild_type_row = df[df['mutation_type'] == 'wildtype'].iloc[0]\nwild_type_sequence = wild_type_row['sequence'][:MAX_CONTEXT_LENGTH]\n\nresults = []\n\n# Process all sequences\nfor idx, row in df.iterrows():\n    sequence = row['sequence'][:MAX_CONTEXT_LENGTH]\n    mutation_type = row['mutation_type']\n    mutation_position = row['mutation_position'] - 1  # Convert 1-based to 0-based\n\n    # Skip mutants where the mutation position is beyond 512 bases\n    if mutation_type != 'wildtype' and mutation_position &gt;= MAX_CONTEXT_LENGTH:\n        continue\n\n    print(idx)\n\n    llr = None\n    log_prob_wild = None\n    prob_wild = None\n\n    if mutation_type != 'wildtype':\n        llr, log_prob_wild, prob_wild = compute_mutant_log_likelihood_ratio(\n            wild_type_sequence, sequence, int(mutation_position), tokenizer, model\n        )\n\n    # append results for each mutation:\n    results.append({\n        'sequence': sequence,\n        'mutation_type': mutation_type,\n        'pll': 0,\n        'llr': llr,\n        'wildtype_log_prob': log_prob_wild,\n        'wildtype_prob': prob_wild,\n        'mutation_position': mutation_position + 1\n    })\n\n\n# Convert to DataFrame for saving or inspection\nresults_df = pd.DataFrame(results)\n\n# Save or print results\nprint(results_df)\n\n# Optionally, save to CSV\nresults_df.to_csv(\"sequence_log_likelihoods.csv\", index=False)\n\n\n3.3.2 Language Models Provide Biological Insight through the likelihood ratio\nWhy do we care about these log likelihoods and log likelihood ratios? Because they provide a direct, data-driven estimate of how plausible or “natural” each mutated sequence looks to the model compared to the wild type sequence. Since the model was trained on real DNA sequences, sequences with high likelihoods resemble biological reality, while sequences with low likelihoods deviate from patterns the model has learned. A high “mutation log likelihood ratio” corresponds to the model strongly favoring the reference sequences over the mutation. This test lets us flag potentially deleterious mutations (those with sharp likelihood drops), prioritize candidate variants for functional follow-up, or even explore adaptive evolution by identifying mutations that DNA BERT “likes” more than the wild-type\nTo explore our result here, we can plot the LLR versus the position within the DRD2 gene, this can give us insight into the location within the coding sequence where we find unlikely (and therefore potentially damaging) mutations. in the plot below a LOW LLR means the variant is unlikely. Most variants cluster around a neutral LLR, consistent with some statistical noise.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter to only mutations (skip wildtype which has no llr)\nplot_df = results_df[results_df['mutation_type'].isin(['synonymous', 'missense'])].copy()\n\n\n# Optional: Clip LLR to avoid excessive sizes\nplot_df['size'] = plot_df['llr'].clip(-5, 5)  # LLRs smaller than -5 get maximum size\n\n# Scatter plot with enhanced size scaling\nplt.figure(figsize=(14, 5))\nsns.scatterplot(\n    x='mutation_position', \n    y='llr', \n    hue='mutation_type', \n    size='size',  # Use clipped size column\n    sizes=(20, 200),  # Bigger range for better visibility\n    alpha=0.7, \n    palette={'synonymous': 'green', 'missense': 'orange'},\n    data=plot_df\n)\nplt.axhline(0, color='gray', linestyle='--', label='Neutral LLR')\nplt.title('Mutation Log Likelihood Ratio (LLR) Along DRD2 Gene')\nplt.xlabel('Position in Gene')\nplt.ylabel('Log Likelihood Ratio (LLR)')\nplt.legend(title='Mutation Type', bbox_to_anchor=(1.02, 1), loc='upper left')\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\nFigure 3.2 The LLR for the mutation (y-axis) give the position in the DRD2 gene (x-axis). High values indicate the reference, or wild, type sequence is far more likely than the mutation. Very high LLR values are almost exclusively missense mutations, consistent with a DNA model able to pick up deleterious variants based on its training\n\n\nIt’s obvious from Figure 3.2 that 1. really almost all very unlikely mutations (positive LLR) are missense mutations and 2. There are potentially certain locations within this particular coding sequence where there is an abundance of unlikely mutations packed closely together, these could be regions that are intolerant to deleterious mutations.\nIt’s important to not get overconfident in our predictions! Remember this is a relatively tiny DNA sequence model (±5m parameters) we trained on sequences for 13 fairly randomly picked vertebrate species. Let’s look at the likelihood of the true base in the references (wild-type) sequence given the model. The mean probability is 40% (Figure 3.3), given the model essentially is trying to pick between 4 tokens (G,C,T & A) 40% is considerably better than chance! It’s also clear the probability is not even across the gene, the first few bases are almost certain (almost all coding sequences in these species start with the start codon ATG, the model obviously learned this). After that, there is quite a spread, which is logical I think, in many places across the sequence the specific base might be very obvious, as all three alternates might be missense mutations, but in other spots one, two or even all three alternate tokens might be synonymous, and perhaps even present in the analog gene in the other 12 species we trained our model on! This would make the model FAR less certain about the true base at that location.\n\n\n\nFigure 3.3 The probability of the base in the reference, or wild type, sequence given the DNABert model we trained. The model clearly performed above random (random guessing would be 1 in 4, or 25%).",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#summary",
    "href": "Chapter3_DNA.html#summary",
    "title": "3  Evaluating DNA Language Models",
    "section": "3.4 Summary",
    "text": "3.4 Summary\nThis chapter introduced key pieces that are essential for those who want to train, or just understand, DNA Language models.\n\nWe explored how to systematically generate all synonymous and missense mutations in a gene, these simulated mutations then form an important part in initial evaluation of our model.\nWe discussed how to compute the log-likelihood of sequences, and log-likelihood ratio of a single mutation using DNA BERT. These metrics are a proxy for how natural the model considers each sequence.\nWe finally used these simulated mutations and some knowledge of biology (whether the mutations are synonymous or missense) to validate that our language model actually did do some learning.\n\nThe analyses outlined in this chapter form the foundation for variant effect prediction using genomic language models.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html",
    "href": "Chapter4_DNA.html",
    "title": "4  Evolution-Aware Encoders",
    "section": "",
    "text": "4.1 Introduction\nIn previous chapters, we introduced the basic principles of BERT for DNA sequences. We took inspiration from natural language processing (NLP), treating DNA as a language, where sequences of nucleotides (A, T, C, G, -) could be processed using transformers. This approach, while powerful, carries over several assumptions from natural language that do not perfectly align with biological sequences. In this chapter, we will re-examine how we encode genomic data and introduce a new design paradigm — evolutionary-aware encoding — inspired by the recently proposed GPN (Genomic Pre-trained Network).",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#tokenization-and-embedding-in-language-models",
    "href": "Chapter4_DNA.html#tokenization-and-embedding-in-language-models",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.2 Tokenization and Embedding in Language Models",
    "text": "4.2 Tokenization and Embedding in Language Models\nModern language models, whether BERT, GPT, or similar architectures, rely heavily on how input sequences are tokenized and encoded before they ever reach the attention layers. This initial step — often overlooked — plays a profound role in shaping how the model learns.\n\n4.2.1 Tokenization in Natural Language\nIn human languages like English or French, the vocabulary is large, often comprising tens of thousands of tokens. These tokens could be:\n\nWhole words (“cat”, “sat”).\nSubwords (“cat” might break into “c”, “at”).\nEven characters (in rare cases).\n\nSince the number of tokens is so large, each token is assigned a unique vector embedding — a dense, learnable representation of its “meaning”. These embeddings are gradually refined during training as the model learns how tokens behave in different contexts. The model learns, based on the massive amounts of training data, what the word means, what other words have similar or related meanings. This is essential because linguists and those who study language have vast knowledge of word meaning, numerically encoding that knowledge so that a computational model could process isn’t currently a feasible task. Therefore, in a natural (as opposed to biological) large language model, word embeddings are learned from the data, the data being all the text on the internet.\n\n\n4.2.2 The Embedding Process (NLP BERT)\nInput Sentence:  \"The cat sat on the mat\"\n\nStep 1 - Tokenization:\n    [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n\nStep 2 - Lookup:\n    Each token gets a fixed vector from an embedding table.\n\n    \"The\" -&gt; [0.25, 0.13, -0.11, ..., 0.04]\n    \"cat\" -&gt; [0.88, -0.23, 0.45, ..., -0.67]\n\nStep 3 - Transformer Layers:\n    These embeddings are updated based on surrounding words (context).\n\n\n4.2.3 Language Evolution is Decayed\nThe design of these token embeddings reflects a key fact about human languages: the evolutionary history of words might be relevant to understanding their meaning today, but the words’ context in text is way more informative. While linguistic etymology exists, the meaning of “cat” today does not rely on whether the word originated from Latin or Proto-Indo-European. Context (the words around “cat”) matters far more than distant etymology. Even if I am unfairly discounting the importance of etymology in linguistics (I am no linguist, don’t take my word for it), the quantity of older texts, relative to the quantity of modern texts, the lack of an obvious coding scheme for embedding a word in its etymological history are problematic and would have to be very effective given how effective “word in textual context” embeddings are. However, biology, and DNA in particular, is different.\n\n\n4.2.4 Biological Sequences are Fundamentally Different\nThe DNA encoding we have been working with (A, T, G, C, -) has 5 tokens, perhaps 20 if we encode all the codes used in genetics to code for ambiguous or missing bases. Protein language models we’ll cover later have ±20 amino-acids commonly found in proteins. If we use longer vocabularies, like k-mer or BPE tokenizer vocabularies, it’s not clear the longer sequences we obtain really are comparable or interchangeable. The point of embedding is to cluster similar and dissimilarities, in order to predict the next or a masked token if the presence of up to 128,000 tokens to choose from, some of which have very similar meanings or could fully alter the meaning of a sentence (by negation or omission). In biology, we have a small vocabulary, 5 or 20, or if you wish up to a few hundred tokens. We do, however, have an incredible understanding of the evolutionary history (Figure 4.1) of each base in the genome, we know its place in the genome of other species and can align those to each other!\n\n\n\nFigure 4.1 The Evogeneao Tree of Life diagram, all rights reserved Leonard Eisenberg (2008 & 2017). Get posters and relevant teaching materials here: https://www.evogeneao.com/en/learn/tree-of-life\n\n\n\n\n4.2.5 Evolutionary Context as an Embedding\nThe evolutionary history of a genomic position — how conserved it is, how it varies across species — directly influences our estimation of its importance and its tolerance to mutation. A nucleotide in a highly conserved enhancer region requires different levels of attention (from the model or us scientists) than a nucleotide in a rapidly evolving spacer.\n\n\n4.2.6 \n\nTable 4.1 Key Differences Between Language and DNA\n\n\n\n\n\n\n\nAspect\nNatural Language\nGenomics\n\n\n\n\nNumber of Tokens\nTens of thousands\n~5 (A, T, G, C, -)\n\n\nMeaning\nFlexible, evolves over time\nBiochemically fixed\n\n\nEvolutionary Context\nMostly irrelevant to meaning\nOften crucial (conservation, divergence)\n\n\nToken Embedding\nFully learned\nNo unique encoding for each token, but predefined based on token-specific evolutionary history\n\n\nNeighboring Context\nDefines meaning\nDefines local motifs, but evolutionary context adds extra layer\n\n\n\nTo capture this cross-species evolutionary context, we need an embedding strategy that combines:\n\nThe identity of the nucleotide itself (A, T, G, C, -).\nThe state of this position in aligned species (what bases appear at the same position in other species).\n\nThis evolutionary-aware encoding is at the heart of the Genomic Pre-trained Network (GPN) architecture and various famous protein language models like AlphaFold(Benegas et al. 2023; Lupo, Sgarbossa, and Bitbol 2022; Jumper et al. 2021). In DNA networks, we’ll discuss in this chapter, the encoding is computed for each base given its history. So while the model has 5 tokens (G, C, T, A, and -), these tokens do not map to a fixed embedding; rather, the base “A” maps to an encoding (one-hot encoding) for A, but then also for the same base in aligned sequences of 99 non-human species. This fundamentally changes the model architecture, changing it from a language model applied to DNA as we did in Chapter 3 to a DNA language model, or maybe even just a DNA model.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#introducing-gpn-msa-bert",
    "href": "Chapter4_DNA.html#introducing-gpn-msa-bert",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.3 4. Introducing GPN-MSA-BERT",
    "text": "4.3 4. Introducing GPN-MSA-BERT\nGPN-MSA-BERT (inspired by Benegas et al. (2023)) adapts BERT-style masked language modeling (MLM) to DNA sequences, but incorporates multispecies alignment (MSA) data directly into the model’s input.\n\n\n\nFigure 4.2 An example multiple sequence alignment (MSA) across 7 sequences (usually species). Source: https://www.biorender.com/template/multiple-sequence-alignment-dna author: Eunice Huang\n\n\n\n4.3.1 Key Idea: Dynamic Position Embeddings\nFor each position in the human genome, the model receives:\n\nThe human base (A, T, G, C, -) — this is the usual input.\nThe aligned bases from other species — these are additional features.\nThese aligned bases are one-hot encoded and concatenated to the human base’s embedding.\n\nThis turns a simple nucleotide embedding, for any given nucleotide, into a dynamic, position-specific vector that depends on its evolutionary context across species.\n\n\n4.3.2 Visualization\nHuman Position:     A\nAligned Species:    A  G  A  (species 1, species 2, species 3)\n\nEmbedding:\n    [ OneHot_A | OneHot_A | OneHot_G | OneHot_A ]\nThis combined vector captures:\n\nWhat the human base is.\nHow conserved the site is.\nWhich substitutions are tolerated across species.\n\n\n\n4.3.3 Practical Implementation - Replacing the BERT Encoder\nTo implement this in practice, we can directly modify a Hugging Face model class (like ModernBertForMaskedLM) to use our custom GPNEmbedding layer in place of the standard token embedding layer.\nThis requires:\n\nDefining a tokenizer that tokenizes each base and aligned bases in other species into the structure expected by the embedding.\nDefining a GPNEmbedding class that can handle one-hot human base with species features and builds the embedding for each base.\nReplacing ModernBertForMaskedLM with a custom GPNBERTMaskedLM class.\nEnsuring all forward methods accept both input_ids and aux_features, which are passed into the embedding layer.\nWe additionally define our own tokenizer and data collator (not shown here but available in the full script).\n\nThe code below takes a human sequence, encodes it in a one-hot encoding (so A: 10000, T: 01000, G: 00100, C: 00010, -: 00001 for example), does the same for any auxiliary aligned sequences from other species. Then the embedding function combines both into one.\n# --------------------------------\n# 5. Encode Human and Auxiliary Species Sequences in \n# --------------------------------\n\ndef one_hot_encode_base(base):\n    \"\"\"One-hot encodes A, T, G, C, - (5 bases total).\"\"\"\n    base_to_idx = {\"A\": 0, \"T\": 1, \"C\": 2, \"G\": 3, \"-\": 4}\n    one_hot = np.zeros(5, dtype=np.float32)\n    if base in base_to_idx:\n        one_hot[base_to_idx[base]] = 1.0\n    return one_hot\n\ndef tokenize_with_aux(examples):\n    human_seq = clean_sequence(examples[\"human_sequence\"])\n\n    # Drop first 10 species (closest relatives)\n    species_seqs = [clean_sequence(seq) for seq in examples[\"species_sequences\"]]\n    species_seqs = species_seqs[10:]  # &lt;-- This line omits the first 10 species\n\n    # Tokenize human sequence\n    tokens = hf_tokenizer(human_seq, truncation=True, padding=\"max_length\", max_length=512)\n    input_ids = tokens[\"input_ids\"]\n\n    # Process species sequences into concatenated one-hot vectors (aux features)\n    seq_len = len(input_ids)\n    num_species = len(species_seqs)\n\n    aux_features = np.zeros((seq_len, num_species * 5), dtype=np.float32)\n\n    for pos in range(seq_len):\n        if pos &gt;= len(human_seq):  # Handle padding case\n            break\n        for species_idx, species_seq in enumerate(species_seqs):\n            if pos &lt; len(species_seq):\n                aux_features[pos, species_idx * 5:(species_idx + 1) * 5] = one_hot_encode_base(species_seq[pos])\n\n    tokens[\"aux_features\"] = aux_features.tolist()\n    return tokens\n\n\n# --------------------------------\n# 8. Define GPNEmbedding \n# --------------------------------\nclass GPNEmbedding(nn.Module):\n    def __init__(self, config, n_species):\n        super().__init__()\n        self.config = config\n        self.n_species = n_species\n        self.vocab_size = 5  # A, T, G, C, -\n        self.species_feature_size = n_species * self.vocab_size\n\n    def forward(self, input_ids, aux_features):\n        one_hot = F.one_hot(input_ids, num_classes=self.config.vocab_size).float()\n\n        # Combine human one-hot with species aux_features\n        combined = torch.cat([one_hot, aux_features], dim=-1)\n\n        if combined.shape[-1] &lt; self.config.hidden_size:\n            pad = self.config.hidden_size - combined.shape[-1]\n            combined = F.pad(combined, (0, pad))\n\n        return combined\nFrom here on out, things are fairly standard. A transformer model (here BERT but could be anything really) is initialized to learn the relationship between adjacent tokens using a masked language model training regime. In the code below, the custom embeddings are introduced into the masked language model (ModernBert in this case) while the encoder part of the model (the core part of the model that learns the relation between adjacent tokens) remains unchanged.\n# --------------------------------\n# 6. GPNBERTMaskedLM\n# --------------------------------\n\nclass GPNBERTMaskedLM(nn.Module):\n    def __init__(self, config, n_species):\n        super().__init__()\n        self.config = config\n        self.n_species = n_species\n        self.vocab_size = 5  # A, T, G, C, -\n        self.species_feature_size = n_species * self.vocab_size\n\n        self.encoder = ModernBertModel(config)  # Directly initialize the transformer backbone\n        self.cls = nn.Linear(config.hidden_size, config.vocab_size)\n        self.embedding = GPNEmbedding(config, n_species)\n\n    def forward(self, input_ids=None, aux_features=None, labels=None, **kwargs):\n        embeddings = self.embedding(input_ids, aux_features)\n\n        # Only pass valid args to the encoder\n        encoder_kwargs = {k: v for k, v in kwargs.items() if k in {\"attention_mask\", \"position_ids\", \"head_mask\"}}\n\n        outputs = self.encoder(inputs_embeds=embeddings, **encoder_kwargs)\n\n        sequence_output = outputs.last_hidden_state\n        prediction_scores = self.cls(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n            loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        return MaskedLMOutput(\n            loss=loss,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#what-are-we-masking",
    "href": "Chapter4_DNA.html#what-are-we-masking",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.4 What are we masking?",
    "text": "4.4 What are we masking?\nI promised we’d have to deeply consider what we count as prediction, and that’s going to have to happen right now. In Chapter 2, we trained a DNA language model, and in Chapter 3, we saw how well it did and did not predict specific features. In Figure 3.3, you saw the model predicts the true bases in the DRD2 gene with about 40%. What if I told you I can predict the bases in the human reference genome with &gt; 95% probability with a “model” based on a supervised model? To do so, I’d just pick the consensus base across other species! Human DNA and chimpanzee DNA are &gt; 90% identical, and human and mouse genomes are remarkably similar (85%, I think). In Figure 4.3 (from (“Initial Sequencing and Comparative Analysis of the Mouse Genome” 2002)), we see the human sequences that are preserved well in the mouse genome, and they cover a staggering portion of it. This means we can “beat” base predictions made by our previous model by a mile, simply by picking the base that is most frequent across evolutionary history.\n\n\n\nFigure 4.3 The mouse genome, with sections of the human genome (color-coded) that are largely preserved across evolution (Figure 3 in (“Initial Sequencing and Comparative Analysis of the Mouse Genome” 2002))\n\n\nIn training our GPNBert model with auxiliary sequences, we train by masking the human base only (following (Benegas et al. 2023)). This very obviously and dramatically improves base prediction, and does so very quickly. After a few hundred iterations (trained on about 5,000 genes), the model learns that it should just assign the base most often found in other species. But as the evaluations in the original GPN-MSA paper make clear, eventually the model learns more than that. The model outperforms just picking the consensus base across species. They are able to show fairly convincingly that the predicted probabilities are a better predictor of the allele frequency in humans than just picking the allele frequency across species (Figure 2B in Benegas et al. (2023)) as an estimate of the allele frequency within humans. Furthermore, their model is able to identify deleterious mutations better than CADD scores, based on supervised machine learning ((Rentzsch et al. 2021)), or than sophisticated evolutionary constraint scores ((Sullivan et al. 2023)).",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#recap-of-our-approach",
    "href": "Chapter4_DNA.html#recap-of-our-approach",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.5 Recap of Our Approach",
    "text": "4.5 Recap of Our Approach\nIn Chapter 2, we trained a vanilla BERT on DNA sequences alone — treating DNA as just another language. That model only had access to the human sequence, with no evolutionary context.\nIn this chapter, we’ve re-imagined that process. Instead of treating A, T, G, C, - as abstract symbols, leaving the model entirely unsupervised when picking embeddings, we inject evolutionary history directly into the embedding. This allows our model to:\n\nUse the aligned species data as a rich evolutionary prior.\nStill leverage transformers for learning sequence motifs.\nPredict masked human bases using both local sequence and cross-species evolutionary patterns.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#preview-of-chapter-5",
    "href": "Chapter4_DNA.html#preview-of-chapter-5",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.6 Preview of Chapter 5",
    "text": "4.6 Preview of Chapter 5\nIn Chapter 5, we will put these two models — Vanilla BERT and GPN-BERT — to the test. We will evaluate their performance on:\n\nPredicting masked bases (MLM accuracy).\nPredicting the functional impact of mutations.\n\nThis head-to-head comparison will highlight the strengths and weaknesses of each approach and show the value of embedding evolutionary context directly into genomic language models.\n\n\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2023. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” http://dx.doi.org/10.1101/2023.10.10.561776.\n\n\n“Initial Sequencing and Comparative Analysis of the Mouse Genome.” 2002. Nature 420 (6915): 520–62. https://doi.org/10.1038/nature01262.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nLupo, Umberto, Damiano Sgarbossa, and Anne-Florence Bitbol. 2022. “Protein Language Models Trained on Multiple Sequence Alignments Learn Phylogenetic Relationships.” Nature Communications 13 (1). https://doi.org/10.1038/s41467-022-34032-y.\n\n\nRentzsch, Philipp, Max Schubach, Jay Shendure, and Martin Kircher. 2021. “CADD-Spliceimproving Genome-Wide Variant Effect Prediction Using Deep Learning-Derived Splice Scores.” Genome Medicine 13 (1). https://doi.org/10.1186/s13073-021-00835-9.\n\n\nSullivan, Patrick F., Jennifer R. S. Meadows, Steven Gazal, BaDoi N. Phan, Xue Li, Diane P. Genereux, Michael X. Dong, et al. 2023. “Leveraging Base-Pair Mammalian Constraint to Understand Genetic Variation and Human Disease.” Science 380 (6643). https://doi.org/10.1126/science.abn2937.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter5_DNA.html",
    "href": "Chapter5_DNA.html",
    "title": "5  Weaving Together Models",
    "section": "",
    "text": "5.1 Evaluating Model accuracy (code)\nWe have trained two models with very different architectures and goals. We can now compare these models (though it remains to be seen whether we can do so fairly). The first comparison we’ll make is whether the two models accurately predict the human base. This is an entirely unfair comparison—the deck is stacked massively towards the GPN-BERT model we trained in Chapter 4, as for that model we hard-code evolutionary history in the embedding, and when masking, we only mask the human base. So we’ll do a few things to level the playing field. First, we trained the GPN model with access to only 12 out of 100 auxiliary species. As you might recall, the DNABERT model developed and trained in Chapter 2, a version of which is available on Hugging Face (MichelNivard/DNABert-CDS-13Species-v0.1), was trained on human coding sequences and those of 12 further species. At a minimum, the two models saw approximately the same amount of genomic content during training (though the content is used in different ways). Then we also evaluate the GPN model while masking the auxiliary sequences (by setting all auxiliary species bases for the focal base to “-”). This scenario is comparable to inferences on a patient’s genome where the patient has a genomic feature (a small inversion, insertion, deletion, or duplication, etc.) that doesn’t align to other species since it is novel, but we still want to infer its potential deleteriousness. Our first hypothesis is that without the help of the auxiliary sequences, the GPN model’s accuracy takes a dive. Our second hypothesis is that for bases where the human base differs from the most frequently observed ancestral base, the GPN model accuracy will take a dive, but DNABERT might not. One of these hypotheses will prove true…\nBelow is a minimal code example where we 1. load the two models and write a helper function (get_predictions) that evaluates the likelihood of a given base in both models. This function can then be repeatedly applied to generate all kinds of comparisons. The full (and somewhat verbose) code for those evaluations is found in the script Chapter5_competative_eval.py available on GitHub: https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/DNA/Chapter_5\nThe important skill we learn here is to load two models, both trained on slightly different datasets, with slightly different DNA tokenizers, and apply them both to a single dataset, so we can make a direct comparison.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Weaving Together Models</span>"
    ]
  },
  {
    "objectID": "Chapter5_DNA.html#evaluating-model-accuracy-code",
    "href": "Chapter5_DNA.html#evaluating-model-accuracy-code",
    "title": "5  Weaving Together Models",
    "section": "",
    "text": "# --------------------------------\n# 1. Competative evaluations!!\n# --------------------------------\n\n# Load GPN-enhanced ModernBERT (your custom model)\ndevice = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n\nmodel_gpn = torch.load(\"./bert-dna-gpn/gpn_bert_model.pt\") # Assuming it's already trained and loaded\ntokenizer_gpn = AutoTokenizer.from_pretrained(\"./bert-dna-gpn\")\n# Load the full model\nmodel_gpn.eval()\n\n# Load DNABert-CDS-13Species\nmodel_name_dnabert = \"MichelNivard/DNABert-CDS-13Species-v0.1\"\ntokenizer_dnabert = AutoTokenizer.from_pretrained(model_name_dnabert)\nmodel_dnabert = AutoModelForMaskedLM.from_pretrained(model_name_dnabert).to(device)\nmodel_dnabert.eval()\n\n# Helper to get vocab mapping\nid_to_token_gpn = {v: k for k, v in tokenizer_gpn.get_vocab().items()}\nid_to_token_dnabert = {v: k for k, v in tokenizer_dnabert.get_vocab().items()}\n\n\n# Helper function to get predictions of the same base for both models it requires all the model-specific elements, the models themselves, attention masks, tokenizers, tokenized inputs, for both models.\n\ndef get_predictions(pos, input_ids_gpn, attention_mask_gpn, aux_features,\n                   input_ids_dnabert, attention_mask_dnabert,\n                   model_gpn, model_dnabert, tokenizer_gpn, tokenizer_dnabert,\n                   device):\n    \"\"\"Helper function to get predictions from both models for a specific position\"\"\"\n    \n    # Mask the position in both models\n    masked_input_ids_gpn = input_ids_gpn.clone()\n    masked_input_ids_gpn[0, pos] = tokenizer_gpn.mask_token_id\n    \n    masked_input_ids_dnabert = input_ids_dnabert.clone()\n    masked_input_ids_dnabert[0, pos] = tokenizer_dnabert.mask_token_id\n    \n    # Get predictions from GPN\n    with torch.no_grad():\n        output_gpn = model_gpn(\n            input_ids=masked_input_ids_gpn,\n            attention_mask=attention_mask_gpn,\n            aux_features=aux_features\n        )\n        logits_gpn = output_gpn.logits\n        log_probs_gpn = torch.log_softmax(logits_gpn[0, pos], dim=-1)\n        \n        # Get predictions from DNABERT\n        output_dnabert = model_dnabert(\n            masked_input_ids_dnabert,\n            attention_mask=attention_mask_dnabert\n        )\n        logits_dnabert = output_dnabert.logits\n        log_probs_dnabert = torch.log_softmax(logits_dnabert[0, pos], dim=-1)\n    \n    # Get top predictions\n    top_preds_gpn = torch.topk(log_probs_gpn, k=4)\n    top_preds_dnabert = torch.topk(log_probs_dnabert, k=4)\n    \n    return {\n        'gpn_probs': top_preds_gpn,\n        'dnabert_probs': top_preds_dnabert\n    }",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Weaving Together Models</span>"
    ]
  },
  {
    "objectID": "Chapter5_DNA.html#evaluating-model-accuracy-results",
    "href": "Chapter5_DNA.html#evaluating-model-accuracy-results",
    "title": "5  Weaving Together Models",
    "section": "5.2 Evaluating Model accuracy (results)",
    "text": "5.2 Evaluating Model accuracy (results)\nWhen evaluating up to 500 bases (first 500) for the first 30 genes, and all bases where the human base doesn’t match the most frequently observed ancestral base across 100 species we can generate model accuracy stats. Here we use the mean # bases correctly called by the model (where called means the model assigns the true human references base the highest probability when predicting it.). As expected the GPN based Bert model we trained, which has access to the evolutionary history of the base, blows vanilla DNA Bert out of the water. This isn’t unexpected though, very large portions of the coding sequences are conserved across species, and training on human species and 12 aligned species, where those other species are available at inference time just makes prediction VERY easy. However as per our first hypothesis the GPN model doesn’t do well if we mask the bases evolutionary history. In those cases, it’s bested by DNA Bert, which is trained to predict sequences without explicit evolutionary history and so does a reasonable job at it.\n# 1. Overall Accuracy Bar Plot\nplt.figure(figsize=(10, 6))\naccuracies = {\n    'GPN (normal)': df['gpn_normal_correct'].mean(),\n    'DNABERT': df['dnabert_normal_correct'].mean(),\n    'GPN (no species)': df['gpn_missing_correct'].mean()\n}\nplt.bar(accuracies.keys(), accuracies.values(), color=['blue', 'green', 'red'])\nplt.title('Model Accuracy Comparison')\nplt.ylabel('Accuracy')\nplt.ylim(0, 1)\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\nFigure 1: Model prediction accuracy compared across a common set of coding sequences of 30 genes (up to ±15k bases). GPN (no species) are predictions by the GPN model but with other species base masked (changed to “-” , the missing/sequences gap token) during prediction.\n\n\nI had expected that if considering bases where the human references base differs from the majority of the other species bases, and the base is highly conserved (&gt; 75%) then GPN models might be thrown off, and perhaps DNA Bert would not? It appears though (See Figure 2) that all models drop in accuracy, but DNA Bert drops more than the GPN model! So our second hypothesis wasn’t confirmed. Now this is a textbook-like document, these models are under-trained, we only evaluated the first 500 bases in 30 genes, within them there are only a few hundred bases where the human and dominant ancestral base differ, so please don’t generalize any of these conclusions!\n\n\n\nFigure 2: The model prediction accuracy for bases where the human base differs from the base most frequently observed in other species.\n\n\nFinally, we can plot model accuracy as a function of “conservation score” which is an ad-hoc measure of the % of valid bases (so A, T, C, G) across species that is the dominant most frequent) base. You could argue a more comprehensive conservation score might account for the number of species where the part of the sequences could not be aligned (e.g. the percentage of “-” bases), and that might be a great exercise for you!\n# 4. Conservation Score vs Accuracy\nplt.figure(figsize=(12, 6))\nconservation_bins = np.linspace(0, 1, 11)\ndf['conservation_bin'] = pd.cut(df['conservation_score'], bins=conservation_bins)\n\nfor model, color in zip(['gpn_normal', 'dnabert_normal', 'gpn_missing'], ['blue', 'green', 'red']):\n    accuracy_by_conservation = df.groupby('conservation_bin')[f'{model}_correct'].mean()\n    plt.plot(conservation_bins[:-1] + 0.05, accuracy_by_conservation, \n            marker='o', label=model.replace('_', ' ').title(), color=color)\n\nplt.title('Model Accuracy vs Conservation Score')\nplt.xlabel('Conservation Score')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Weaving Together Models</span>"
    ]
  },
  {
    "objectID": "Chapter5_DNA.html#discussion-of-results",
    "href": "Chapter5_DNA.html#discussion-of-results",
    "title": "5  Weaving Together Models",
    "section": "5.3 Discussion of Results",
    "text": "5.3 Discussion of Results\nOur comparative analysis of the GPN-BERT and DNABERT models reveals several key insights:\n\nBase Prediction Performance:\n\nGPN-BERT with evolutionary context significantly outperforms DNABERT in standard conditions\nWhen evolutionary information is masked, GPN-BERT’s performance drops below DNABERT\nBoth models show reduced accuracy when predicting bases that differ from the ancestral consensus\n\nConservation Score Impact:\n\nHigher conservation scores correlate with better prediction accuracy across all models\nThe relationship between conservation and accuracy appears to be non-linear\nGPN-BERT maintains a performance advantage even at lower conservation levels\n\nModel Architecture Trade-offs:\n\nGPN-BERT’s superior performance comes at the cost of requiring cross-species alignment data\nDNABERT shows more robust performance when evolutionary context is unavailable\nThe results suggest potential benefits in combining both approaches\n\n\nThese findings align with our project’s focus on comparing these two DNA language models, particularly in their ability to handle positions where human sequences differ from ancestral sequences. The analysis of prediction ranks and base-by-base accuracy provides valuable insights into each model’s strengths and limitations.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Weaving Together Models</span>"
    ]
  },
  {
    "objectID": "Chapter5_DNA.html#next-steps",
    "href": "Chapter5_DNA.html#next-steps",
    "title": "5  Weaving Together Models",
    "section": "5.4 Next Steps",
    "text": "5.4 Next Steps\nIn the upcoming chapters, we will: 1. Explore hybrid architectures that combine the strengths of both models 2. Evaluate performance on specific mutation types 3. Investigate the relationship between conservation patterns and prediction accuracy 4. Consider practical applications in genomic research and clinical settings\nThe code and detailed analysis for all experiments are available in the project repository, allowing for reproduction and extension of these results.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Weaving Together Models</span>"
    ]
  },
  {
    "objectID": "Chapter6_DNA.html",
    "href": "Chapter6_DNA.html",
    "title": "6  A Review of Current DNA Language Models",
    "section": "",
    "text": "6.1 Modeling Paradigms\nThis is not an academic review, those are obviously far more comprehensive, see Benegas et al. (2025) for a review I really liked.\nAs we leared in the first 5 chapters, gLMs (genomic Language Models) borrow from natural language processing by treating DNA sequences as “text” composed of four characters (A, C, G, T). Early models used k-mer tokenization (e.g., DNABERT), but newer approaches experiment with both nucleotide‐level and subword tokenizations (such as 3-mer or 6-mer) to better capture biological semantics.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Review of Current DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter6_DNA.html#architectural-innovations",
    "href": "Chapter6_DNA.html#architectural-innovations",
    "title": "6  A Review of Current DNA Language Models",
    "section": "6.2 Architectural Innovations",
    "text": "6.2 Architectural Innovations\nThere are two specific architectural innovations in DNA langage models worth discussing, the first is GPA-MSA, the core idea behind a model we discussed in Chapter 4. In this model the trainable embedding layer is replaced with a biologically informed deterministic embeding, that reflects the evolutionary history of the genome at a given base.\nA second innovation was necessitated by the need to model logn range dependence in DNA (changes to DNA can have effect over thousands of bases “downstream”). While transformer-based models initially dominated the field, their quadratic scaling with sequence length has prompted the development of more efficient architectures. Models such as HyenaDNA extend context lengths up to 1 million tokens at single-nucleotide resolution, and hybrid architectures like HybriDNA combine transformers with selective state-space models (Mamba2) to process sequences up to 131 kilobases. Omni-DNA and GENERator further illustrate the trend toward unified, cross-modal genomic foundation models capable of multitask learning.\nBelow is a summary table of several prominent DNA language models eith innovative architectures, along with links to their corresponding paper and GitHub (or related resource) repositories:\n\n\n\nModel\nDescription\nPaper Link\nGitHub / Resource Link\n\n\n\n\nDNABERT\nA transformer-based model that learns bidirectional representations from DNA sequences using k-mer tokenization.\nDNABERT Paper\nGitHub\n\n\nNucleotide Transformer (NT‑v2)\nA large transformer pretrained on massive human genomic data to learn robust DNA representations for various downstream tasks.\nNucleotide Transformer v2\nGitHub/HuggingFace\n\n\nGPN\nThe Genomic Pre-trained Network that leverages unsupervised DNA language modeling to predict genome-wide variant effects.\nGPN Paper (PNAS 2023)\nGitHub\n\n\nGNA-MSA\nThe Genomic Pre-trained Network that leverage multiple sequenece alignment across species to develop specialized evolution aware token embedding.\nGPN-MSA preprint\nGitHub\n\n\nHyenaDNA\nA long-range genomic language model operating at single-nucleotide resolution using Hyena’s implicit convolutional approach to overcome transformer scaling issues.\nHyenaDNA (arXiv)\nGitHub\n\n\nHybriDNA\nA hybrid model combining Transformer and Mamba2 (state-space) architectures for efficient long-range DNA modeling.\nHybriDNA (arXiv)\n\n\n\nOmni‑DNA\nA unified genomic foundation model that supports cross‑modal and multi‑task learning across a wide range of genomic applications.\nOmni‑DNA (arXiv)\nHugging Face Collection\n\n\nGENERator\nA long-context generative genomic foundation model designed for sequence generation and optimization tasks with a context length of up to 98k bp.\nGENERator (arXiv)\nGitHub\n\n\n\nThis table highlights each model’s core featurs and provides direct access to the publication and code repository (or resource page) where available.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Review of Current DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter6_DNA.html#applications",
    "href": "Chapter6_DNA.html#applications",
    "title": "6  A Review of Current DNA Language Models",
    "section": "6.3 Applications",
    "text": "6.3 Applications\nThese models have demonstrated state-of-the-art performance across multiple downstream tasks including: - Variant Effect Prediction: Unsupervised approaches can predict deleterious mutations by modeling the “grammar” of the genome. - Regulatory Element Identification: By learning long-range interactions, gLMs help detect promoters, enhancers, and other regulatory motifs. - Sequence Generation and Protein Tasks: Some models generate synthetic regulatory sequences or transform coding DNA into meaningful protein representations, bridging genomics and proteomics.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Review of Current DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter6_DNA.html#challenges-and-future-directions",
    "href": "Chapter6_DNA.html#challenges-and-future-directions",
    "title": "6  A Review of Current DNA Language Models",
    "section": "6.4 Challenges and Future Directions",
    "text": "6.4 Challenges and Future Directions\nDespite impressive progress, challenges remain in tokenization choices, computational efficiency, and interpretability of learned representations. Future work is likely to focus on integrating multimodal data (e.g., epigenomic signals), enhancing model scalability, and further bridging the gap between genomic sequences and cellular function—paving the way toward a “virtual cell.”\n\n\n\n\nBenegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun S. Song. 2025. “Genomic Language Models: Opportunities and Challenges.” Trends in Genetics, January. https://doi.org/10.1016/j.tig.2024.11.013.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Review of Current DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Scaling_training.html",
    "href": "Scaling_training.html",
    "title": "Scale up Training",
    "section": "",
    "text": "Don’t Try to Win the Compute Race\nThis chapter isn’t a part of the “DNA” section of the book, because the lessons are really quite general, but it comes after because we needed a little bit of experience with language model training before even considering training a serious model. This is also a somewhat awkward chapter for me to write, especially for the part of the readership that has a background in ML. See, I am a psychologist by training (though I have worked in genetic epidemiology for years and years), and while a lot of my academic work is fairly computational, I am no expert in language model scaling by any means! Remember, the preamble to the book explains this book is an account of me learning about biological language models and taking others along for the ride, not an authoritative text!\nAmong the DNA models I could find on Hugging Face is a 7B parameter model like https://huggingface.co/genbio-ai/AIDO.DNA-7B. AIDO is trained on “256 H100 GPUs” in “8 days”. The training data consisted of 10.6 billion bases. That’s not even a particularly large model in the grand scheme of things, but if you consider a cost of ±$2 per hour per H100, you are going to spend $100k. Obviously, there are academic compute resources you can get access to by appointment or based on fair use at your institute, university, or through collaborative national infrastructure, but even those are finite.\nYou have to consider feasibility. Today (March 2025), the Dutch national computer cluster for research (Snellius at SURF Sara) has 88 nodes with 4 H100 GPUs and 72 nodes with 4 A100 GPUs. TACC, the University of Texas at Austin compute provider, has ±80 A100 nodes (each with 3 GPUs). Those are two examples of reasonably well-funded HPC providers in academia. In my experience, you could get time reserved for your research at your local academic HPC provider at steep discounts, and these systems are likely large enough to train that 7B model I linked to. However, note how on either TACC or Snellius, 256 GPUs for 8 days would block the entire system for over a week. Perhaps you could apply for access to larger national research clusters, like Isambard-AI in the UK (being built in Bristol right now, a motivation for me to write this) which has 5,000 H200 GPUs. However, in general, it is likely you are going to be relatively compute constrained. Don’t be discouraged though—most breakthroughs are not going to be compute-based, and there are immense efficiency gains to be made that will level the playing field.",
    "crumbs": [
      "Scale up Training"
    ]
  },
  {
    "objectID": "Scaling_training.html#smart-architectures",
    "href": "Scaling_training.html#smart-architectures",
    "title": "Scale up Training",
    "section": "Smart Architectures",
    "text": "Smart Architectures\nIn Chapter 4, we studied smarter, DNA-specific model architectures. The GPN model inspired by Benegas et al. (2023) we introduced can blow away a standard BERT in an hour of training on my 2022 MacBook Air (the BERT we trained and compared to our GPN-BERT trained for ±8 hours on a strong GPU). The massive efficiency gain may mean you can beat the 7B BERT-like model we took as an example of compute costs with a fraction of the compute! As briefly remarked on in Chapter 6 reseachers have designed alternatives for the transformer module in order to expand its context window up to 1 million bases, with far less compute reuirement than the transformer (Nguyen et al. 2023). If you are to design and run you own model, it may pay off to consider implementing some of these optimisations.",
    "crumbs": [
      "Scale up Training"
    ]
  },
  {
    "objectID": "Scaling_training.html#focus-on-a-specific-question",
    "href": "Scaling_training.html#focus-on-a-specific-question",
    "title": "Scale up Training",
    "section": "Focus on a specific question",
    "text": "Focus on a specific question\nIst is tempting to train a model on all DNA known to man, but honestly, there is actully more of that then peopel an even beginn to train on. The models disucssed so far ofeten train on the reference seqeucnes, a sort of modal genome, but indiviual people’s genomes are different from that references. YOu culd consider thousands of species, or (tens/hundreds of) thousands of individual genomes. That would require a lot of bioinformatcs. You’d have to phase the genomes to untangle the maternal and paternal strnd, youd hae to decided whether you want to get rid of the reference entirely and build a specific reference/genome for eah individual, you migh require some reference, or a graph genome? It’s also worth considering whether your task really requires the whole genome. Are you performing gene centric tasks (mutation consequence prediction, gene expresssion prediction, alternative splice modeling)? If your speciic tasks dont requre the whole genome, why not consider traning on coding sequences only or genes and a few thousand bases around them?",
    "crumbs": [
      "Scale up Training"
    ]
  },
  {
    "objectID": "Scaling_training.html#optimize-optimize-optimize",
    "href": "Scaling_training.html#optimize-optimize-optimize",
    "title": "Scale up Training",
    "section": "Optimize, Optimize, Optimize",
    "text": "Optimize, Optimize, Optimize",
    "crumbs": [
      "Scale up Training"
    ]
  },
  {
    "objectID": "Scaling_training.html#parallel-training",
    "href": "Scaling_training.html#parallel-training",
    "title": "Scale up Training",
    "section": "Parallel Training",
    "text": "Parallel Training\n\n\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2023. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” http://dx.doi.org/10.1101/2023.10.10.561776.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” https://doi.org/10.48550/ARXIV.2306.15794.",
    "crumbs": [
      "Scale up Training"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Benegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S.\nSong. 2023. “GPN-MSA: An Alignment-Based DNA Language Model for\nGenome-Wide Variant Effect Prediction.” http://dx.doi.org/10.1101/2023.10.10.561776.\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. “DNA\nLanguage Models Are Powerful Predictors of Genome-Wide Variant\nEffects.” Proceedings of the National Academy of\nSciences 120 (44). https://doi.org/10.1073/pnas.2311219120.\n\n\nBenegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun\nS. Song. 2025. “Genomic Language Models: Opportunities and\nChallenges.” Trends in Genetics, January. https://doi.org/10.1016/j.tig.2024.11.013.\n\n\n“Initial Sequencing and Comparative Analysis of the Mouse\nGenome.” 2002. Nature 420 (6915): 520–62. https://doi.org/10.1038/nature01262.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael\nFigurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021.\n“Highly Accurate Protein Structure Prediction with\nAlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nLupo, Umberto, Damiano Sgarbossa, and Anne-Florence Bitbol. 2022.\n“Protein Language Models Trained on Multiple Sequence Alignments\nLearn Phylogenetic Relationships.” Nature Communications\n13 (1). https://doi.org/10.1038/s41467-022-34032-y.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum\nBirch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA:\nLong-Range Genomic Sequence Modeling at Single Nucleotide\nResolution.” https://doi.org/10.48550/ARXIV.2306.15794.\n\n\nRentzsch, Philipp, Max Schubach, Jay Shendure, and Martin Kircher. 2021.\n“CADD-Spliceimproving Genome-Wide Variant Effect\nPrediction Using Deep Learning-Derived Splice Scores.” Genome\nMedicine 13 (1). https://doi.org/10.1186/s13073-021-00835-9.\n\n\nSullivan, Patrick F., Jennifer R. S. Meadows, Steven Gazal, BaDoi N.\nPhan, Xue Li, Diane P. Genereux, Michael X. Dong, et al. 2023.\n“Leveraging Base-Pair Mammalian Constraint to Understand Genetic\nVariation and Human Disease.” Science 380 (6643). https://doi.org/10.1126/science.abn2937.",
    "crumbs": [
      "References"
    ]
  }
]