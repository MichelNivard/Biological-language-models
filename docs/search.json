[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Biological Language Models & Neural Networks",
    "section": "",
    "text": "Preface\nThese are my study notes on training DNA/RNA/Protein and other biomedical language models. The text/book is intended for people who want to casually explore the field before they on-ramp to actually training large DNA/Biological language models, or for their PIs, an anxious aging millennial or bitter but wise gen-X’ers who want to be able to understand the next generation of computational genomics that is about to wash over us all.\nAt all times I’ll try to add minimal biological context (though I am no biologist!) for people who have an ML background but no college bio experience and ill try to add context on ML concepts for those with a bio background but limited experience with language models.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-times-we-live-in.",
    "href": "index.html#the-times-we-live-in.",
    "title": "Biological Language Models & Neural Networks",
    "section": "The times we live in.",
    "text": "The times we live in.\nMore than natural language models biologial/sequence language models rely heavily on NIH funded databases, datasets, resources and scientits. The data we train on was bought and payd for by taxpayers all over the globe. The Human Genome Project was to a great extend funded, directed and conceived under the auspice of the US federal government, under both Democratic and Republican presidents. Had they not, pharmasuitical companies might have done it, and while thoes can be highly innovative there would have been no space for startups, no space for Google Deep-mind to come in and itterate, revolutionize or grow biological modeling. There is nu uproar over trainign data in biology because under the firm guidance of US federal policy all the data sequencers generate is generally in the public domain, ar accessible for those willing and able to meet ethical standards. All scientits readign this know this, should you find yourself as someone from silicon valley, from a well funded startup even, take a beat and think through whether you’d stand a snowballs chance in hell to compete if the next wave of sequence data isnt public but generated inside google/microsoft/pharma. Then adjust your politics accordingly.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Biological Language Models & Neural Networks",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese notes are written by me, Michel Nivard, a professor of Epidemiology at the University of Bristol, and as this book is not a core outputs for my job, I rely on LLMs to help me with drafting, spelling and formatting.\nThese study notes are influences by discussion with Robbee Wedow and Seyedeh Zahra Paylakhi with whom I work on related projects.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Preamble1.html",
    "href": "Preamble1.html",
    "title": "What is this book about?”",
    "section": "",
    "text": "A brief glossary of ML model types\nIn this book we learn about the current state of the art in ML for biology, with an innitial focus on language models. When doing so its good to resit the temptation to be awed by the models. Some are great, and it can feel magical to see an unsuperviosed model pick up important signals and be very predictive by only munging sequence data! However, the current state of the art in biology, genetics especially is remarkable, we know a lot about the gneome, about how DNA is trascribed nto RNA and then proteins, which proteins are conserved across evolutions (i.e. essential for all life). So troughout we have to keep in mind that in some domains while it may feel, and actually be, remarkable a language model picks up fundepmental biology just from munging data, it might not be tstae of the art or even close to it.\nSupervised machine learning is a type of machine learning where a model learns to make predictions based on examples that come with known answers, or “labels.” In biology, this could mean training a model to predict whether a DNA sequence comes from a healthy or diseased tissue, or identifying which species a DNA sample belongs to. The model sees many examples where the input (the DNA sequence) is paired with the correct output (the label, like “healthy” or “diseased”), and learns to find patterns that link the two. Supervised learning is very powerful when we have lots of high-quality labeled data, but in biology, obtaining these labels can be expensive, time-consuming, and sometimes even impossible if we don’t know the “right answer” in advance.\nUnsupervised machine learning, in contrast, is used when we don’t have labels — the model only sees the raw data and has to find patterns on its own. This is especially useful in biology when exploring large datasets where the structure isn’t fully understood, such as grouping similar cells in single-cell RNA sequencing or discovering new subtypes of proteins. In the case of biological language models, the “language” is made up of sequences like DNA, RNA, or proteins. Unsupervised models, such as transformers trained on genome sequences, learn the “grammar” and “vocabulary” of these biological molecules just by seeing lots of sequences, without being told what they mean. This allows them to uncover hidden rules of biology, like which sequences are likely to code for stable proteins or which mutations might disrupt function.\nBiological language models have become particularly important because DNA, RNA, and proteins all follow sequential, language-like patterns — just as words form sentences, nucleotides form genes, and amino acids form proteins. By training on vast amounts of biological data in an unsupervised way, these models can learn useful representations of biological sequences, even without human-provided labels. Researchers can then use these pretrained models for many downstream tasks, such as predicting gene function, identifying regulatory regions, or studying how genetic variation might affect disease — combining the power of unsupervised learning to understand biology’s “language” with supervised learning for more targeted, disease-specific predictions.\nIn some cases, the boundary between supervised and unsupervised learning is blurry — for example, in protein language models trained to predict 3D structure from amino acid sequences. These models are not given simple “labels” like “healthy” or “diseased,” but they are provided with 3D structural information that acts as an open-ended example rather than a strict classification label. The model isn’t being asked to sort sequences into a few categories, but rather to learn a very rich and flexible relationship between sequence and structure. This kind of learning — where the system uses biological context to guide its training without explicit classification tasks — occupies a middle ground between supervised and unsupervised methods, illustrating how biological complexity often resists fitting into neat ML categories. A key differneces between learnign labels, and learnign open ended structures is that learning labels is data reduction (from complrx 1d sequence to two, or a few, labels) while structure prediction is data expansion (from 1D protein sequence to 3D spatial molecular map).\nIn sequence analysis, there are also biologically-driven models that sit outside traditional machine learning entirely, or only use minimal regression or statistical modeling. For example, methods to predict whether a missense mutation (a single amino acid change) is deleterious often rely on biological theory, such as identifying mutation-depleted regions — parts of the genome or protein where harmful mutations rarely appear in healthy populations. These models leverage evolutionary conservation, functional annotations, and biochemical properties to prioritize mutations for further study, sometimes incorporating simple regression to combine different biological signals into a final score. These biologically-informed approaches are critical in genomic medicine and show how biology itself can provide a strong prior for prediction, even without extensive machine learning.",
    "crumbs": [
      "What is this book about?\""
    ]
  },
  {
    "objectID": "Preamble2.html",
    "href": "Preamble2.html",
    "title": "How to read this book",
    "section": "",
    "text": "Practicalities\nThe book is accompanied by scripts in both the R and Python programming languages. I had to make some choices, some of the biological data repositories have great integrated perl and R packages, I wouldn’t want to force people into perl (especially not myself!), I am more comfortable wrangling the initial data in R then in Python so here we are.\nIf you want to code along, rest assured you can run most of this on a macbook. Maybe you’ll need to run a training run overnight a few times. If you want a bit more performance, or not have your MAckbook turn into a space heater for 24 hours you can use google colab for access to A100 GPUs. Training the DNABert model we outline in Chapter 2 on 500k coding sequences from 13 species took ±6 hours on an A100 on Colab, which means that cost me ±4$ in colab credit.\nThe Gihub repo that hosts the book will be populated with all the scripts I discuss and use. The data used to train the models, and some of the models themselves, will be hosted on Huggingface (a repository of ML training data). I will try to make jupyter notebooks available though given my R background I usually run Puython in a REPL because that is what R people do…\nIf you come at this from an R background, and Python isn’t your native language, I can highly recommend using positron as an IDE when following along with this book, its a vscode derivative developed by Posit, the company that developed RStudio. Positron has tooling integrated for data science in both python and R and I can switch between python and R sessions instantly!",
    "crumbs": [
      "How to read this book"
    ]
  },
  {
    "objectID": "Preamble2.html#structure",
    "href": "Preamble2.html#structure",
    "title": "How to read this book",
    "section": "Structure",
    "text": "Structure\nThe book is divided up into sections that deal with a specific biological modality or data type. the last chapter in each section is a review of current models. Its more common to begin a chapter reviewing whats available out there, but given the novelty of these models it makes sense to learn how they work before reading a review of what’s out there. There is risk of the reader attributing insights to me, simply because I describe it to you first. I’ll always cite my sources as if this is a peer reviewed article, and you should assume most models we build together are directly, or indirectly, influenced by the literature. I also ask you do not cite this book other then for novel content, or to refer to it as teaching material, please for models, architectures, insights cite the underlying empirical literature.\n\nDNA language models\nChapter 1 covers downloading, and pocessing (DNA) sequences data from ensembl and uploading it to Huggingface. Chapter 2 covers training a first small DNA sequence language mode, the model is a bog standard language model, meant for natural languages simply applied to DNA. in Chapter 3 we explore how you’d evaluate whether a DNA model is any good, is our model learning anything at all? Then in Chapter 4\n\n\nProtein language models\n\n\nMulti-modal models (DNA meets proteins)",
    "crumbs": [
      "How to read this book"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html",
    "href": "Chapter1_DNA.html",
    "title": "1  Preparing DNA data for training",
    "section": "",
    "text": "1.1 Garbage in garbage out\nHigh-quality data is essential for training effective machine learning models. In natural language processing (NLP), researchers invest significant effort in curating clean datasets from raw internet text. For example a dataset like ‘fineweb-edu’ contains English text that is of very high quality. Models trained on fineweb-edu (and similar high quality datasets) will improve MUCH faster then the equivalent model trained on other less carefully processed and evaluated datasets.\nThose with experience with genetics will know most of what I am about to explain when it comes to the sources of genetic data and the file formats these come in. Those with an ML background will be very familiar with Huggingface and the amazing data/model/training integration their libraries offer. Assembing high quality genomics datasets for language modeling requires familiarity with both. When working with DNA, RNA, and protein sequences, the authoritative sources of data are specialized databases and infrastructures rather than data scraped from the internet. If you want to learn to train DNA/RNA/Protein-based language model, we must learn how, and where, to retrieve data and convert it into a structured format.\nIn this tutorial, we will begin by collecting coding DNA sequences (CDS) from the human genome using the Ensembl database via BioMart. The goal is to store these sequences in a format suitable for training a DNA-based language model. We will then upload the processed data to Huggingface, a platform for hosting datasets and models for machine learning tasks.\nThis chapter will introduce key biological and computational concepts, ensuring that both biology newcomers and those unfamiliar with language modeling tools can follow along.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#garbage-in-garbage-out",
    "href": "Chapter1_DNA.html#garbage-in-garbage-out",
    "title": "1  Preparing DNA data for training",
    "section": "",
    "text": "Relative training efficiency using a high quality dataset like fineweb-edu and other massive datasets of English language texts. Image obtained from: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#understanding-ensembl-and-biomart",
    "href": "Chapter1_DNA.html#understanding-ensembl-and-biomart",
    "title": "1  Preparing DNA data for training",
    "section": "1.2 Understanding Ensembl and BioMart",
    "text": "1.2 Understanding Ensembl and BioMart\nFortunately for us, there is decades of work cleaning up genomic data and we can just go and get it from US government funded websites, where it is deposited by the global scientific community. Ensembl is a genomic database that provides extensive annotations for various species, including humans. It offers access to gene sequences, transcript annotations, and protein-coding information. One of its most powerful tools is BioMart, a flexible data retrieval system that allows users to download specific genomic datasets easily.\nIf we want t work with the data in a language model its efficient to store it in a format that is tailored for machine learning libraries. Unlike text-based NLP datasets, genomic data is structured differently and must be properly formatted before use in a model.\n\n1.2.1 What Are FASTA Files?\nA FASTA file is a simple text-based format used for storing biological sequences. Each entry in a FASTA file consists of: 1. A header line (starting with &gt;), which contains metadata such as gene IDs and chromosome locations. 2. A sequence line, which contains the nucleotide or protein sequence.\nThere is a very comprehensive Wikipedia entry on the FASTA format.\n“Sequences may be protein sequences or nucleic acid sequences, and they can contain gaps or alignment characters (see sequence alignment). Sequences are expected to be represented in the standard IUB/IUPAC amino acid and nucleic acid codes, with these exceptions: lower-case letters are accepted and are mapped into upper-case; a single hyphen or dash can be used to represent a gap character; and in amino acid sequences, U and * are acceptable letters (see below). Numerical digits are not allowed but are used in some databases to indicate the position in the sequence.” ((source: https://en.wikipedia.org/wiki/FASTA_format))\n\n\n\nNucleic Acid Code\nMeaning\nMnemonic\n\n\n\n\nA\nA\nAdenine\n\n\nC\nC\nCytosine\n\n\nG\nG\nGuanine\n\n\nT\nT\nThymine\n\n\nU\nU\nUracil\n\n\n(i)\ni\ninosine (non-standard)\n\n\nR\nA or G (I)\npuRine\n\n\nY\nC, T or U\npYrimidines\n\n\nK\nG, T or U\nbases which are Ketones\n\n\nM\nA or C\nbases with aMino groups\n\n\nS\nC or G\nStrong interaction\n\n\nW\nA, T or U\nWeak interaction\n\n\nB\nnot A (i.e. C, G, T or U)\nB comes after A\n\n\nD\nnot C (i.e. A, G, T or U)\nD comes after C\n\n\nH\nnot G (i.e., A, C, T or U)\nH comes after G\n\n\nV\nneither T nor U (i.e. A, C or G)\nV comes after U\n\n\nN\nA C G T U\nNucleic acid\n\n\n-\ngap of indeterminate length\n\n\n\n\nThe amino acid codes supported (22 amino acids and 3 special codes) are:\n\n\n\nAmino Acid Code\nMeaning\n\n\n\n\nA\nAlanine\n\n\nB\nAspartic acid (D) or Asparagine (N)\n\n\nC\nCysteine\n\n\nD\nAspartic acid\n\n\nE\nGlutamic acid\n\n\nF\nPhenylalanine\n\n\nG\nGlycine\n\n\nH\nHistidine\n\n\nI\nIsoleucine\n\n\nJ\nLeucine (L) or Isoleucine (I)\n\n\nK\nLysine\n\n\nL\nLeucine\n\n\nM\nMethionine/Start codon\n\n\nN\nAsparagine\n\n\nO\nPyrrolysine (rare)\n\n\nP\nProline\n\n\nQ\nGlutamine\n\n\nR\nArginine\n\n\nS\nSerine\n\n\nT\nThreonine\n\n\nU\nSelenocysteine (rare)\n\n\nV\nValine\n\n\nW\nTryptophan\n\n\nY\nTyrosine\n\n\nZ\nGlutamic acid (E) or Glutamine (Q)\n\n\nX\nany\n\n\n*\ntranslation stop\n\n\n-\ngap of indeterminate length\n\n\n\nYou’ll notice the FASTA format has a well defined structure, and it could be leveraged to build a complete tokenizer, for now though our 4 character (+6 special characters) tokenizer will have to do.\n\n\n1.2.2 Why Focus on Coding DNA Sequences (CDS)?\nIn the example, we retrieve the human coding DNA sequences (CDS), which represent the DNA sequence of protein-coding regions of genes.\nWhile our ultimate goal is to model the entire human genome—and potentially multiple genomes across species or individuals—such tasks require significant computational resources. The tutorials in this book are designed to be accessible, running efficiently on a MacBook or Google Colab. Therefore, we focus on CDS, which are highly structured DNA sequences within genes, that directly transcribed into RNA which is in turn translate into proteins. the Table below contains the direct translation from 3 letter DNA sequence to amino-acid (which are the building blocks of proteins).\n\n\n\nThe Genetic code to translate codins (3 leter DNA sequences) to amino-acids that are in turn the building blocks of proteins (source: https://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/_images/P7_image1.png)\n\n\nIn contrast, much of the rest of the genome consists of regulatory regions, which are more complex and less structured. CDS sequences provide a strong foundation for a small DNA-based language model because they contain well-defined biological structure, making them a logical starting point before expanding to more complex genomic elements.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#why-upload-dna-data-to-hugging-face",
    "href": "Chapter1_DNA.html#why-upload-dna-data-to-hugging-face",
    "title": "1  Preparing DNA data for training",
    "section": "1.3 Why Upload DNA Data to Hugging Face?",
    "text": "1.3 Why Upload DNA Data to Hugging Face?\nHugging Face provides a robust ecosystem for hosting and sharing datasets, particularly for machine learning applications. Some key advantages include: - Easy accessibility: Researchers and models can easily retrieve datasets. - Standardized format: Datasets are structured for seamless integration with deep learning frameworks. - Direct integration with Hugging Face tools: The data on the Hugging Face Hub integrates seamlessly with their Transformers and Trainer Python libraries, making it easy to load datasets and train models. - Version control and updates: Data can be refined and expanded over time.\nBy storing our dataset on Hugging Face, we enable efficient training and collaboration for DNA language modeling.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#the-script-downloading-and-formatting-human-cds-data",
    "href": "Chapter1_DNA.html#the-script-downloading-and-formatting-human-cds-data",
    "title": "1  Preparing DNA data for training",
    "section": "1.4 The Script: Downloading and Formatting Human CDS Data",
    "text": "1.4 The Script: Downloading and Formatting Human CDS Data\nBelow is the R script that downloads human CDS from Ensembl using BioMart, extracts metadata, and saves the processed data into a CSV file. the package we use, biomartr isn’t the official R package but its a great option! it has very extensive documentation, so if you want to download other sequences in the future make sure to start here: https://docs.ropensci.org/biomartr/\n# Install necessary packages\ninstall.packages(\"biomartr\", dependencies = TRUE)\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n  install.packages(\"BiocManager\")\n}\n\n# Load required libraries\nlibrary(Biostrings)\nlibrary(biomartr)\n\n# Download the human CDS dataset from Ensembl\nHS.cds.ensembl &lt;- getCDS(db = \"ensembl\", organism = \"Homo sapiens\", path = file.path(\"_ncbi_downloads\",\"CDS\"))\n\n# Read CDS data as a Biostrings object\nHuman_CDS &lt;- read_cds(file = HS.cds.ensembl, obj.type = \"Biostrings\")\n\n# Extract headers and sequences\nheaders &lt;- names(Human_CDS)\nsequences &lt;- as.character(Human_CDS)\n\n# Function to extract metadata from headers\nextract_metadata &lt;- function(header) {\n  transcript_id &lt;- sub(\"^&gt;([^ ]+).*\", \"\\\\1\", header)\n  chromosome &lt;- sub(\".*chromosome:([^ ]+).*\", \"\\\\1\", header)\n  start &lt;- sub(\".*chromosome:[^:]+:([^:]+).*\", \"\\\\1\", header)\n  end &lt;- sub(\".*chromosome:[^:]+:[^:]+:([^:]+).*\", \"\\\\1\", header)\n  strand &lt;- sub(\".*chromosome:[^:]+:[^:]+:[^:]+:([^ ]+).*\", \"\\\\1\", header)\n  gene_id &lt;- sub(\".*gene:([^ ]+).*\", \"\\\\1\", header)\n  gene_biotype &lt;- sub(\".*gene_biotype:([^ ]+).*\", \"\\\\1\", header)\n  transcript_biotype &lt;- sub(\".*transcript_biotype:([^ ]+).*\", \"\\\\1\", header)\n  gene_symbol &lt;- sub(\".*gene_symbol:([^ ]+).*\", \"\\\\1\", header)\n  description &lt;- sub(\".*description:(.*)\", \"\\\\1\", header)\n\n  list(\n    transcript_id = transcript_id,\n    chromosome = chromosome,\n    start = start,\n    end = end,\n    strand = strand,\n    gene_id = gene_id,\n    gene_biotype = gene_biotype,\n    transcript_biotype = transcript_biotype,\n    gene_symbol = gene_symbol,\n    description = description\n  )\n}\n\n# Extract metadata from headers\nmetadata_list &lt;- lapply(headers, extract_metadata)\nmetadata_df &lt;- do.call(rbind, lapply(metadata_list, as.data.frame))\nmetadata_df$sequence &lt;- sequences\n\n# Save data to CSV\nwrite.csv(metadata_df, \"genome_sequences.csv\", row.names = FALSE, quote = TRUE)\n\n# Print sample data\nhead(metadata_df)\nYou can run the script yourself, but I have also gone ahead and uploaded it to huggingface: https://huggingface.co/datasets/MichelNivard/Human-genome-CDS-GRCh38",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#summary",
    "href": "Chapter1_DNA.html#summary",
    "title": "1  Preparing DNA data for training",
    "section": "1.5 Summary",
    "text": "1.5 Summary\nIn this chapter, we: - Introduced Ensembl and BioMart as tools for retrieving genomic data. - Explained FASTA files and human CDS, which form the core of our dataset. - Discussed the advantages of uploading datasets to Hugging Face, emphasizing its integration with Transformers and Trainer libraries. - Provided an R script to download, process, and store human CDS in a structured format.\nIn the next chapter, we will explore preprocessing techniques like tokenization and strategies for encoding DNA sequences into a format suitable for training a deep learning language model, and we use Huggingface Transformers and Trainer library to train our first little DNA language model!",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html",
    "href": "Chapter2_DNA.html",
    "title": "2  Training our first DNA Language Model",
    "section": "",
    "text": "2.1 Introduction\nNow that we have collected and formatted a DNA dataset in Chapter 1, we can proceed to train a language model using the (Modern)BERT model architecture. In this chapter, we will walk through the process of tokenizing DNA sequences, configuring a BERT model, and training it using the Masked Language Modeling (MLM) objective.\nWe will cover the utility and rational behing DNA language models, and the key concepts behind tokenization, the BERT model, and the idea of masked language modeling (MLM) before diving into the Python script that trains the actual model.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html#why-would-we-train-dna-language-models",
    "href": "Chapter2_DNA.html#why-would-we-train-dna-language-models",
    "title": "2  Training our first DNA Language Model",
    "section": "2.2 Why would we train DNA language models?",
    "text": "2.2 Why would we train DNA language models?\nFor a full review of the utility of language models you should dig into the literature. I can recommend (Benegas et al. 2025) for example. Genomic language models (gLMs) apply AI techniques to DNA sequences, enabling breakthroughs in variant effect prediction, sequence design, and genomic analysis.\nLike larger language models like chatGPT DNA language models have “emergent properties:. If you train a gnomic Language models (gLM) on the reference genome sequence of humans, and various other species then the model that emerges is able to detect damaging mutations, without ever being trained on mutations (as mutations are defined as deviations from the reference)(Benegas, Batra, and Song 2023). To assess functional constraints, a widely used metric is the log-likelihood ratio (LLR) between two alleles. This measures the probability of a nucleotide variant appearing in a given context, with lower probabilities indicating potential deleterious effects. This application will be one of the examples I use throughout, simply because my experience in genetics align with it.\nAnother key application is transfer learning, where pretrained gLMs improve predictions in tasks like gene expression and chromatin accessibility. However, training effective models is difficult due to the vast, complex, and often non-functional nature of genomes. Unlike protein models, gLMs struggle with limited genomic diversity in training data and require more sophisticated benchmarks for evaluation.\nFuture advancements will focus on improving long-range genomic interactions, integrating multimodal biological data, and refining sequence design for practical applications. Despite challenges, gLMs hold great promise for revolutionizing genome research, advancing genetic disease understanding, and enabling synthetic biology innovations.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html#understanding-tokenization",
    "href": "Chapter2_DNA.html#understanding-tokenization",
    "title": "2  Training our first DNA Language Model",
    "section": "2.3 Understanding Tokenization",
    "text": "2.3 Understanding Tokenization\n\n2.3.1 What is a Tokenizer?\nA tokenizer is a fundamental component of any language model. Language models are essentially large multinomial models that predict the next token in a sequence based on previous tokens, or predict a masked token using the surrounding tokens. Since machine learning models operate on numerical representations, tokenized text must be converted into integers, which serve as indices for lookup in a vocabulary.\nThese integers, however, have no inherent numeric value—they simply act as categorical labels that the model learns to associate with semantic meaning. For example, if we tokenize the sentence:\n\n“The quick brown fox jumps over the lazy dog”\n\nat the word level, we might obtain a numerical sequence like:\n\n[4, 123, 678, 89, 245, 983, 56, 4564]\n\nwhere each number corresponds to a word based on a pre-defined tokenization dictionary, such as:\n{\"the\": 4, \"quick\": 123, \"brown\": 678, \"fox\": 89, \"jumps\": 245, \"over\": 983, \"lazy\": 56, \"dog\": 4564}\nSimilarly, for DNA sequences, each nucleotide (A, T, C, G) is assigned a unique integer ID.\n\n\n2.3.2 Our DNA Tokenizer\nOur tokenizer uses a character-level approach, where each nucleotide is assigned a unique integer ID. Special tokens are also included for various purposes:\n\n[UNK] (unknown token)\n[PAD] (padding token for equal-length sequences)\n[CLS] (classification token, useful for downstream tasks)\n[SEP] (separator token, used in tasks like sequence-pair classification)\n[MASK] (used for masked language modeling training)\n\nPython Code:\nimport torch\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.pre_tokenizers import Split\nfrom transformers import PreTrainedTokenizerFast\n\n# --------------------------------\n# 1. DNA Tokenizer with Full FASTA Nucleic Acid Code\n# --------------------------------\n\n# Define vocabulary to include all FASTA nucleotides and symbols\ndna_vocab = {\n    \"A\": 0, \"T\": 1, \"C\": 2, \"G\": 3, \"N\": 4, \"U\": 5, \"i\": 6,  # Standard bases + Inosine\n    \"R\": 7, \"Y\": 8, \"K\": 9, \"M\": 10, \"S\": 11, \"W\": 12,  # Ambiguous bases\n    \"B\": 13, \"D\": 14, \"H\": 15, \"V\": 16,  # More ambiguity codes\n    \"-\": 17,  # Gap character\n    \"[UNK]\": 18, \"[PAD]\": 19, \"[CLS]\": 20, \"[SEP]\": 21, \"[MASK]\": 22\n}\n\n# Create tokenizer\ntokenizer = Tokenizer(WordLevel(vocab=dna_vocab, unk_token=\"[UNK]\"))\ntokenizer.pre_tokenizer = Split(\"\", \"isolated\")  # Character-level splitting\n\n# Convert to Hugging Face-compatible tokenizer\nhf_tokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\"\n)\n\n\n2.3.3 Other Tokenization Strategies for DNA, RNA, and Proteins\nWhile character-level tokenization is effective, other tokenization approaches can offer different tradeoffs:\n\n2.3.3.1 Byte Pair Encoding (BPE)\nBPE is widely used in NLP and allows frequently occurring patterns to be merged into subword units. While BPE could be useful for repetitive genome sequences, it may not capture biologically meaningful units.\n\n\n2.3.3.2 K-mer Tokenization\nK-mer tokenization groups nucleotides into fixed-length substrings (e.g., 3-mers like “ATG”). This approach retains local sequence structure but can lead to a large vocabulary size.\n\n\n2.3.3.3 Tiktoken and Similar Models\nSome modern tokenization methods, such as Tiktoken, optimize speed and efficiency by precomputing merges. These are often optimized for large-scale NLP tasks but could be adapted for biological sequences.\nChoosing the best tokenizer depends on the specific modeling task. For example, RNA secondary structures or protein folding models might benefit from more complex tokenization strategies.\nSource: RPubs Tokenization Review",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html#loading-and-tokenizing-the-dna-dataset",
    "href": "Chapter2_DNA.html#loading-and-tokenizing-the-dna-dataset",
    "title": "2  Training our first DNA Language Model",
    "section": "2.4 Loading and Tokenizing the DNA Dataset",
    "text": "2.4 Loading and Tokenizing the DNA Dataset\n\n2.4.1 Understanding the Dataset\nWe will use a pre-existing dataset, Human-genome-CDS-GRCh38, which contains coding sequences from the human genome.\n\n\n2.4.2 Tokenizing the Dataset\nTo prepare the dataset for training, we must apply the tokenizer to each sequence while ensuring:\n\nSequences are truncated or padded to a fixed length (512 tokens)\nUnwanted columns are removed\n\nPython Code:\nfrom datasets import load_dataset\n\ndataset_name = \"MichelNivard/Human-genome-CDS-GRCh38\"\ndataset = load_dataset(dataset_name)\n\ncolumn_name = \"sequence\"\n\ndef tokenize_function(examples):\n    return hf_tokenizer(examples[column_name], truncation=True, padding=\"max_length\", max_length=512)\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[column_name])\n\n\n2.4.3 Saving and Preparing the Dataset for Training\nOnce tokenized, we save the dataset for efficient access during training.\nPython Code:\ntokenized_dataset.save_to_disk(\"tokenized_dna_dataset\")",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html#understanding-bert-and-masked-language-modeling-mlm",
    "href": "Chapter2_DNA.html#understanding-bert-and-masked-language-modeling-mlm",
    "title": "2  Training our first DNA Language Model",
    "section": "2.5 Understanding BERT and Masked Language Modeling (MLM)",
    "text": "2.5 Understanding BERT and Masked Language Modeling (MLM)\n\n2.5.1 What is BERT?\nBERT (Bidirectional Encoder Representations from Transformers) is a powerful transformer-based language model. Unlike traditional left-to-right models, BERT learns bidirectional context, allowing it to understand sequences more effectively.\nReturning to our earlier example sentence:\n\n“The quick brown fox jumps over the lazy dog”\n\nBERT does not process words one at a time but instead considers the entire sequence simultaneously. This bidirectional approach allows BERT to infer missing words based on context from both directions.\n\n\n2.5.2 What is Masked Language Modeling (MLM)?\nMLM is a self-supervised learning objective where the model learns by predicting missing tokens in a sequence. During training:\n\nSome tokens are randomly replaced with [MASK]\nThe model must predict the original token based on surrounding context\n\nFor example, if we mask the word “fox” in our sentence:\n\n“The quick brown [MASK] jumps over the lazy dog”\n\nBERT will analyze the remaining words and attempt to predict “fox.”\nThis technique enables BERT to learn useful representations without requiring labeled data.\n\n\n2.5.3 Understanding Transformer Layers, Attention Heads, and Hidden Size\nA transformer layer consists of self-attention and feed-forward layers that help the model learn relationships between tokens. The number of transformer layers determines how deep the model is.\nAn attention head is a component of the self-attention mechanism that learns different types of relationships within the data. Having multiple attention heads allows the model to capture various dependencies between tokens.\nReturning to our example:\n\nOne attention head might focus on subject-verb relationships, recognizing that “fox” is the subject of “jumps.”\nAnother head might capture adjective-noun relationships, linking “brown” to “fox.”\n\nThe hidden size defines the dimensionality of the model’s internal representations. A larger hidden size enables the model to capture more complex patterns, but also increases computational cost.\nBy stacking multiple transformer layers and attention heads, BERT gains a deep understanding of sentence structures.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html#defining-the-bert-model-for-dna-sequences",
    "href": "Chapter2_DNA.html#defining-the-bert-model-for-dna-sequences",
    "title": "2  Training our first DNA Language Model",
    "section": "2.6 Defining the BERT Model for DNA Sequences",
    "text": "2.6 Defining the BERT Model for DNA Sequences\nWhile the “quick brown fox” example helps us understand how BERT processes natural language, our goal is to apply the same principles to DNA sequences. Instead of predicting missing words in a sentence, we want our model to learn biological patterns and genomic structure by predicting masked nucleotides within DNA sequences.\nIn DNA modeling, understanding sequence context is just as critical as in language modeling. Just as BERT learns that “fox” fits within a given sentence structure, our model should learn that specific nucleotide sequences appear in biologically meaningful patterns. This could involve recognizing gene coding regions, regulatory motifs, or conserved sequence elements across different genomes.\nTo accomplish this, we define a custom BERT model designed specifically for processing DNA sequences. Unlike traditional text-based models, our DNA BERT model uses a character-level vocabulary of nucleotides (A, T, C, G) and special tokens to represent sequence structure. By leveraging masked language modeling (MLM), the model will learn to predict missing nucleotides based on surrounding context, allowing it to capture meaningful genomic features.\nWith this in mind, let’s move forward and define our BERT architecture for DNA sequences.\nPython Code:\nfrom transformers import ModernBertConfig, ModernBertForMaskedLM\n\nconfig = ModernBertConfig(\n    vocab_size=len(dna_vocab),\n    hidden_size=256,\n    num_hidden_layers=8,\n    num_attention_heads=8,\n    intermediate_size=512,\n    max_position_embeddings=512,\n    type_vocab_size=1,\n)\nconfig.pad_token_id = dna_vocab[\"[PAD]\"]\nmodel = ModernBertForMaskedLM(config)\n\n2.6.1 Configuring Training for DNA BERT\nNow that we have defined our BERT model for DNA sequences, we need to set up the training process. This involves specifying various training hyperparameters, handling masked language modeling (MLM) data, and preparing for efficient learning.\nUnlike traditional NLP tasks where models are trained on massive text corpora, training on DNA sequences presents unique challenges. The structure of genomic data, sequence length, and biological patterns all influence how the model learns. Our configuration ensures that the training process is efficient while maintaining biological relevance.\n\n\n\n2.6.2 Setting Training Parameters\nTo train our DNA BERT model, we use the Hugging Face TrainingArguments class, which allows us to define key training settings. These include:\n\nBatch size: We set a batch size of 16 for both training and evaluation. This determines how many sequences are processed at once.\nLogging & Saving: We log loss every 50 steps and save model checkpoints every 100 steps to monitor training progress.\nLearning Rate: We use a learning rate of 5e-5, a common choice for transformer models that balances learning speed and stability.\nWeight Decay: A value of 0.01 is used to prevent overfitting by applying L2 regularization to model weights.\nTraining Steps: The model is trained for 4000 steps. This ensures sufficient learning without excessive computation.\nModel Saving: The model checkpoints are stored in ./bert-dna, allowing us to resume training if needed.\n\nPython Code:\ntraining_args = TrainingArguments(\n    output_dir=\"./bert-dna\",\n    overwrite_output_dir=True,\n    logging_steps=50,  # Log loss every step\n    save_steps=100,\n    save_total_limit=2,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    max_steps=4000,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    push_to_hub=False,\n    report_to=\"none\",  # Disables wandb logging\n)\n\n\n\n2.6.3 Preparing for Masked Language Modeling (MLM)\nSince we are training our DNA BERT model using masked language modeling (MLM), we need to handle masked tokens properly. This is done using the DataCollatorForLanguageModeling, which:\n\nRandomly masks nucleotides in the training sequences.\nCreates labels automatically, meaning the model learns by trying to predict these masked tokens.\nUses a masking probability of 5%, ensuring that a small but meaningful portion of the sequence is masked during training.\n\nBy applying MLM, we allow the model to generalize nucleotide relationships and capture sequence dependencies, just like how BERT learns relationships between words in text.\nPython Code:\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=hf_tokenizer,\n    mlm=True,\n    mlm_probability=0.05\n)\n\n\n\n2.6.4 Training the DNA BERT Model\nWith our configuration and data collator in place, we now train the model. We use the Hugging Face Trainer API, which simplifies the training process by handling:\n\nDataset iteration: Automatically loads and batches training sequences.\nGradient updates: Adjusts model weights based on training loss.\nLogging & saving: Tracks training progress and stores checkpoints.\n\nOnce training begins, the model will gradually learn nucleotide dependencies and improve its ability to predict missing DNA bases.\nPython Code:\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=hf_tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()\nyou set up free wandb logging (go to https://wandb.ai/site for more info) you can track your training runs online, wherever they are running. You then get a dashboard full of pretty loss vs progress plots like the one below which I screencapped about ± 30 minutes into training on my macbook.\n\n\n\n2.6.5 Saving the Trained Model\nAfter training completes, we save both the model and tokenizer so they can be used for future predictions or fine-tuning.\n\nThe model weights are stored in ./bert-dna, allowing us to reload the trained model.\nThe tokenizer is also saved, ensuring that input sequences can be processed the same way during inference.\n\nFinally, a success message is printed, confirming that the training process has been completed.\nPython Code:\n# Save the final model and tokenizer\ntrainer.save_model(\"./bert-dna\")\nhf_tokenizer.save_pretrained(\"./bert-dna\")\n\nprint(\"🎉 Training complete! Model saved to ./bert-dna\")\n\n\n2.6.6 Summary\nIn this section, we:\n\nDefined training hyperparameters such as batch size, learning rate, and training steps.\nUsed masked language modeling (MLM) to train the model on DNA sequences.\nLeveraged the Hugging Face Trainer API to automate model training.\nSaved the final trained model and tokenizer for future use.\n\nWith this trained model, we can now fine-tune or apply it to various genomic tasks, such as predicting genetic variations or classifying functional DNA sequences. In the next chapter, we will explore how to fine-tune our DNA BERT model for specific applications.\n\n\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. “DNA Language Models Are Powerful Predictors of Genome-Wide Variant Effects.” Proceedings of the National Academy of Sciences 120 (44). https://doi.org/10.1073/pnas.2311219120.\n\n\nBenegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun S. Song. 2025. “Genomic Language Models: Opportunities and Challenges.” Trends in Genetics, January. https://doi.org/10.1016/j.tig.2024.11.013.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html",
    "href": "Chapter3_DNA.html",
    "title": "3  Evaluating DNA Language Models",
    "section": "",
    "text": "3.1 Introduction\nIn Chapters 1 and 2, we introduced the process of preparing DNA sequences and training a BERT language model for genomic data. In this chapter, we will turn our attention to how single nucleotide mutations can be systematically generated and evaluated using the trained DNA language model.\nThese synethetic mutations can form the basis for an evaluation of our DNA language model.\nThis chapter has two goals:",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#introduction",
    "href": "Chapter3_DNA.html#introduction",
    "title": "3  Evaluating DNA Language Models",
    "section": "",
    "text": "To explain to machine learning readers how to enumerate synonymous and missense mutations based on the standard genetic code, and why this is biologically meaningful. Consider the Kahn Academy “ap bio” course if this chapter doesn’t really offer enough for you https://www.khanacademy.org/science/ap-biology\nTo explain to bioinformatics and genetics readers how the Masked Language Modeling (MLM) objective provides a way to compute the pseudo-log-likelihood (PLL) of entire sequences and to score mutations in terms of their “naturalness” under the trained model.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#biological-background-the-genetic-code-and-mutation-types",
    "href": "Chapter3_DNA.html#biological-background-the-genetic-code-and-mutation-types",
    "title": "3  Evaluating DNA Language Models",
    "section": "3.2 Biological Background: The Genetic Code and Mutation Types",
    "text": "3.2 Biological Background: The Genetic Code and Mutation Types\nBefore diving into code, it’s useful to recall the basics of how DNA encodes proteins. DNA is transcribed into RNA, and RNA is translated into proteins using codons, groups of three nucleotides. Each codon corresponds to a specific amino acid — this mapping is called the genetic code.\n\n\n\nFigure 3.1 DNA is translated to RNA then transcribed to amino-acids which form proteins. Source: https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/intro-to-gene-expression-central-dogma\n\n\nCrucially, some amino acids can be encoded by multiple codons, a property called degeneracy. This degeneracy is why synonymous mutations exist — changes in the DNA sequence that do not alter the resulting amino acid. In contrast, missense mutations alter the encoded amino acid, which may change protein function.\nThis distinction between synonymous and missense mutations will allow us to systematically categorize the impact of each possible single nucleotide substitution. Below is the standard genetic code table it contains a full translation from DNA to protein.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodon\nAmino Acid\nCodon\nAmino Acid\nCodon\nAmino Acid\nCodon\nAmino Acid\n\n\n\n\nTTT\nF\nTTC\nF\nTTA\nL\nTTG\nL\n\n\nTCT\nS\nTCC\nS\nTCA\nS\nTCG\nS\n\n\nTAT\nY\nTAC\nY\nTAA\nStop\nTAG\nStop\n\n\nTGT\nC\nTGC\nC\nTGA\nStop\nTGG\nW\n\n\nCTT\nL\nCTC\nL\nCTA\nL\nCTG\nL\n\n\nCCT\nP\nCCC\nP\nCCA\nP\nCCG\nP\n\n\nCAT\nH\nCAC\nH\nCAA\nQ\nCAG\nQ\n\n\nCGT\nR\nCGC\nR\nCGA\nR\nCGG\nR\n\n\nATT\nI\nATC\nI\nATA\nI\nATG\nM\n\n\nACT\nT\nACC\nT\nACA\nT\nACG\nT\n\n\nAAT\nN\nAAC\nN\nAAA\nK\nAAG\nK\n\n\nAGT\nS\nAGC\nS\nAGA\nR\nAGG\nR\n\n\nGTT\nV\nGTC\nV\nGTA\nV\nGTG\nV\n\n\nGCT\nA\nGCC\nA\nGCA\nA\nGCG\nA\n\n\nGAT\nD\nGAC\nD\nGAA\nE\nGAG\nE\n\n\nGGT\nG\nGGC\nG\nGGA\nG\nGGG\nG\n\n\n\nA single nucleotide mutation can cause:\n\nA synonymous mutation: The amino acid does not change, meaning the mutation is “silent” in terms of protein sequence.\n\nFor example in row 1 pf the table we see that if we mutate the codon TTT to TTC both before and after the mutation the amino-acid F (phe) is produced. While its not guaranteed by any means that a synonymous mutation is entirely harmless their very likely to be harmless.\n\nA missense mutation: The amino acid changes, potentially altering protein structure and function.\n\nFor example in row 1 pf the table we see that if we mutate the codon TTT to TTA the amino-acid F (phe) is replaced by L (leu) in the protein, potentially changing the function. While missense mutations aren’t always damaging, they are fare more likely to be damaging.\n\n\nEarlier we trained a DNA language model on coding sequences for humans, ad I actually expanded that to a training run of 2 epochs (the data was all used twice) on 500k sequences from 13 vertebrae species. This model should, with probabilities slightly above chance,",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#enumerating-all-single-nucleotide-mutants",
    "href": "Chapter3_DNA.html#enumerating-all-single-nucleotide-mutants",
    "title": "3  Evaluating DNA Language Models",
    "section": "3.3 Enumerating All Single-Nucleotide Mutants",
    "text": "3.3 Enumerating All Single-Nucleotide Mutants\nThe code in this section systematically generates every possible single nucleotide substitution across the input sequence. Since each codon consists of three nucleotides, and each nucleotide can mutate into three alternatives, there are up to 9 potential codon variants for each original codon.\n\n\n\n\n\n\nTip\n\n\n\nThe data generated by applying this “mutator” to the DRD2 (Dopamine receptor D2) gene is on huggingface: https://huggingface.co/datasets/MichelNivard/DRD2-mutations\n\n\nFor each mutation, we check the original amino acid and the new amino acid using the standard genetic code table. This allows us to classify each mutation as either:\n\nSynonymous — Same amino acid, no apparent change to the protein.\nMissense — Different amino acid, potential change to protein function.\n\nThis step is crucial in genomics, where we often want to prioritize functional variants — mutations that actually change protein products, rather than silent changes that do not.\nI have a preference for R myself, so I wrote this specific job in R.We provide the gene sequence, starting at the start codon, I use the dopamine receptor gene DRD2. Based on the genetic code, which translates DNA to the amino-acids that eventually are produced, we then write code to mutate each codon in a gene.\nDRD2 &lt;- \"ATGGATCCACTGAATCTGTCCTGGTATGATGATGATCTGGAGAGGCAGAACTGGAGCCGGCCCTTCAACGGGTCAGACGGGAAGGCGGACAGACCCCACTACAACTACTATGCCACACTGCTCACCCTGCTCATCGCTGTCATCGTCTTCGGCAACGTGCTGGTGTGCATGGCTGTGTCCCGCGAGAAGGCGCTGCAGACCACCACCAACTACCTGATCGTCAGCCTCGCAGTGGCCGACCTCCTCGTCGCCACACTGGTCATGCCCTGGGTTGTCTACCTGGAGGTGGTAGGTGAGTGGAAATTCAGCAGGATTCACTGTGACATCTTCGTCACTCTGGACGTCATGATGTGCACGGCGAGCATCCTGAACTTGTGTGCCATCAGCATCGACAGGTACACAGCTGTGGCCATGCCCATGCTGTACAATACGCGCTACAGCTCCAAGCGCCGGGTCACCGTCATGATCTCCATCGTCTGGGTCCTGTCCTTCACCATCTCCTGCCCACTCCTCTTCGGACTCAATAACGCAGACCAGAACGAGTGCATCATTGCCAACCCGGCCTTCGTGGTCTACTCCTCCATCGTCTCCTTCTACGTGCCCTTCATTGTCACCCTGCTGGTCTACATCAAGATCTACATTGTCCTCCGCAGACGCCGCAAGCGAGTCAACACCAAACGCAGCAGCCGAGCTTTCAGGGCCCACCTGAGGGCTCCACTAAAGGAGGCTGCCCGGCGAGCCCAGGAGCTGGAGATGGAGATGCTCTCCAGCACCAGCCCACCCGAGAGGACCCGGTACAGCCCCATCCCACCCAGCCACCACCAGCTGACTCTCCCCGACCCGTCCCACCATGGTCTCCACAGCACTCCCGACAGCCCCGCCAAACCAGAGAAGAATGGGCATGCCAAAGACCACCCCAAGATTGCCAAGATCTTTGAGATCCAGACCATGCCCAATGGCAAAACCCGGACCTCCCTCAAGACCATGAGCCGTAGGAAGCTCTCCCAGCAGAAGGAGAAGAAAGCCACTCAGATGCTCGCCATTGTTCTCGGCGTGTTCATCATCTGCTGGCTGCCCTTCTTCATCACACACATCCTGAACATACACTGTGACTGCAACATCCCGCCTGTCCTGTACAGCGCCTTCACGTGGCTGGGCTATGTCAACAGCGCCGTGAACCCCATCATCTACACCACCTTCAACATTGAGTTCCGCAAGGCCTTCCTGAAGATCCTCCACTGCTGA\"}\nnchar(DRD2)/3\n\n# Genetic code table (Standard Code)\ngenetic_code &lt;- c(\n  \"TTT\"=\"F\", \"TTC\"=\"F\", \"TTA\"=\"L\", \"TTG\"=\"L\",\n  \"TCT\"=\"S\", \"TCC\"=\"S\", \"TCA\"=\"S\", \"TCG\"=\"S\",\n  \"TAT\"=\"Y\", \"TAC\"=\"Y\", \"TAA\"=\"Stop\", \"TAG\"=\"Stop\",\n  \"TGT\"=\"C\", \"TGC\"=\"C\", \"TGA\"=\"Stop\", \"TGG\"=\"W\",\n  \"CTT\"=\"L\", \"CTC\"=\"L\", \"CTA\"=\"L\", \"CTG\"=\"L\",\n  \"CCT\"=\"P\", \"CCC\"=\"P\", \"CCA\"=\"P\", \"CCG\"=\"P\",\n  \"CAT\"=\"H\", \"CAC\"=\"H\", \"CAA\"=\"Q\", \"CAG\"=\"Q\",\n  \"CGT\"=\"R\", \"CGC\"=\"R\", \"CGA\"=\"R\", \"CGG\"=\"R\",\n  \"ATT\"=\"I\", \"ATC\"=\"I\", \"ATA\"=\"I\", \"ATG\"=\"M\",\n  \"ACT\"=\"T\", \"ACC\"=\"T\", \"ACA\"=\"T\", \"ACG\"=\"T\",\n  \"AAT\"=\"N\", \"AAC\"=\"N\", \"AAA\"=\"K\", \"AAG\"=\"K\",\n  \"AGT\"=\"S\", \"AGC\"=\"S\", \"AGA\"=\"R\", \"AGG\"=\"R\",\n  \"GTT\"=\"V\", \"GTC\"=\"V\", \"GTA\"=\"V\", \"GTG\"=\"V\",\n  \"GCT\"=\"A\", \"GCC\"=\"A\", \"GCA\"=\"A\", \"GCG\"=\"A\",\n  \"GAT\"=\"D\", \"GAC\"=\"D\", \"GAA\"=\"E\", \"GAG\"=\"E\",\n  \"GGT\"=\"G\", \"GGC\"=\"G\", \"GGA\"=\"G\", \"GGG\"=\"G\"\n)\n\n# Function to get all mutations for a codon\nmutate_codon &lt;- function(codon, codon_index, full_sequence) {\n  nucleotides &lt;- c(\"A\", \"T\", \"C\", \"G\")\n  mutations &lt;- data.frame()\n  \n  original_aa &lt;- genetic_code[[codon]]\n  \n  for (pos in 1:3) {\n      original_base &lt;- substr(codon, pos, pos)\n      for (nuc in nucleotides) {\n          if (nuc != original_base) {\n              # Mutate the codon at this position\n              mutated_codon &lt;- codon\n              substr(mutated_codon, pos, pos) &lt;- nuc\n              mutated_aa &lt;- genetic_code[[mutated_codon]]\n              \n              # Create the mutated sequence\n              mutated_sequence &lt;- full_sequence\n              start &lt;- (codon_index - 1) * 3 + 1\n              substr(mutated_sequence, start, start+2) &lt;- mutated_codon\n              \n              mutation_type &lt;- if (mutated_aa == original_aa) \"synonymous\" else \"missense\"\n              \n              mutations &lt;- rbind(mutations, data.frame(\n                  codon_index = codon_index,\n                  position = pos,\n                  original_codon = codon,\n                  mutated_codon = mutated_codon,\n                  original_aa = original_aa,\n                  mutated_aa = mutated_aa,\n                  mutation_position = (codon_index -1)*3 + pos,\n                  mutation_type = mutation_type,\n                  sequence = mutated_sequence\n              ))\n          }\n      }\n  }\n  return(mutations)\n}\nTHen we write code to mutate all codons within the gene and save all the mutations, and add a column that indicates whether their missense or synonymous mutations.\n# Main function to process the whole sequence\nmutate_sequence &lt;- function(dna_sequence) {\n  codons &lt;- strsplit(dna_sequence, \"\")[[1]]\n  codons &lt;- sapply(seq(1, length(codons), by=3), function(i) paste(codons[i:(i+2)], collapse=\"\"))\n  all_mutations &lt;- data.frame()\n  \n  for (i in seq_along(codons)) {\n      codon &lt;- codons[i]\n      mutations &lt;- mutate_codon(codon, i, dna_sequence)\n      all_mutations &lt;- rbind(all_mutations, mutations)\n  }\n  return(all_mutations)\n}\n\n# Example usage\nsequence &lt;- DRD2\nmutations &lt;- mutate_sequence(sequence)\n\n\n# Filter synonymous and missense if needed\nsynonymous_mutations &lt;- subset(mutations, mutation_type == \"synonymous\")\nmissense_mutations &lt;- subset(mutations, mutation_type == \"missense\")\n\nsource &lt;- c(NA,\"wildtype\",DRD2)\n\noutput &lt;- rbind(source,mutations[,7:9])\n\n\nwrite.csv(file=\"DRD2_mutations.csv\",output)\nThe code can be used to generate massive amounts of mutations for validation, but missense vs synonymous is a very simple and crude distinction to make! to fully validate a DNA model we’ll want to obtain additional external information and well do so in later chapters!",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#evaluating-base-position-likelihoods-with-a-bert-model",
    "href": "Chapter3_DNA.html#evaluating-base-position-likelihoods-with-a-bert-model",
    "title": "3  Evaluating DNA Language Models",
    "section": "3.4 Evaluating base position likelihoods with a BERT Model",
    "text": "3.4 Evaluating base position likelihoods with a BERT Model\nIn machine learning terms, the MLM loss is the negative log likelihood (NLL) of the correct nucleotide. For example, if the correct nucleotide is “A” at a given position, and the model assigns “A” a probability of 0.8, then the contribution to the loss is:\n\\[loss = −ln(0.8) = 0.22\\]\nThe lower this value, the better the model’s confidence matches reality — indicating that the nucleotide was expected. Near the end of training our models loss hovered around 1.09, meaning that the average true base had a predicted probability of ±34%. The loss is highly dependent on the tokenizer, for example if we would have used a more complex tokenizer with say 100 options for each next token (encoding for example all 3-mer combinations of bases: A,C,T,G,AA,AC,AT,AG etc etc until GGA,GGG) the the probability of geting the one correct token is way lower as the base rate is way lower!\nWhen we compute the pseudo-log-likelihood (PLL) for an entire sequence, we mask and score each position, adding up these log probabilities:\n\\[log⁡P(nucleotide_1)+log⁡P(nucleotide_2)+⋯+log⁡P(nucleotide_n)​\\]\nThis sum is the total log likelihood of the sequence under the model — it quantifies how natural the model thinks the sequence is.\nFirst we load the model I trained in Chapter 2, if you trained your own on more data, or for longer, or want to evaluate a different model you can load those yourself easily.\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport torch\nimport pandas as pd\n\n# Load model & tokenizer\nmodel_name = \"MichelNivard/DNABert-CDS-13Species-v0.1\"  # Replace if needed\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\nmodel.eval()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Maximum context length — BERT's trained context window\nMAX_CONTEXT_LENGTH = 512\nThen we define 2 functions to compute the pseudo likelihood of the whole sequence up to 512 bases as that is the sequence length we trained DNABert for (in full scale applications you’d use a longer sequence length) and 2. the log likelihood ratio of the mutation vs the wildtype (original DRD2 sequence).\ndef compute_log_likelihood(sequence, tokenizer, model):\n    \"\"\"Compute pseudo-log-likelihood (PLL) for the first 512 bases.\"\"\"\n    tokens = tokenizer(sequence, return_tensors='pt', add_special_tokens=True)\n    input_ids = tokens['input_ids'].to(device)\n    attention_mask = tokens['attention_mask'].to(device)\n\n    log_likelihood = 0.0\n    seq_len = input_ids.shape[1] - 2  # Exclude [CLS] and [SEP]\n\n    with torch.no_grad():\n        for i in range(1, seq_len + 1):\n            masked_input = input_ids.clone()\n            masked_input[0, i] = tokenizer.mask_token_id\n\n            outputs = model(masked_input, attention_mask=attention_mask)\n            logits = outputs.logits\n\n            true_token_id = input_ids[0, i]\n            log_probs = torch.log_softmax(logits[0, i], dim=-1)\n            log_likelihood += log_probs[true_token_id].item()\n\n    return log_likelihood\n\n\ndef compute_mutant_log_likelihood_ratio(wild_type, mutant, position, tokenizer, model):\n    \"\"\"Compare wild type and mutant likelihood at a single position (within 512 bases).\"\"\"\n    assert len(wild_type) == len(mutant), \"Wild type and mutant must have the same length\"\n    assert wild_type[position] != mutant[position], f\"No mutation detected at position {position + 1}\"\n\n    tokens = tokenizer(wild_type[:MAX_CONTEXT_LENGTH], return_tensors='pt', add_special_tokens=True)\n    input_ids = tokens['input_ids'].to(device)\n    attention_mask = tokens['attention_mask'].to(device)\n\n    mask_position = position + 1  # Shift for [CLS] token\n\n    masked_input = input_ids.clone()\n    masked_input[0, mask_position] = tokenizer.mask_token_id\n\n    with torch.no_grad():\n        outputs = model(masked_input, attention_mask=attention_mask)\n        logits = outputs.logits\n\n        log_probs = torch.log_softmax(logits[0, mask_position], dim=-1)\n\n    wild_base_id = tokenizer.convert_tokens_to_ids(wild_type[position])\n    mutant_base_id = tokenizer.convert_tokens_to_ids(mutant[position])\n\n    log_prob_wild = log_probs[wild_base_id].item()\n    log_prob_mutant = log_probs[mutant_base_id].item()\n\n    return log_prob_wild - log_prob_mutant\n\n3.4.1 The Likelihood Ratio to evaluate mutations\nThe log-likelihood ratio (LLR) compares how much more (or less) likely the wild-type sequence is compared to a mutant sequence, given the DNA language model. Specifically, we compare the log likelihood of the correct wild-type nucleotide to the log likelihood of the mutant nucleotide at the mutated position only.\n\\[LLR = log ⁡ P ( wild-type nucleotide ∣ context ) − log ⁡ P ( mutant nucleotide ∣ context )\\]\nThis metric is widely used in bioinformatics because it focuses on the exact site of the mutation, instead of comparing entire sequences. A positive LLR indicates the wild-type is favored by the model (the mutation is unlikely and therefore possibly deliterious), while a negative LLR means the mutant is more likely (the mutation is neural or maybe even protectivr).\nWe then apply these functions to all the synthetic DRD2 mutations we generated (in the first 512 bases) to evaluate whether the DNABert we trained thinks the missense mutations are generally less likely, and therefore possibly damaging, given the model.\n# Load dataset directly from Hugging Face dataset repo\ndataset_url = \"https://huggingface.co/datasets/MichelNivard/DRD2-mutations/raw/main/DRD2_mutations.csv\"\ndf = pd.read_csv(dataset_url)\n\n# Find wild-type sequence\nwild_type_row = df[df['mutation_type'] == 'wildtype'].iloc[0]\nwild_type_sequence = wild_type_row['sequence'][:MAX_CONTEXT_LENGTH]\n\nresults = []\n\n# Process all sequences\nfor idx, row in df.iterrows():\n    sequence = row['sequence'][:MAX_CONTEXT_LENGTH]\n    mutation_type = row['mutation_type']\n    mutation_position = row['mutation_position'] - 1  # Convert 1-based to 0-based\n\n    # Skip mutants where the mutation position is beyond 512 bases\n    if mutation_type != 'wildtype' and mutation_position &gt;= MAX_CONTEXT_LENGTH:\n        continue\n\n    print(idx)\n\n    llr = None\n    log_prob_wild = None\n    prob_wild = None\n\n    if mutation_type != 'wildtype':\n        llr, log_prob_wild, prob_wild = compute_mutant_log_likelihood_ratio(\n            wild_type_sequence, sequence, int(mutation_position), tokenizer, model\n        )\n\n    # append results for each mutation:\n    results.append({\n        'sequence': sequence,\n        'mutation_type': mutation_type,\n        'pll': 0,\n        'llr': llr,\n        'wildtype_log_prob': log_prob_wild,\n        'wildtype_prob': prob_wild,\n        'mutation_position': mutation_position + 1\n    })\n\n\n# Convert to DataFrame for saving or inspection\nresults_df = pd.DataFrame(results)\n\n# Save or print results\nprint(results_df)\n\n# Optionally, save to CSV\nresults_df.to_csv(\"sequence_log_likelihoods.csv\", index=False)\n\n\n3.4.2 Language Models Provide Biological Insight trough the likelihood ratio\nWhy do we care about these log likelihoods and log likelihood ratios? Because they provide a direct, data-driven estimate of how plausible or “natural” each mutated sequence looks to the model compared to the wild type sequence. Since the model was trained on real DNA sequences, sequences with high likelihoods resemble biological reality, while sequences with low likelihoods deviate from patterns the model has learned. A high “mutation log likelihood ratio” corresponds tot he model strongly favoring the reference sequences over the mutation. This test lets us flag potentially deleterious mutations (those with sharp likelihood drops), prioritize candidate variants for functional follow-up, or even explore adaptive evolution by identifying mutations that DNA BERT “likes” more than the wild-type\nTo explore our result here, we can plot the LLR versus the position within the DRD2 gene, this can give us insight into the location within the coding sequence where we find unlikely (and therefore potentially damaging) mutations. in the lot below a LOW LLR means the variant is unlikely. Most variants cluster around a neural LLR, consistent with some statistical noise.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter to only mutations (skip wildtype which has no llr)\nplot_df = results_df[results_df['mutation_type'].isin(['synonymous', 'missense'])].copy()\n\n\n# Optional: Clip LLR to avoid excessive sizes\nplot_df['size'] = plot_df['llr'].clip(-5, 5)  # LLRs smaller than -5 get maximum size\n\n# Scatter plot with enhanced size scaling\nplt.figure(figsize=(14, 5))\nsns.scatterplot(\n    x='mutation_position', \n    y='llr', \n    hue='mutation_type', \n    size='size',  # Use clipped size column\n    sizes=(20, 200),  # Bigger range for better visibility\n    alpha=0.7, \n    palette={'synonymous': 'green', 'missense': 'orange'},\n    data=plot_df\n)\nplt.axhline(0, color='gray', linestyle='--', label='Neutral LLR')\nplt.title('Mutation Log Likelihood Ratio (LLR) Along DRD2 Gene')\nplt.xlabel('Position in Gene')\nplt.ylabel('Log Likelihood Ratio (LLR)')\nplt.legend(title='Mutation Type', bbox_to_anchor=(1.02, 1), loc='upper left')\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\nFigure 3.2 The LLR for the mutation (y-axis) give the position in the DRD2 gne (x-axis). High values indicate the reference, or wild, type sequence is far more likely then the mutation. Very high LLR values are almost exclusively missense mutations, consistent with a DNA model able to pick up dewleterious variants based on its training\n\n\nIts obvious from Figure 3.2 that 1. really almost all very unlikely mutations (positive LLR) are missense mutations and 2. There are potentially certain locations within this particular coding sequence where there is an abundance of unlikely mutations packed closely together, these could be regions that are intolerant to deleterious mutations.\nIts important to not get overconfident in our predictions! Rember this is a relatively tiny DNA sequence model (±5m parameters) we trained on sequences for 13 fairly randomly picked vertebrae. Lets look at the likelihood of the true base in the references (wild-type) sequence given the model. The mean probability is 40% (Figure 3.3), given the model essentially is trying to pick between 4 tokens (G,C,T & A) 40% is considerably better then chance! Its also clear the probability is not even across the gene, the first few based are almost certain (almost all coding sequences in these species start with the start codon ATG, the model obviously learned this). After that there is quite a spread, which is logical I think, in many places across the sequence the specific base might be very obvious, as all three alternates might be missense mutations, but in other spot one, two or even all three alternate tokens might be synonymous, and perhaps even present int he analog gene int he other 12 species we trained our model on! This would make the model FAR less certain about the true base at that location.\n\n\n\nFigure 3.3 The probability of the base in the reference, or wild type, sequence given the DNABert model we trained. The model clearly performed above random (random guessing would be 1 in 4, or 25%).",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#summary",
    "href": "Chapter3_DNA.html#summary",
    "title": "3  Evaluating DNA Language Models",
    "section": "3.5 Summary",
    "text": "3.5 Summary\nThis chapter introduced key pieces that are esential for those who want to train, or just understand, DNA Language models.\n\nWe explored how to systematically generate all synonymous and missense mutations in a gene, these simuated mutations then form an important part in imnnitial evaluation of our model.\nWe discussed how to compute the log-likelihood. of a sequences, and log-likelihood ratio of a single mutation using DNA BERT. These metrics are a proxy for how natural the model considers each sequence.\nWe finally used these simulated mutation, some knowledge of biology (whether the mutations are synonymous or missense) to validate our language model actually did do some learning.\n\nThe analyses outlined in this chapter form the foundation for variant effect prediction using genomic language models.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html",
    "href": "Chapter4_DNA.html",
    "title": "4  Evolution-Aware Encoders",
    "section": "",
    "text": "4.1 Introduction\nIn previous chapters, we introduced the basic principles of BERT for DNA sequences. We took inspiration from natural language processing (NLP), treating DNA as a language, where sequences of nucleotides (A, T, C, G, -) could be processed using transformers. This approach, while powerful, carries over several assumptions from natural language that do not perfectly align with biological sequences. In this chapter, we will re-examine how we encode genomic data and introduce a new design paradigm — evolutionary-aware encoding — inspired by the recently proposed GPN (Genomic Pre-trained Network).",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#tokenization-and-embedding-in-language-models",
    "href": "Chapter4_DNA.html#tokenization-and-embedding-in-language-models",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.2 Tokenization and Embedding in Language Models",
    "text": "4.2 Tokenization and Embedding in Language Models\nModern language models, whether BERT, GPT, or similar architectures, rely heavily on how input sequences are tokenized and encoded before they ever reach the attention layers. This initial step — often overlooked — plays a profound role in shaping how the model learns.\n\n4.2.1 Tokenization in Natural Language\nIn human languages like English or French, the vocabulary is large, often comprising tens of thousands of tokens. These tokens could be:\n\nWhole words (“cat”, “sat”).\nSubwords (“cat” might break into “c”, “at”).\nEven characters (in rare cases).\n\nSince the number of tokens is so large, each token is assigned a unique vector embedding — a dense, learnable representation of its “meaning”. These embedding are gradually refined during training as the model learns how tokens behave in different contexts. The model learns, based on the massive amounts of training data what the word means, what other words have similar or related meanings. This is essential because linguists and those who study language have vast knowledge of word meaning, numerically encoding that knowledge, such that a computational model could process isn’t currently a feasible task. Therefore in a natural (as opposed to biological) large language model word embedding are learned from the data, the data being all the text on the internet.\n\n\n4.2.2 The Embedding Process (NLP BERT)\nInput Sentence:  \"The cat sat on the mat\"\n\nStep 1 - Tokenization:\n    [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n\nStep 2 - Lookup:\n    Each token gets a fixed vector from an embedding table.\n\n    \"The\" -&gt; [0.25, 0.13, -0.11, ..., 0.04]\n    \"cat\" -&gt; [0.88, -0.23, 0.45, ..., -0.67]\n\nStep 3 - Transformer Layers:\n    These embeddings are updated based on surrounding words (context).\n\n\n4.2.3 Language Evolution is Decayed\nThe design of these token embeddings reflects a key fact about human languages: the evolutionary history of words might be relevant to understanding their meaning today, but the words context in text is way more informative. While linguistic etymology exists, the meaning of “cat” today does not rely on whether the word originated from Latin or Proto-Indo-European. Context (the words around “cat”) matters far more than distant etymology. Even if I am unfairly discounting the importance of etymology in linguistics (I am no linguist, don’t take my word for it), the quantity of older texts, relative to the quantity of modern texts, the lack of obvious coding scheme for embedding a word in its etymological history are problematic and would have to be very effecice given how effective “word in textual context” embeddings are. However, biology, and DNA in particular, is different.\n\n\n4.2.4 Biological Sequences are Fundamentally Different\nThe DNA encoding we have been working with (A, T, G, C, -) has 5 tokens, perhaps 20 if we encode all the codes used in genetics to code for ambiguous or missing bases. Protein language models we’ll cover later have ±20 amino-acids commonly found in proteins. If we use longer vocabularies, like k-mer or BPE tokenizer vocabularies its not clear the longer sequences we obtain really are comparable, or interchangeable. The point of embedding is to cluster similar and dissimilarities, in order to predict the next, or a masked, token if the presence of up to 128.000 tokens to chose from some of which have very similar meanings, or could fully alter the meaning of a sentence (by negation, or omission). Ins biology we have a small vocabulary, 5 or 20 or if you wish up to a few hundred tokens. We do however have an incredible understanding of the evolutionary history (Figure 4.1) of each base in the genome, we know its place in the genome of other species an can align those to eachother!\n\n\n\nFigure 4.1 The Evogeneao Tree of Life diagram all rights reserved Leonard Eisenberg (2008 & 2017) get posters and relevant teaching materials here: https://www.evogeneao.com/en/learn/tree-of-life\n\n\n\n\n4.2.5 Evolutionary context as an embedding\nThe evolutionary history of a genomic position — how conserved it is, how it varies across species — directly influences our estimation of its importance and its tolerance to mutation. A nucleotide in a highly conserved enhancer region requires different level of attention (from the model, or us scientists) then a nucleotide in a rapidly evolving spacer.\n\n\n4.2.6 \n\nTable 4.1 Key Differences Between Language and DNA\n\n\n\n\n\n\n\nAspect\nNatural Language\nGenomics\n\n\n\n\nNumber of Tokens\nTens of thousands\n~5 (A, T, G, C, -)\n\n\nMeaning\nFlexible, evolves over time\nBiochemically fixed\n\n\nEvolutionary Context\nMostly irrelevant to meaning\nOften crucial (conservation, divergence)\n\n\nToken Embedding\nFully learned\nNo unique encoding for each token, but predefined based on token specific evolutionary history\n\n\nNeighboring Context\nDefines meaning\nDefines local motifs, but evolutionary context adds extra layer\n\n\n\nTo capture this cross-species evolutionary context, we need an embedding strategy that combines:\n\nThe identity of the nucleotide itself (A, T, G, C, -).\nThe state of this position in aligned species (what bases appear at the same position in other species).\n\nThis evolutionary-aware encoding is at the heart of the Genomic Pre-trained Network (GPN) architecture an various famous protein language models like AlphaFold(Benegas et al. 2023; Lupo, Sgarbossa, and Bitbol 2022; Jumper et al. 2021). in DNA nwetworks we’ll discuss in this chapter the encoding is computed for each base given tis history. So while the model has 5 tokens (G,C,T,A, and - ) these tokens do not map to a fixed embedding, rather the base “A” maps to an encoding (one-hot encoding) for A, but then also for the same base in aligned sequences of 99 non-human species. This fundamentally chances the model architecture, changing it from the a language model applied to DNA as we did in chapter 3 to a DNA language model, or maybe even just a DNA model,",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#introducing-gpn-msa-bert",
    "href": "Chapter4_DNA.html#introducing-gpn-msa-bert",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.3 4. Introducing GPN-MSA-BERT",
    "text": "4.3 4. Introducing GPN-MSA-BERT\nGPN-MSA-BERT (inspired by Benegas et al. (2023)) adapts BERT-style masked language modeling (MLM) to DNA sequences, but incorporates multispecies alignment (MSA) data directly into the model’s input.\n\n\n\nFigure 4.2 an example multiple sequence alignment (MSA) across 7 sequences (usually species). Source: https://www.biorender.com/template/multiple-sequence-alignment-dna author: Eunice Huang\n\n\n\n4.3.1 Key Idea: Dynamic Position Embeddings\nFor each position in the human genome, the model receives:\n\nThe human base (A, T, G, C, -) — this is the usual input.\nThe aligned bases from other species — these are additional features.\nThese aligned bases are one-hot encoded and concatenated to the human base’s embedding.\n\nThis turns a simple nucleotide (A) into a dynamic, position-specific vector that depends on its evolutionary context across species.\n\n\n4.3.2 Visualization\nHuman Position:     A\nAligned Species:    A  G  A  (species 1, species 2, species 3)\n\nEmbedding:\n    [ OneHot_A | OneHot_A | OneHot_G | OneHot_A ]\nThis combined vector captures:\n\nWhat the human base is.\nHow conserved the site is.\nWhich substitutions are tolerated across species.\n\n\n\n4.3.3 Practical Implementation - Replacing the BERT Encoder\nTo implement this in practice, we can directly modify a Hugging Face model class (like ModernBertForMaskedLM) to use our custom GPNEmbedding layer in place of the standard token embedding layer.\nThis requires:\n\nDefining a tokenizer that toekenizes each base, and aligned bases in other species into the structure expected by the embedding\nDefining a GPNEmbedding class that can handle one-hot human base with species features and builds the embedding for each base.\nrequires we replace ModernBertForMaskedLM with a custom GPNBBERTMaskedLM class\nEnsuring all forward methods accept both input_ids and aux_features, which are passed into the embedding layer.\nWe additionally define our own tokenizer, and data collator (not show here but available in the full script)\n\nTHe code below takes a human sequence, encodes it in a one hot encoding (so A: 10000, T: 01000, G: 00100, C: 00010, -: 00001 for example), does the same for any auxiliary aligned sequences from other species. Then the embedding function combines both into one.\n# --------------------------------\n# 5. Encode Humand and auxillary species sequnces in \n# --------------------------------\n\ndef one_hot_encode_base(base):\n    \"\"\"One-hot encodes A, T, G, C, - (5 bases total).\"\"\"\n    base_to_idx = {\"A\": 0, \"T\": 1, \"C\": 2, \"G\": 3, \"-\": 4}\n    one_hot = np.zeros(5, dtype=np.float32)\n    if base in base_to_idx:\n        one_hot[base_to_idx[base]] = 1.0\n    return one_hot\n\ndef tokenize_with_aux(examples):\n    human_seq = clean_sequence(examples[\"human_sequence\"])\n\n    # Drop first 10 species (closest relatives)\n    species_seqs = [clean_sequence(seq) for seq in examples[\"species_sequences\"]]\n    species_seqs = species_seqs[10:]  # &lt;-- This line omits the first 10 species\n\n    # Tokenize human sequence\n    tokens = hf_tokenizer(human_seq, truncation=True, padding=\"max_length\", max_length=512)\n    input_ids = tokens[\"input_ids\"]\n\n    # Process species sequences into concatenated one-hot vectors (aux features)\n    seq_len = len(input_ids)\n    num_species = len(species_seqs)\n\n    aux_features = np.zeros((seq_len, num_species * 5), dtype=np.float32)\n\n    for pos in range(seq_len):\n        if pos &gt;= len(human_seq):  # Handle padding case\n            break\n        for species_idx, species_seq in enumerate(species_seqs):\n            if pos &lt; len(species_seq):\n                aux_features[pos, species_idx * 5:(species_idx + 1) * 5] = one_hot_encode_base(species_seq[pos])\n\n    tokens[\"aux_features\"] = aux_features.tolist()\n    return tokens\n\n\n# --------------------------------\n# 8. Define GPNEmbedding \n# --------------------------------\nclass GPNEmbedding(nn.Module):\n    def __init__(self, config, n_species):\n        super().__init__()\n        self.config = config\n        self.n_species = n_species\n        self.vocab_size = 5  # A, T, G, C, -\n        self.species_feature_size = self.n_species * self.vocab_size\n\n    def forward(self, input_ids, aux_features):\n        one_hot = F.one_hot(input_ids, num_classes=self.config.vocab_size).float()\n\n        # Combine human one-hot with species aux_features\n        combined = torch.cat([one_hot, aux_features], dim=-1)\n\n        if combined.shape[-1] &lt; self.config.hidden_size:\n            pad = self.config.hidden_size - combined.shape[-1]\n            combined = F.pad(combined, (0, pad))\n\n        return combined\nFrom here on out things are fairly standard, a transformer model (here bert but could be anything really) is initialized to learn the relationship between adjacent tokens using a masked language model training regime. In the code below the custom embedding are introduced into the masked language model (ModernBert in this case) while the encoder part of the model (the core part of the model that learns the relation between adjacent tokens) remain unchanged.\n# --------------------------------\n# 6. GPNBERTMaskedLM\n# --------------------------------\n\nclass GPNBERTMaskedLM(nn.Module):\n    def __init__(self, config, n_species):\n        super().__init__()\n        self.config = config\n        self.n_species = n_species\n        self.vocab_size = 5  # A, T, G, C, -\n        self.species_feature_size = n_species * self.vocab_size\n\n        self.encoder = ModernBertModel(config)  # Directly initialize the transformer backbone\n        self.cls = nn.Linear(config.hidden_size, config.vocab_size)\n        self.embedding = GPNEmbedding(config, n_species)\n\n    def forward(self, input_ids=None, aux_features=None, labels=None, **kwargs):\n        embeddings = self.embedding(input_ids, aux_features)\n\n        # Only pass valid args to the encoder\n        encoder_kwargs = {k: v for k, v in kwargs.items() if k in {\"attention_mask\", \"position_ids\", \"head_mask\"}}\n\n        outputs = self.encoder(inputs_embeds=embeddings, **encoder_kwargs)\n\n        sequence_output = outputs.last_hidden_state\n        prediction_scores = self.cls(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n            loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        return MaskedLMOutput(\n            loss=loss,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#what-are-we-masking",
    "href": "Chapter4_DNA.html#what-are-we-masking",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.4 What are we masking?",
    "text": "4.4 What are we masking?\nI promised dwe’d have to deeply consider what we count as prediction, and that’s going to have to happen right now. In Chapter 2 we trained a DNA language model, and in Chapter 3 we saw how well idt did, and did not predict specific features. In Figure 3.3 you saw the model predicts the true bases in the DRD2 gene with about 40%. What if I told you I can predict the bases in the human reference genome with &gt; 95% probability with a “model” or based on. supervised model. TO do so I’d just pick the consensus base across other species! Humand DNA and chimpanzee DNA are &gt; 90% identical, so are human and mouse genomes are remarkably similar (85% I think). In Figure 4.3 (from (“Initial Sequencing and Comparative Analysis of the Mouse Genome” 2002)) we see the human sequences that are preserved well in the mouse genome, and they cover a staggering portion of it. This means we can “beat” base predictions made by our previous model by a mile, simply by piking the base the is most frequent across evolutionary history.\n\n\n\nFigure 4.3 The mouse genome, with in it sections of the human genome (color codes) that are largely preserved across evolution (figure 3 in (“Initial Sequencing and Comparative Analysis of the Mouse Genome” 2002))\n\n\nIn training our GPNBert model with auxiliary sequences we train by masking the human base only (following (Benegas et al. 2023)). This very obviously, and dramatically, improves base prediction, and does so very quickly. after a few hundred iterations (trained on about 5000 genes) the model learns that it should just assign the base most often found in other species. But as the evaluations the authors of the original GNP-MSA paper make clear, eventually the model learns more than that. The model outperforms just picking the consensus base across species. It is a better predictor of the allele frequency in humans then just picking the allele frequency across species ans an estimate of the allele frequency within human.\n\n4.4.1 Ablation studies\nThe authors use ablation studies to proof their model learns more then just the base history, and still learns from the base in its sequence. Anyone who wants to train language models should come to understand ablation studies so this is a good a point as any to discuss theirs and run our own!",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#recap-of-our-approach",
    "href": "Chapter4_DNA.html#recap-of-our-approach",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.5 Recap of Our Approach",
    "text": "4.5 Recap of Our Approach\nIn Chapter 2, we trained a vanilla BERT on DNA sequences alone — treating DNA as just another language. That model only had access to the human sequence, with no evolutionary context.\nIn this chapter, we’ve re-imagined that process. Instead of treating A, T, G, C, - as abstract symbols, leaving the model entirely unsupervised when picking embedding, we inject evolutionary history directly into the embedding directly. This allows our model to:\n\nUse the aligned species data as a rich evolutionary prior.\nStill leverage transformers for learning sequence motifs.\nPredict masked human bases using both local sequence and cross-species evolutionary patterns.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#emperical-ideas-to-explore",
    "href": "Chapter4_DNA.html#emperical-ideas-to-explore",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.6 Emperical ideas to explore",
    "text": "4.6 Emperical ideas to explore\nObviously embedding",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#preview-of-chapter-5",
    "href": "Chapter4_DNA.html#preview-of-chapter-5",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.7 Preview of Chapter 5",
    "text": "4.7 Preview of Chapter 5\nIn Chapter 5, we will put these two models — Vanilla BERT and GPN-BERT — to the test. We will evaluate their performance on:\n\nPredicting masked bases (MLM accuracy).\nIdentifying regulatory elements.\nPredicting the functional impact of mutations.\n\nThis head-to-head comparison will highlight the strengths and weaknesses of each approach and show the value of embedding evolutionary context directly into genomic language models.\n\n\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2023. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” http://dx.doi.org/10.1101/2023.10.10.561776.\n\n\n“Initial Sequencing and Comparative Analysis of the Mouse Genome.” 2002. Nature 420 (6915): 520–62. https://doi.org/10.1038/nature01262.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nLupo, Umberto, Damiano Sgarbossa, and Anne-Florence Bitbol. 2022. “Protein Language Models Trained on Multiple Sequence Alignments Learn Phylogenetic Relationships.” Nature Communications 13 (1). https://doi.org/10.1038/s41467-022-34032-y.",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter5_DNA.html",
    "href": "Chapter5_DNA.html",
    "title": "5  Weaving Together Models",
    "section": "",
    "text": "Abstract\n\n\n\nIn Chapter 5, we will put these two models — Vanilla BERT and GPN-MSA-BERT — to the test. We will evaluate their performance on:\n\nPredicting masked bases (MLM accuracy).\nIdentifying regulatory elements.\nPredicting the functional impact of mutations.\n\nThis head-to-head comparison will highlight the strengths and weaknesses of each approach. Finally we’ll build a hybrid model, in which the DNA sequence is model is trained in evolutionary context and in its sequence context and the results are blended, a model architecture that is a little like alphafold, the Nobel winning protein language model developed at google deepmind.\nAll scripts for this chapter are found here: https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/Chapter_4",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Weaving Together Models</span>"
    ]
  },
  {
    "objectID": "Chapter6_DNA.html",
    "href": "Chapter6_DNA.html",
    "title": "6  A review of current DNA language models",
    "section": "",
    "text": "6.1 The state of the art",
    "crumbs": [
      "DNA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A review of current DNA language models</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Benegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S.\nSong. 2023. “GPN-MSA: An Alignment-Based DNA Language Model for\nGenome-Wide Variant Effect Prediction.” http://dx.doi.org/10.1101/2023.10.10.561776.\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. “DNA\nLanguage Models Are Powerful Predictors of Genome-Wide Variant\nEffects.” Proceedings of the National Academy of\nSciences 120 (44). https://doi.org/10.1073/pnas.2311219120.\n\n\nBenegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun\nS. Song. 2025. “Genomic Language Models: Opportunities and\nChallenges.” Trends in Genetics, January. https://doi.org/10.1016/j.tig.2024.11.013.\n\n\n“Initial Sequencing and Comparative Analysis of the Mouse\nGenome.” 2002. Nature 420 (6915): 520–62. https://doi.org/10.1038/nature01262.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael\nFigurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021.\n“Highly Accurate Protein Structure Prediction with\nAlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nLupo, Umberto, Damiano Sgarbossa, and Anne-Florence Bitbol. 2022.\n“Protein Language Models Trained on Multiple Sequence Alignments\nLearn Phylogenetic Relationships.” Nature Communications\n13 (1). https://doi.org/10.1038/s41467-022-34032-y.",
    "crumbs": [
      "References"
    ]
  }
]