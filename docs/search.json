[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sequence Language Models & Deep Learning in Genomics",
    "section": "",
    "text": "Preface\nThese are my study notes on training DNA/RNA/Protein sequence models and other biological deeplearning models, over the last ~year, year and a half. They are arranged in what is a public alpha/draft of a book/study guide on biological language models, deep-learning models or sequence models for academics and trainees who missed the boat on AI and want to catch up.\nThe text/book is intended for people who want to on-ramp into the field and begin to actually train large DNA/Biological language models,, but also for their PIs—anxious aging millennial or Gen-X’ers who want to be able to understand the next generation of computational genomics tools that are about to wash over us all.\nAs I come at this from my background in statistical genetics/psychiatric genetics/epidemiology, I’ll clearly be interest in how mutations or variants I might identify in GWAS alter proteins function, and how protein models can help me asses/model those consequences. In later chapters (currently being drafted) I tackle issues more closely related to statistical genetics, like finemapping and how (language) models of protein complexes might enable prioritization of long range co-evolution between amino-acids and help us prioritize potential GxG interactions between distal variants, or help study biological mechanisms for trans-QTLs.\nAt all times, I’ll try to add some biological context (though I am no biologist!) for people who have an ML background but no college bio experience, and I’ll try to add context on ML concepts (though I am not an AI engineer!) for those with a bio background but limited experience with language models/deeplearning.\nIf you have a formal training in Machine Learnign and a background in biology or genomics, this book might be too basic for you, and in fact I’d love it if you could flag any errors or omissions (which I am sure exist), each page has a comments section or you can email me at: m.g.nivard (at) bristol (dot) ac.uk",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-times-we-live-in",
    "href": "index.html#the-times-we-live-in",
    "title": "Sequence Language Models & Deep Learning in Genomics",
    "section": "The Times We Live In",
    "text": "The Times We Live In\nMore than natural language models, biological/sequence language models rely heavily on NIH-funded databases, datasets, resources, and scientists. The data we train on was bought and paid for by taxpayers all over the globe. The Human Genome Project was to a great extent funded, directed, and conceived under the auspices of the US federal government, under both Democratic and Republican presidents. Had they not, pharmaceutical companies might have done it, and while those can be highly innovative, there would have been no space for startups, no space for Google DeepMind to come in and iterate, revolutionize, or grow biological modeling. There is no uproar over training data in biology because under the firm guidance of US federal policy, all the data sequencers generate is generally in the public domain, or accessible for those willing and able to meet ethical standards. All scientists reading this know this—should you find yourself as someone from Silicon Valley, from a well-funded startup even, take a beat and think through whether you’d stand a snowball’s chance in hell to compete if the next wave of sequence data isn’t public but generated inside Google/Microsoft/pharma. Then adjust your politics accordingly.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Sequence Language Models & Deep Learning in Genomics",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese notes are written by me, Michel Nivard, a professor of Epidemiology at the University of Bristol, and as this book is not a core output for my job, I frequently rely on LLMs to help me with grammar, spelling, review, coding, and formatting to a level that exceeds my reliance on those for publications or teaching materials for Uni courses.\nThese study notes are influenced by discussions with Robbee Wedow and Seyedeh Zahra Paylakhi, with whom I work on related projects.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Preamble1.html",
    "href": "Preamble1.html",
    "title": "What is this Book About?",
    "section": "",
    "text": "A Brief Glossary of ML Model Types\nThis book is me trying to keep up with the current state of the art in ML for biology, with an initial focus on language models, tohugh more acurately all models we discuss are “attention” models. The fundamental goal of these models is to learn the dependencies (the joint probability distribution) between elements in a sequence (a DNA or protein sequences) or a 2D field (an image or a protein contact map) or 3D structure (the full protein structure for example).\nWhen studying the latest hyped tools, it’s good to resist the temptation to be awed by the models. Some are great, and it can feel magical to see an unsupervised model pick up important biological signals and be very predictive by only processing sequence data! However, the current state of the art in biology, genetics especially, is remarkable—we know a lot about the genome, about how DNA is transcribed into RNA and then proteins, which proteins are conserved across evolution (i.e., essential for all life). So throughout, we have to keep in mind that in some domains, while it may feel (and actually be) remarkable that a language model picks up fundamental biology just from processing data, it might not be state of the art or even close to it.\nSupervised machine learning is a type of machine learning where a model learns to make predictions based on examples that come with known answers, or “labels.” In biology, this could mean training a model to predict whether a DNA sequence comes from a healthy or diseased tissue, or identifying which species a DNA sample belongs to. The model sees many examples where the input (the DNA sequence) is paired with the correct output (the label, like “healthy” or “diseased”), and learns to find patterns that link the two. Supervised learning is very powerful when we have lots of high-quality labeled data, but in biology, obtaining these labels can be expensive, time-consuming, and sometimes even impossible if we don’t know the “right answer” in advance.\nUnsupervised machine learning, in contrast, is used when we don’t have labels—the model only sees the raw data and has to find patterns on its own. This is especially useful in biology when exploring large datasets where the structure isn’t fully understood, such as grouping similar cells in single-cell RNA sequencing or discovering new subtypes of proteins. In the case of biological language models, the “language” is made up of sequences like DNA, RNA, or proteins. Unsupervised models, such as transformers trained on genome sequences, learn the “grammar” and “vocabulary” of these biological molecules just by seeing lots of sequences, without being told what they mean. This allows them to uncover hidden rules of biology, like which sequences are likely to code for stable proteins or which mutations might disrupt function.\nBiological language models have become particularly important because DNA, RNA, and proteins all follow sequential, language-like patterns—just as words form sentences, nucleotides form genes, and amino acids form proteins. By training on vast amounts of biological data in an unsupervised way, these models can learn useful representations of biological sequences, even without human-provided labels. Researchers can then use these pretrained models for many downstream tasks, such as predicting gene function, identifying regulatory regions, or studying how genetic variation might affect disease—combining the power of unsupervised learning to understand biology’s “language” with supervised learning for more targeted, disease-specific predictions.\nIn some cases, the boundary between supervised and unsupervised learning is blurry—for example, in protein language models trained to predict 3D structure from amino acid sequences. These models are not given simple “labels” like “healthy” or “diseased,” but they are provided with 3D structural information that acts as an open-ended example rather than a strict classification label. The model isn’t being asked to sort sequences into a few categories, but rather to learn a very rich and flexible relationship between sequence and structure. This kind of learning—where the system uses biological context to guide its training without explicit classification tasks—occupies a middle ground between supervised and unsupervised methods, illustrating how biological complexity often resists fitting into neat ML categories. A key difference between learning labels and learning open-ended structures is that learning labels is data reduction (from complex 1D sequence to two, or a few, labels) while structure prediction is data expansion (from 1D protein sequence to 3D spatial molecular map).\nIn sequence analysis, there are also biologically-driven models that sit outside traditional machine learning entirely, or only use minimal regression or statistical modeling. For example, methods to predict whether a missense mutation (a single amino acid change) is deleterious often rely on biological theory, such as identifying mutation-depleted regions—parts of the genome or protein where harmful mutations rarely appear in healthy populations. These models leverage evolutionary conservation, functional annotations, and biochemical properties to prioritize mutations for further study, sometimes incorporating simple regression to combine different biological signals into a final score. These biologically-informed approaches are critical in genomic medicine and show how biology itself can provide a strong prior for prediction, even without extensive machine learning.",
    "crumbs": [
      "What is this Book About?"
    ]
  },
  {
    "objectID": "Preamble2.html",
    "href": "Preamble2.html",
    "title": "How to Read this Book",
    "section": "",
    "text": "Structure\nThe book is accompanied by scripts in both the R and Python programming languages. I had to make some choices—some of the biological data repositories have great integrated Perl and R packages. I wouldn’t want to force people into Perl (especially not myself!). I am more comfortable wrangling the initial data in R than in Python, so here we are.\nIf you want to code along, rest assured you can run most of this on a MacBook. Maybe you’ll need to do a training run overnight a few times. If you want a bit more performance, or do not want to your MacBook turn into a space heater for 24 hours, you can use Google Colab, a notebook environment with reasonably priced access to A100 GPUs. Training the DNABERT model we outline in Chapter 2 on 500k coding sequences from 13 species took ±6 hours on an A100 on Colab, which means that cost me ±$4 in Colab credit. In the chapters that explicitly discuss scaling trainign up we’ll cover how to set up a training run on a server with any number of high power datacentre GPUs.\nSo keeping things manageable means well be working with small models, on relatively small, or narrow datasets, at times we’ll build prototype or toy models that lack certain features purely to further our understanding of a more complex model. I cant emphasize enough that so far none of the models trained in the making of this book are state of the art models, or trained on sufficient data to be applied for actual biological understanding. I am seriously weighting, pending someone donating the compute time needed, a “lets put all the parts together” chapters at the end of the sections about DNA and Proteins where I train one, still modestly sized, state of the art model integrating lessons on data, architecture, scaling and evaluation into the kind of project that would for the basic for an academic paper, or initial prototype in the biomedical industry.\nThe GitHub repo that hosts the book will be populated with all the scripts I discuss and use. The data used to train the models, and some of the models themselves, will be hosted on Hugging Face (a repository of ML training data). I will try to make Jupyter notebooks available, though given my R background, I usually run Python from a script, or interactively in a REPL because that is what R people do.\nIf you come at this from an R background, and Python isn’t your native language, I can highly recommend using Positron as an IDE when following along with this book. It’s a VSCode derivative developed by Posit, the company that developed RStudio. Positron has tooling integrated for data science in both Python and R, and I can switch between Python and R sessions instantly!\nThere were two potential ways to structure the book. I could have structured it around elements of, or classes of, deep-learning models: attention, tokenzation, embedding, diffusion, or I could have structured it around biological themes DNA, RNA, Proteins. I opted for the second structure. but that doesn’t mean I don’t cover ML/AI topics extensively. Tokenization is discussed at length in 2  Training our first DNA Language Model, attention is covered in details in 10  Protein contact maps from attention maps, and diffusion is covered fully in 11  Integrated protein diffusion language models.\nThe book is divided into sections that deal with a specific biological modality or data type. The last chapter in each section is a review of current models of that modality or data type. It’s more common to begin a chapter reviewing what’s available out there, but given the novelty of these models, it makes sense to learn how they work before reading a review of what’s out there. There is a risk of the reader attributing insights to me simply because I describe it to you first. I’ll cite my sources as if this is a peer-reviewed article, and you should assume most models we build together are directly, or indirectly, influenced by the literature. I also ask you do not cite this book other than for novel content, or to refer to it as teaching material—please, for models, architectures, and insights, cite the underlying empirical literature.",
    "crumbs": [
      "How to Read this Book"
    ]
  },
  {
    "objectID": "Preamble2.html#structure",
    "href": "Preamble2.html#structure",
    "title": "How to Read this Book",
    "section": "",
    "text": "DNA Language Models\n1  Preparing DNA data for training covers downloading and processing (DNA) sequence data from Ensembl and uploading it to Hugging Face. 2  Training our first DNA Language Model covers training a first small DNA sequence language model; the model is a bog-standard language model meant for natural languages, simply applied to DNA. In 3  Evaluating DNA Language Models, we explore how you’d evaluate whether a DNA model is any good—is our model learning anything at all? Then in 4  Evolution-Aware Encoders, we explore evolutionary-aware encoders and how they relate to DNA language models. In 5  Comparing Models we compare the two model we trained on a number of tasks, getting a feel for comparative evaluation. If you stuck with it and get to **6  A Review of Current DNA Language Models* you are ready for a brief review of existing DNA language models.\n\n\nScaling Training\nAfter the book section on DNA models, we step back and consider scaling up model training. To train a full “production” model you’d need to scale from running things interactively on a MacBook, to a GPU in the cloud to 8 GPUs in a server. Conditional on me getting some funds and/or arranging HPC compute access I might even write about/run training on a whole rack of servers, each with 1-8 GPUs. When scaling we are confronted with a whole host of new issues around training stability and parallel compute.\n\n\nProtein Language Models\nIn 7  Proteins: from sequence to structure we discuss the advent of protein language model (initially deep learning models not language models, though they share the key attention mechanisms) and their incredible success at solving a core problem in biology: Predicting the shape of a folded protein from the protein sequence. Their success won the Google DeepMind team a Nobel Prize, rightfully so. In 8  Selecting and curating protein sequences we discuss one of the key ingredients of Google’s success: the ubiquitous (taxpayer-funded) access to protein (structure) data. Not only are sequences accessible, but there are also bi-annual drops of protein sequence and structure experiments that haven’t been shared yet, creating a perfect holdout set which ensures healthy competition, no risk of over-fitting, and a lush testing bed for protein models. Then in 9  Training our first Protein Language Model we train a small protein language model, mostly to just get acquainted with the code. Later in the chapter “putting it all together” we’ll actually train a reasonably sized protein language model on a balanced data mixture.",
    "crumbs": [
      "How to Read this Book"
    ]
  },
  {
    "objectID": "Preamble3.html",
    "href": "Preamble3.html",
    "title": "The software stack",
    "section": "",
    "text": "To do any kind of machine learning at scale, you need a machine learning library that takes care of interfacing between your high level programming language (almost always Python if you are into niche/alt things Julia or if you are really into punishing yourself R (I tried…)) and low level matrix operations on CPU/GPU.\nThese machine learning libraries facilitates a few essential things. First, these libraries offer optimal implementations of elementary mathematical operations on data matrices/tensors on CPU or GPU(sums, min, mean, products, dot products etc. etc.). Second these libraries facilitate automatic differentiation. Most libraries have the options to store/compute a gradient (1st derivative wrt some function) value for all parameters that are to be estimated.\nEstimating parameters in a model almost always boils down to moving parameters values (intelligently) such that the model does its ‘job’ better or achieves its ‘goal’. The way we define a models’ ‘goal’ is in terms of a ‘loss function’. We’re not going to go into loss functions specifically here, there are plenty resources to learn about those. The loss is generally a function of all parameters in the model. A lot of optimization (though not all) is done by gradient descent, the gradient of the parameters wrt the loss is minimized, because if all parameters have gradient 0 wrt the loss their at a (local) minimum. The brute force way of establishing the gradient for a set of parameter values (still sued in many SEM or statistical software frameworks) is to sequentially move each parameter up, then down a little step, record the change in loss. This takes 2*#-parameters evaluations of the loss function, after which all parameters move in the optimal direction and the process re-starts. A key aspect of any machine learning framework is to use automatic differentiation(Wengert 1964; Griewank 2012) to provide the partial derivatives of the loss function with respect to the individual parameters, in order to speed up optimization greatly (see autodiff on Wikipedia for a fuller explanation). the logic here is that the sign and magnitude of the derivative/gradient tells you which steps to take for optimal gradient descent, to reduce the loss. Automatic differentiation takes twice the compute as a single evaluation of the loss, regardless of the number of parameters involved.\n\n\n\n\n\n\nImportant\n\n\n\nTroughout the book callouts labeled important will highlight key concepts in ML, we don’t have separate ML chapters but the concepts are sprinkled throughout. Studying these topics until you have a conceptual grip on them will make you a better consumer/developer of biological ML models. You don’t have to immediately as they are frequently abstracted away by layers of software and convenience.\nThe key concepts to study further here are gradient decent and automatic differentiation, which are basically the building blocks of how all deep learning models are trained.\n\n\nArguably the four most popular libraries that provide a framework for automatic differentiation, GPU/CPU compute abstraction and reconfigure model elements (attention layers, dense layers, convolutional layers, recurrent networks etc etc) are PyTorch, JAX,tensorflow and keras. These libraries are still quite general, they are able to accommodate a vast universe of models. Build on top of these framework there are libraries like Huggingfaces’ transformers. Huggingface (HF), a company named after an emoji (🤗) and started to develop a chatbot for teens in 2016, at some point pivoted to being the backbone behind language and vision language models and maintain a few powerful libraries that form a top layer over PyTorch,TensorFlow and keras.\n\n\n\nFigure 1: diagram of the software stack used in this book\n\n\nWe’ll be working with transformers (and a few derivative libraries like trainer and accelerate), when we cannot us the off the shelf models we’ll implement small pieces of code in Pytorch to fit within transformers models. What transformers offers us are highly structured validated implementations of models, of training loops/code of tokenizers (we’ll get into all those). It also directly integrate with all models and dataset hosted on https://huggingface.co. Hosting on HF is free for public models and datasets, though given these can go into the terabytes one wonders how long that can last. When training a model, you can save its weights directly to their hub, from the script. This greatly reduces the friction between model training and deployment.\nUnder that layer pytorch is a python API that lets you interface with tensors, matrices, their gradients, the full graph of a model containing multiple matrices etc. Under the hood libtorch is the C++ library that then translates (and in some cases complies) those models and graphs into hardware specific languages (so CUDA for NVIDIA GPUs for example) where the actual computation happens. In this course we will work in pytorch directly but at no point will concern ourselves with the lower layers.\n\n\n\n\nGriewank, Andreas. 2012. “Who Invented the Reverse Mode of Differentiation?” In, 389–400. EMS Press. https://doi.org/10.4171/dms/6/38.\n\n\nWengert, R. E. 1964. “A Simple Automatic Derivative Evaluation Program.” Communications of the ACM 7 (8): 463–64. https://doi.org/10.1145/355586.364791.",
    "crumbs": [
      "The software stack"
    ]
  },
  {
    "objectID": "Preamble4.html",
    "href": "Preamble4.html",
    "title": "Structure of an ML model",
    "section": "",
    "text": "Matrices and Vectors in the Model\nUltimately the exact software stack, or framework. you use aren’t that important. Once you begin to grasp the models and their estimation, switching between frameworks and languages around those models becomes easier. I used to think “I code in R, I can’t work with Python” or “I use frequentist SEM models I can’t train neural networks”. While the lower level choices PyTorch or JAX, R or Python are hidden away under high level libraries in parts of this course, it is a good exercise to see the same model represented in different languages and frameworks.\nThe goal here is twofold, to see the basic structure of an ML training script and to begin to cognitively separate the model, from the language and the framework. Look trough the model specifications below by flickign trough the tabs containing the code, in each you will see the same multi-layer perceptron (MLP) across five deep learning frameworks: PyTorch, torch for R, Keras (R), Keras (Python), and JAX. The goal is to help you see one model, represented in various ways.\nOur minimal MLP has:\nWe’ll describe each matrix and bias term, then express the full model as a symbolic equation.\nThe model uses the following matrices and vectors:",
    "crumbs": [
      "Structure of an ML model"
    ]
  },
  {
    "objectID": "Preamble4.html#the-machinery-around-the-model",
    "href": "Preamble4.html#the-machinery-around-the-model",
    "title": "Structure of an ML model",
    "section": "The machinery around the model",
    "text": "The machinery around the model\nThese scripts then all contain the following structural elements related tot the model, and to the optimisation of the model given data:\nModel definition: This is where the architecture of the neural network is specified, including a forward function, which defines how inputs are transformed through layers to produce outputs.\nIoss-function and optimizer: After the model makes a prediction we need to somehow compare the prediction to the ground truth in the training data, the loss function defines how this comparison is made, and summarized into a single value. The user can also specify an optimizer, which is a mathermatical functino that determines, based on the gradients of the parameters with respect tot he loss in this and sometimes previous iterations, on how to change the parameters in order to minimize the loss.\nhaving defined the model, loss and optimizer the user specifies a training loop:\nForward pass: In this step, input data is passed through the model to compute predictions based on the current weights.\nLoss computation: The model’s predictions are compared to the true target values using a loss function that quantifies the prediction error.\nBackpropagation: The gradients of the loss with respect to each model parameter are calculated by propagating the error backward through the network.\nWeight updates: The model’s parameters are adjusted using an optimization algorithm (like stochastic gradient descent) to reduce the loss in future predictions.\n\nPyTorch (Python)torch for RKeras (R)Keras (Python)JAX\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Model\nclass MLP(nn.Module):\ndef \\_\\_init\\_\\_(self):\nsuper().\\_\\_init\\_\\_()\nself.net = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 1))\n\ndef forward(self, x):\nreturn self.net(x)\n\nmodel = MLP()\nopt = optim.SGD(model.parameters(), lr=0.01)\nloss_fn = nn.MSELoss()\n\n# Training loop\nfor x, y in data:\npred = model(x)\nloss = loss_fn(pred, y)\nopt.zero_grad()\nloss.backward()\nopt.step()\n\n\nlibrary(torch)\n\n# Model\nmlp &lt;- nn_module(\n  initialize = function() {\n    self$fc1 &lt;- nn_linear(10, 20)\n    self$fc2 &lt;- nn_linear(20, 1)\n  },\n  forward = function(x) {\n    x %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% self$fc2()\n  }\n)\n\nmodel &lt;- mlp()\nopt &lt;- optim_sgd(model$parameters, lr = 0.01)\nloss_fn &lt;- nn_mse_loss()\n\n# Training loop\nfor (batch in dataloader) {\n  x &lt;- batch$x\n  y &lt;- batch$y\n  pred &lt;- model(x)\n  loss &lt;- loss_fn(pred, y)\n  opt$zero_grad()\n  loss$backward()\n  opt$step()\n}\n\n\nlibrary(keras)\n\n# Model\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 20, activation = \"relu\", input_shape = 10) %&gt;%\n  layer_dense(units = 1)\n\nmodel %&gt;% compile(optimizer = \"sgd\", loss = \"mse\")\n\n# Training\nmodel %&gt;% fit(x_train, y_train, epochs = 10)\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Model\nmodel = keras.Sequential([\n    layers.Dense(20, activation='relu', input_shape=(10,)),\n    layers.Dense(1)\n])\nmodel.compile(optimizer='sgd', loss='mse')\n\n# Training\nmodel.fit(x_train, y_train, epochs=10)\n\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, random\n\n# Parameters\nkey = random.PRNGKey(0)\ndef init_params(key):\n    k1, k2 = random.split(key)\n    return {\n        \"W1\": random.normal(k1, (10, 20)),\n        \"b1\": jnp.zeros(20),\n        \"W2\": random.normal(k2, (20, 1)),\n        \"b2\": jnp.zeros(1)\n    }\n\ndef forward(params, x):\n    x = jnp.dot(x, params[\"W1\"]) + params[\"b1\"]\n    x = jax.nn.relu(x)\n    return jnp.dot(x, params[\"W2\"]) + params[\"b2\"]\n\ndef loss_fn(params, x, y):\n    pred = forward(params, x)\n    return jnp.mean((pred - y) ** 2)\n\n@jit\ndef train_step(params, x, y, lr=0.01):\n    grads = grad(loss_fn)(params, x, y)\n    return {k: params[k] - lr * grads[k] for k in params}\n\nparams = init_params(key)\n\n# Training loop\nfor x, y in data:\n    params = train_step(params, x, y)\n\n\n\nThe models clearly are the same, and the different frameworks/languages vary in their verbosity, in keras in R is only a few lines, many common operations such as mean square error loss (mse) are baked in, while in JAX things are defined more manually. I feel very comfortable with PyTorch myself and it does have a wide user base in the language/sequence/image deep-learning communities which means there is a lot of great code available. Switching form one to the other will make you stumble, but its not as insurmountable as many people believe.\nAs you build bigger and bigger models this just means the model section of the code grows. Usually you’d also want to build in structures to read & validate training data, process it per batch, as well as code to save an shared trained models. Those creature comforts are provided by libraries like transformers which do then tend to abstract away the exact representatino of the model.",
    "crumbs": [
      "Structure of an ML model"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html",
    "href": "Chapter1_DNA.html",
    "title": "1  Preparing DNA data for training",
    "section": "",
    "text": "1.1 Garbage in garbage out\nHigh-quality data is essential for training effective machine learning models. In natural language processing (NLP), researchers invest significant effort in curating clean datasets from raw internet text. For example, a dataset like ‘fineweb-edu’ contains English text that is of very high quality. Models trained on fineweb-edu (and similar high-quality datasets) will improve much faster than the equivalent model trained on other less carefully processed and evaluated datasets.\nThose with experience in genetics/genomics will know most of what I am about to explain when it comes to the sources of genetic data and the file formats these come in. Those with an ML background will be very familiar with Huggingface and the amazing data/model/training integration their libraries offer. Assembling high-quality genomics datasets for language modeling requires familiarity with both. When working with DNA, RNA, and protein sequences, the authoritative sources of data are specialized databases and infrastructures rather than data scraped from the internet. If you want to learn to train DNA/RNA/Protein-based language models, we must learn how, and where, to retrieve data and convert it into a structured format.\nIn this tutorial, we will begin by collecting coding DNA sequences (CDS) from the human genome using the Ensembl database via BioMart. The goal is to store these sequences in a format suitable for training a DNA-based language model. We will then upload the processed data to Huggingface, a platform for hosting datasets and models for machine learning tasks.\nThis chapter will introduce key biological and computational concepts, ensuring that both biology newcomers and those unfamiliar with language modeling tools can follow along.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#garbage-in-garbage-out",
    "href": "Chapter1_DNA.html#garbage-in-garbage-out",
    "title": "1  Preparing DNA data for training",
    "section": "",
    "text": "Figure 1: Relative training efficiency using a high-quality dataset like fineweb-edu and other massive datasets of English language texts. Image obtained from: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#understanding-ensembl-and-biomart",
    "href": "Chapter1_DNA.html#understanding-ensembl-and-biomart",
    "title": "1  Preparing DNA data for training",
    "section": "1.2 Understanding Ensembl and BioMart",
    "text": "1.2 Understanding Ensembl and BioMart\nFortunately for us, there is decades of work cleaning up genomic data, and we can just go and get it from US government-funded websites, where it is deposited by the global scientific community. Ensembl is a genomic database that provides extensive annotations for various species, including humans. It offers access to gene sequences, transcript annotations, and protein-coding information. One of its most powerful tools is BioMart, a flexible data retrieval system that allows users to download specific genomic datasets easily.\nIf we want to work with the data in a language model, it’s efficient to store it in a format that is tailored for machine learning libraries. Unlike text-based NLP datasets, genomic data is structured differently and must be properly formatted before use in a model.\n\n1.2.1 What Are FASTA Files?\nA FASTA file is a simple text-based format used for storing biological sequences. Each entry in a FASTA file consists of: 1. A header line (starting with &gt;), which contains metadata such as gene IDs and chromosome locations. 2. A sequence line, which contains the nucleotide or protein sequence.\nThere is a very comprehensive Wikipedia entry on the FASTA format: “Sequences may be protein sequences or nucleic acid sequences, and they can contain gaps or alignment characters (see sequence alignment). Sequences are expected to be represented in the standard IUB/IUPAC amino acid and nucleic acid codes, with these exceptions: lower-case letters are accepted and are mapped into upper-case; a single hyphen or dash can be used to represent a gap character; and in amino acid sequences, U and * are acceptable letters (see below). Numerical digits are not allowed but are used in some databases to indicate the position in the sequence.” (source: https://en.wikipedia.org/wiki/FASTA_format)\nThe nucleic-acid code specification of FASTA:\n\n\n\nNucleic Acid Code\nMeaning\nMnemonic\n\n\n\n\nA\nA\nAdenine\n\n\nC\nC\nCytosine\n\n\nG\nG\nGuanine\n\n\nT\nT\nThymine\n\n\nU\nU\nUracil\n\n\n(i)\ni\ninosine (non-standard)\n\n\nR\nA or G (I)\npuRine\n\n\nY\nC, T or U\npYrimidines\n\n\nK\nG, T or U\nbases which are Ketones\n\n\nM\nA or C\nbases with aMino groups\n\n\nS\nC or G\nStrong interaction\n\n\nW\nA, T or U\nWeak interaction\n\n\nB\nnot A (i.e. C, G, T or U)\nB comes after A\n\n\nD\nnot C (i.e. A, G, T or U)\nD comes after C\n\n\nH\nnot G (i.e., A, C, T or U)\nH comes after G\n\n\nV\nneither T nor U (i.e. A, C or G)\nV comes after U\n\n\nN\nA C G T U\nNucleic acid\n\n\n-\ngap of indeterminate length\n\n\n\n\nThe amino acid codes specification (22 amino acids and 3 special codes) of FASTA:\n\n\n\nAmino Acid Code\nMeaning\n\n\n\n\nA\nAlanine\n\n\nB\nAspartic acid (D) or Asparagine (N)\n\n\nC\nCysteine\n\n\nD\nAspartic acid\n\n\nE\nGlutamic acid\n\n\nF\nPhenylalanine\n\n\nG\nGlycine\n\n\nH\nHistidine\n\n\nI\nIsoleucine\n\n\nJ\nLeucine (L) or Isoleucine (I)\n\n\nK\nLysine\n\n\nL\nLeucine\n\n\nM\nMethionine/Start codon\n\n\nN\nAsparagine\n\n\nO\nPyrrolysine (rare)\n\n\nP\nProline\n\n\nQ\nGlutamine\n\n\nR\nArginine\n\n\nS\nSerine\n\n\nT\nThreonine\n\n\nU\nSelenocysteine (rare)\n\n\nV\nValine\n\n\nW\nTryptophan\n\n\nY\nTyrosine\n\n\nZ\nGlutamic acid (E) or Glutamine (Q)\n\n\nX\nany\n\n\n*\ntranslation stop\n\n\n-\ngap of indeterminate length\n\n\n\nYou’ll notice the FASTA format has a well-defined structure, and it could be leveraged to build a complete tokenizer. For now, though, our 4-character (+6 special characters) tokenizer will have to do.\n\n\n1.2.2 Why Focus on Coding DNA Sequences (CDS)?\nIn the example, we retrieve the human coding DNA sequences (CDS), which represent the DNA sequence of protein-coding regions of genes.\nWhile our ultimate goal is to model the entire human genome—and potentially multiple genomes across species or individuals—such tasks require significant computational resources. The tutorials in this book are designed to be accessible, running efficiently on a MacBook or Google Colab. Therefore, we focus on CDS, which are highly structured DNA sequences within genes, that are directly transcribed into RNA which is in turn translated into proteins. The table below contains the direct translation from 3-letter DNA sequences to amino acids (which are the building blocks of proteins).\n\n\n\nThe Genetic code to translate codins (3-letter DNA sequences) to amino-acids that are in turn the building blocks of proteins (source: https://a-little-book-of-r-for-bioinformatics.readthedocs.io/en/latest/_images/P7_image1.png)\n\n\nIn contrast, much of the rest of the genome consists of regulatory regions, which are more complex and less structured. CDS sequences provide a strong foundation for a small DNA-based language model because they contain well-defined biological structure, making them a logical starting point before expanding to more complex genomic elements.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#why-upload-dna-data-to-hugging-face",
    "href": "Chapter1_DNA.html#why-upload-dna-data-to-hugging-face",
    "title": "1  Preparing DNA data for training",
    "section": "1.3 Why Upload DNA Data to Hugging Face?",
    "text": "1.3 Why Upload DNA Data to Hugging Face?\nHugging Face provides a robust ecosystem for hosting and sharing datasets, particularly for machine learning applications. Some key advantages include: - Easy accessibility: Researchers and models can easily retrieve datasets. - Standardized format: Datasets are structured for seamless integration with deep learning frameworks. - Direct integration with Hugging Face tools: The data on the Hugging Face Hub integrates seamlessly with their Transformers and Trainer Python libraries, during training you can can stream data directly from the hub which means you dont have to wait for a massive download. - Version control and updates: Data is hosted on reporsitories, and can be refined and expanded over time.\nBy storing our dataset on Hugging Face, we can facilitate efficient training and collaboration.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#the-script-downloading-and-formatting-human-cds-data",
    "href": "Chapter1_DNA.html#the-script-downloading-and-formatting-human-cds-data",
    "title": "1  Preparing DNA data for training",
    "section": "1.4 The Script: Downloading and Formatting Human CDS Data",
    "text": "1.4 The Script: Downloading and Formatting Human CDS Data\nBelow is the R script that downloads human CDS from Ensembl using BioMart, extracts metadata, and saves the processed data into a CSV file. The package we use, biomartr, isn’t the official R package, but it’s a great option! It has very extensive documentation, so if you want to download other sequences in the future, make sure to start here: https://docs.ropensci.org/biomartr/\n# Install necessary packages\ninstall.packages(\"biomartr\", dependencies = TRUE)\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n  install.packages(\"BiocManager\")\n}\n\n# Load required libraries\nlibrary(Biostrings)\nlibrary(biomartr)\n\n# Download the human CDS dataset from Ensembl\nHS.cds.ensembl &lt;- getCDS(db = \"ensembl\", organism = \"Homo sapiens\", path = file.path(\"_ncbi_downloads\",\"CDS\"))\n\n# Read CDS data as a Biostrings object\nHuman_CDS &lt;- read_cds(file = HS.cds.ensembl, obj.type = \"Biostrings\")\n\n# Extract headers and sequences\nheaders &lt;- names(Human_CDS)\nsequences &lt;- as.character(Human_CDS)\n\n# Function to extract metadata from headers\nextract_metadata &lt;- function(header) {\n  transcript_id &lt;- sub(\"^&gt;([^ ]+).*\", \"\\\\1\", header)\n  chromosome &lt;- sub(\".*chromosome:([^ ]+).*\", \"\\\\1\", header)\n  start &lt;- sub(\".*chromosome:[^:]+:([^:]+).*\", \"\\\\1\", header)\n  end &lt;- sub(\".*chromosome:[^:]+:[^:]+:([^:]+).*\", \"\\\\1\", header)\n  strand &lt;- sub(\".*chromosome:[^:]+:[^:]+:[^:]+:([^ ]+).*\", \"\\\\1\", header)\n  gene_id &lt;- sub(\".*gene:([^ ]+).*\", \"\\\\1\", header)\n  gene_biotype &lt;- sub(\".*gene_biotype:([^ ]+).*\", \"\\\\1\", header)\n  transcript_biotype &lt;- sub(\".*transcript_biotype:([^ ]+).*\", \"\\\\1\", header)\n  gene_symbol &lt;- sub(\".*gene_symbol:([^ ]+).*\", \"\\\\1\", header)\n  description &lt;- sub(\".*description:(.*)\", \"\\\\1\", header)\n\n  list(\n    transcript_id = transcript_id,\n    chromosome = chromosome,\n    start = start,\n    end = end,\n    strand = strand,\n    gene_id = gene_id,\n    gene_biotype = gene_biotype,\n    transcript_biotype = transcript_biotype,\n    gene_symbol = gene_symbol,\n    description = description\n  )\n}\n\n# Extract metadata from headers\nmetadata_list &lt;- lapply(headers, extract_metadata)\nmetadata_df &lt;- do.call(rbind, lapply(metadata_list, as.data.frame))\nmetadata_df$sequence &lt;- sequences\n\n# Save data to CSV\nwrite.csv(metadata_df, \"genome_sequences.csv\", row.names = FALSE, quote = TRUE)\n\n# Print sample data\nhead(metadata_df)\nYou can run the script yourself, but I have also gone ahead and uploaded this particular dataset to Huggingface: https://huggingface.co/datasets/MichelNivard/Human-genome-CDS-GRCh38",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter1_DNA.html#summary",
    "href": "Chapter1_DNA.html#summary",
    "title": "1  Preparing DNA data for training",
    "section": "1.5 Summary",
    "text": "1.5 Summary\nIn this chapter, we: 1. Introduced Ensembl and BioMart as tools for retrieving genomic data. Explained FASTA files and human CDS, which form the core of our dataset. 2. Discussed the advantages of uploading datasets to Hugging Face, emphasizing its integration with Transformers and Trainer python libraries. 3. Provided an R script to download, process, and store human CDS in a structured format.\nIn the next chapter, we will explore preprocessing techniques like tokenization and strategies for encoding DNA sequences into a format suitable for training a deep learning language model, and we use Huggingface Transformers and Trainer library to train our first little DNA language model!",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing DNA data for training</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html",
    "href": "Chapter2_DNA.html",
    "title": "2  Training our first DNA Language Model",
    "section": "",
    "text": "2.1 Introduction\nNow that we have collected and formatted a DNA dataset in Chapter 1, we can proceed to train a language model using the (Modern)BERT model architecture. In this chapter, we will walk through the process of tokenizing DNA sequences, configuring a BERT model, and training it using the Masked Language Modeling (MLM) objective.\nWe will cover the utility and rationale behind DNA language models, and we’ll dive into the key concepts behind tokenization. Then we’ll discuss key parts of the architecture the BERT model, and the logic of masked language model(MLM) training, before diving into the Python script that trains the actual model. Note that while we briefly discuss key elements of the model architecture like attention we don’t get into the nitty gritty just yet to make sure the chapters aren’t to dense, we’ll discuss individual parts fof the transformer model architecture in far greater detail in later chapters.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html#why-would-we-train-dna-language-models",
    "href": "Chapter2_DNA.html#why-would-we-train-dna-language-models",
    "title": "2  Training our first DNA Language Model",
    "section": "2.2 Why would we train DNA language models?",
    "text": "2.2 Why would we train DNA language models?\nFor a full review of the utility of language models, you should dig into the literature. I can recommend (Benegas et al. 2025) for example. Genomic language models (gLMs) apply AI techniques to DNA sequences, enabling breakthroughs in variant effect prediction, sequence design, and genomic analysis. After we learn about DNA language model we’ll briefly review the current offerings available in Chapter 6.\nLike larger language models like ChatGPT, DNA language models (DNA-LM) have “emergent properties”. If you train an DNA-LM, or a genomic Language models (gLM). which strictly speaking a slightly more general class of models than DNA-LMs, so also RNA or protein LMs, on the reference genome sequence of humans and various other species, then that model is able to detect damaging mutations. It can do so without ever being trained on mutations explicitly as damaging mutations are very unlikely to occur across the many references genomes on which the model is trained(Benegas, Batra, and Song 2023). To assess functional constraints, a widely used metric is the log-likelihood ratio (LLR), the ratio of log likelihoods of the various alleles given the model. This measures the probability of a nucleotide variant appearing in a given context, with lower probabilities indicating potential deleterious effects. This application will be one of the examples I use throughout, simply because my experience in genetics aligns with it.\nAnother key application are types of “transfer learning”, where pre-trained DNA-LMs improve predictions of gene expression, regulatory sequence structure, or chromatin accessibility of a section of DNA in a specific tissue. However, training effective models is difficult due to the vast, complex, and often non-functional nature of genomes. Unlike protein models, DNA-LMs struggle with limited genomic diversity in training data and require sophisticated benchmarks for evaluation.\nCurrent state of the art models(Nguyen et al. 2024) are trained on hundreds of thousands of genomes and focus on improving long-range genomic interactions (i.e. are able to generate realistic genome segments of &gt; 1 million bases long), integrating multi-modal biological data (are effectively joint DNA/RNA/Protein models), and refining sequence design for practical applications. DNA-LM or gLMs hold great promise for revolutionizing genome research, advancing genetic disease understanding, and enabling synthetic biology innovations.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html#understanding-tokenization",
    "href": "Chapter2_DNA.html#understanding-tokenization",
    "title": "2  Training our first DNA Language Model",
    "section": "2.3 Understanding Tokenization",
    "text": "2.3 Understanding Tokenization\n\n2.3.1 What is a Tokenizer?\nA tokenizer is a fundamental component of any language model. Language models are essentially large multinomial models that predict the next token in a sequence based on previous tokens, or predict a masked token using the surrounding tokens. Since machine learning models operate on numerical representations, tokenized text must be converted into integers, which serve as indices for lookup in a vocabulary.\nThese integers, however, have no inherent numeric value—they simply act as categorical labels that the model learns to associate with semantic meaning. For example, if we tokenize the sentence:\n\n“The quick brown fox jumps over the lazy dog”\n\nat the word level, we might obtain a numerical sequence like:\n\n[4, 123, 678, 89, 245, 983, 56, 4564]\n\nwhere each number corresponds to a word based on a pre-defined tokenization dictionary, such as:\n{\"the\": 4, \"quick\": 123, \"brown\": 678, \"fox\": 89, \"jumps\": 245, \"over\": 983, \"lazy\": 56, \"dog\": 4564}\nSimilarly, for DNA sequences, each nucleotide (A, T, C, G) is assigned a unique integer ID.\n\n\n2.3.2 Our DNA Tokenizer\nOur tokenizer uses a character-level approach, where each nucleotide is assigned a unique integer ID. Special tokens are also included for various purposes:\n\n[UNK] (unknown token)\n[PAD] (padding token for equal-length sequences)\n[CLS] (classification token, useful for downstream tasks)\n[SEP] (separator token, used in tasks like sequence-pair classification)\n[MASK] (used for masked language modeling training)\n\nPython Code:\nimport torch\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.pre_tokenizers import Split\nfrom transformers import PreTrainedTokenizerFast\n\n# --------------------------------\n# 1. DNA Tokenizer with Full FASTA Nucleic Acid Code\n# --------------------------------\n\n# Define vocabulary to include all FASTA nucleotides and symbols\ndna_vocab = {\n    \"A\": 0, \"T\": 1, \"C\": 2, \"G\": 3, \"N\": 4, \"U\": 5, \"i\": 6,  # Standard bases + Inosine\n    \"R\": 7, \"Y\": 8, \"K\": 9, \"M\": 10, \"S\": 11, \"W\": 12,  # Ambiguous bases\n    \"B\": 13, \"D\": 14, \"H\": 15, \"V\": 16,  # More ambiguity codes\n    \"-\": 17,  # Gap character\n    \"[UNK]\": 18, \"[PAD]\": 19, \"[CLS]\": 20, \"[SEP]\": 21, \"[MASK]\": 22\n}\n\n# Create tokenizer\ntokenizer = Tokenizer(WordLevel(vocab=dna_vocab, unk_token=\"[UNK]\"))\ntokenizer.pre_tokenizer = Split(\"\", \"isolated\")  # Character-level splitting\n\n# Convert to Hugging Face-compatible tokenizer\nhf_tokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\"\n)\n\n\n2.3.3 Other Tokenization Strategies for DNA, RNA, and Proteins\nWhile character-level tokenization is effective, other tokenization approaches can offer different tradeoffs:\n\n2.3.3.1 Byte Pair Encoding (BPE)\nBPE is widely used in NLP and allows frequently occurring patterns to be merged into subword units. While BPE could be useful for repetitive genome sequences, it may not capture biologically meaningful units.\n\n\n2.3.3.2 K-mer Tokenization\nK-mer tokenization groups nucleotides into fixed-length substrings (e.g., 3-mers like “ATG”). This approach retains local sequence structure but can lead to a large vocabulary size.\n\n\n2.3.3.3 Tiktoken and Similar Models\nSome modern tokenization methods, such as Tiktoken, optimize speed and efficiency by precomputing merges. These are often optimized for large-scale NLP tasks but could be adapted for biological sequences.\nChoosing the best tokenizer depends on the specific modeling task. For example, RNA secondary structures or protein folding models might benefit from more complex tokenization strategies.\nSource: RPubs Tokenization Review",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html#loading-and-tokenizing-the-dna-dataset",
    "href": "Chapter2_DNA.html#loading-and-tokenizing-the-dna-dataset",
    "title": "2  Training our first DNA Language Model",
    "section": "2.4 Loading and Tokenizing the DNA Dataset",
    "text": "2.4 Loading and Tokenizing the DNA Dataset\n\n2.4.1 Understanding the Dataset\nWe will use a pre-existing dataset, Human-genome-CDS-GRCh38, which contains coding sequences from the human genome.\n\n\n2.4.2 Tokenizing the Dataset\nTo prepare the dataset for training, we must apply the tokenizer to each sequence while ensuring:\n\nSequences are truncated or padded to a fixed length (512 tokens)\nUnwanted columns are removed\n\nPython Code:\nfrom datasets import load_dataset\n\ndataset_name = \"MichelNivard/Human-genome-CDS-GRCh38\"\ndataset = load_dataset(dataset_name)\n\ncolumn_name = \"sequence\"\n\ndef tokenize_function(examples):\n    return hf_tokenizer(examples[column_name], truncation=True, padding=\"max_length\", max_length=512)\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[column_name])\n\n\n2.4.3 Saving and Preparing the Dataset for Training\nOnce tokenized, we save the dataset for efficient access during training.\nPython Code:\ntokenized_dataset.save_to_disk(\"tokenized_dna_dataset\")",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter2_DNA.html#understanding-bert-and-masked-language-modeling-mlm",
    "href": "Chapter2_DNA.html#understanding-bert-and-masked-language-modeling-mlm",
    "title": "2  Training our first DNA Language Model",
    "section": "2.5 Understanding BERT and Masked Language Modeling (MLM)",
    "text": "2.5 Understanding BERT and Masked Language Modeling (MLM)\n\n2.5.1 What is BERT?\nBERT (Bidirectional Encoder Representations from Transformers) is a powerful transformer-based language model. Unlike traditional left-to-right models, BERT learns bidirectional context, allowing it to understand sequences more effectively. for genomics, bidirectional models have proven more effective, while for natural language it appears auto-regressive “next work prediction” models appear most effective.\nReturning to our earlier example sentence:\n\n“The quick brown fox jumps over the lazy dog”\n\nBERT does not process words one at a time but instead considers the entire sequence simultaneously. This bidirectional approach allows BERT to infer missing words based on context from both directions. Te inference is effectively a predicting of a specific token, in genomics models a base or amino-acid, and since the predicting results in a probability for each possible token, this fairly naturally translates into predicting the probability of a mutation, or th probability of the presence of a regulatory sequence motif near a specific gene.\n\n\n2.5.2 What is Masked Language Modeling (MLM)?\nMLM is a self-supervised learning objective where the model learns by predicting missing tokens in a sequence. During training:\n\nSome tokens are randomly replaced with [MASK]\nThe model must predict the original token based on surrounding context\n\nFor example, if we mask the word “fox” in our sentence:\n\n“The quick brown [MASK] jumps over the lazy dog”\n\nBERT will analyze the remaining words and attempt to predict “fox.”\nThis technique enables BERT to learn useful representations without requiring labeled data.\n\n\n2.5.3 Understanding the model: Transformer Layers, Attention Heads, and Hidden Size\nWe’ll briefly have to discuss the general architecture of the model, in bold the key elements of the model, in bold and italic parameters we get to set to determine the size of the model.\nA transformer model, like those used for DNA, RNA, or protein sequences, starts with an embedding layer, which plays a critical role in converting raw tokens — such as individual nucleotides (A, T, C, G) or amino acids — into numerical vectors that the model can process. Each token is mapped to a high-dimensional vector that captures some initial information about its identity and, in more advanced models, even its biochemical properties. This embedding layer acts as the interface between the raw sequence data and the deeper transformer layers, ensuring that the model works with continuous mathematical representations rather than raw symbolic letters.\n\n\n\n\n\n\nImportant\n\n\n\nThe embedding layers design is intimately related to the nature of the data, here we simply use a standard BERT embedding layer but in Chapter 4 & 5 we’ll dive deep into researchers efforts to design embedding layers for DNA specifically.\n\n\nA transformer layer consists of two key components: a self-attention mechanism (discussed in great detail Chapter 10 ) and a feed-forward neural network. The self-attention mechanism allows the model to dynamically weigh the importance of every other token in the sequence when predicting a given token, helping it learn relationships across different parts of the sequence — whether those are between neighboring amino acids or between distant regulatory elements in a long DNA strand. After the self-attention step, the feed-forward neural network processes each token’s representation independently, applying a small multi-layer perceptron (MLP) to transform the token’s internal representation. This helps the model refine and enrich the learned features, capturing nonlinear combinations of the attention-derived information. While self-attention captures interactions across the sequence, the feed-forward layer focuses on how to represent each token itself in a biologically meaningful way, helping the model identify local biochemical properties, sequence motifs, or structural preferences. The number of transformer layers (num_hidden_layers) determines how deep the model is, with more layers giving the model more capacity to learn complex biological relationships, but also increasing training time and data requirements.\nWithin each transformer layer, there are multiple attention heads (num_attention_heads), each focusing on different types of relationships within the data. In a natural language example, one attention head might capture subject-verb relationships, while another tracks adjective-noun pairs. In biological sequences, one attention head might learn to link binding motifs in promoters to transcription start sites, while another might focus on co-evolving residues in proteins that contribute to structural stability. The hidden size (hidden_size) refers to the dimensionality of these internal vector representations, defining how much information the model can store at each position. Larger hidden sizes allow the model to capture richer biological context, but they also increase computational cost. By combining deep transformer stacks, multiple attention heads, and flexible embeddings, biological language models can develop a powerful and nuanced understanding of biological sequences, helping researchers uncover new regulatory elements, predict protein folding, or study the effects of mutations.\n\n\n2.5.4 Defining the BERT Model for DNA Sequences\nWhile the “quick brown fox” example helps us understand how BERT processes natural language, our goal is to apply the same principles to DNA sequences. Instead of predicting missing words in a sentence, we want our model to learn biological patterns and genomic structure by predicting masked nucleotides within DNA sequences.\nIn DNA modeling, understanding sequence context is just as critical as in language modeling. Just as BERT learns that “fox” fits within a given sentence structure, our model should learn that specific nucleotide sequences appear in biologically meaningful patterns. This could involve recognizing gene coding regions, regulatory motifs, or conserved sequence elements across different genomes.\nTo accomplish this, we define a custom BERT model designed specifically for processing DNA sequences. Unlike traditional text-based models, our DNA BERT model uses a character-level vocabulary of nucleotides (A, T, C, G) and special tokens to represent sequence structure. By leveraging masked language modeling (MLM), the model will learn to predict missing nucleotides based on surrounding context, allowing it to capture meaningful genomic features.\nThe max_position_embeddings defines the longest sequence the model can process at once, which is crucial for biological sequences like genomes or proteins that can vary widely in length. To help the model understand where each token appears in the sequence, position embeddings are added to the token embeddings, giving the model a sense of order and distance, which is especially important when analyzing long-range interactions, like regulatory elements controlling distant genes.\nWith this in mind, let’s move forward and define a standard BERT architecture, which we’ll apply to DNA sequences. Because we’ll train a standard model, we can basically get away with defining the dimensions of certain aspects of the model. Recall in the chapter on the software stack we discussed the general outline of a machine learning model? We could write a script like that for a BERT model, but the Hugging Face transformers library provided standard models, while the Trainer class abstracts away having to write a training loop.\nPython Code:\nfrom transformers import ModernBertConfig, ModernBertForMaskedLM\n\nconfig = ModernBertConfig(\n    vocab_size=len(dna_vocab),\n    hidden_size=256,\n    num_hidden_layers=8,\n    num_attention_heads=8,\n    intermediate_size=512,\n    max_position_embeddings=512,\n    type_vocab_size=1,\n)\nconfig.pad_token_id = dna_vocab[\"[PAD]\"]\nmodel = ModernBertForMaskedLM(config)\nThe keenly eyed among you see a lot of powers of 2, 8 is 2^3, 256, 512 are also powers of two, etc. Computer memory encodes in bits and bytes, and is designed around powers of two. Building matrices that are powers of 2, 16, 32, 64, etc. makes them fit in memory more efficiently, and this can have serious consequences for training efficiency (see Figure 1).\n\n\n\nFigure 1 the power of powers of two.\n\n\n\n\n2.5.5 Configuring Training for DNA BERT\nNow that we have defined our BERT model for DNA sequences, we need to set up the training process. This involves specifying various training hyperparameters, handling masked language modeling (MLM) data, and preparing for efficient learning.\nUnlike traditional NLP tasks where models are trained on massive text corpora, training on DNA sequences presents unique challenges. The structure of genomic data, sequence length, and biological patterns all influence how the model learns. Our configuration is again rather general and not yet tuned to DNA or biological data. If you would scale this model, you’d likely have to drop the learning rate down, for example. In “production” hyperparameter optimization becomes super important when you train a large model on all your data; each run might be costly, and setting optimal hyperparameters can lead to serious gains in training results.\n\n\n2.5.6 Setting Training Parameters\nTo train our DNA BERT model, we use the Hugging Face TrainingArguments class, which allows us to define key training settings. These include:\n\nBatch size: We set a batch size (per_device_train_batch_size) of 16 for both training and evaluation. This determines how many sequences are processed at once.\nLogging & Saving: We log loss every 50 steps (logging_steps) and save model checkpoints every 100 steps to monitor training progress.\nLearning Rate: We use a learning rate of 5e-5 (learning_rate), a common choice for transformer models that balances learning speed and stability.\nWeight Decay: A value of 0.01 is used to prevent overfitting by applying L2 regularization to model weights.\nTraining Steps: The model is trained for 4000 steps (max_steps), though on the Google Colab code I ran in the cloud, I trained for 2 whole epochs over all data (num_train_epochs = 2), which is a more precise way to ensure the model sees all the data twice.\nModel Saving: The model checkpoints are stored in ./bert-dna, allowing us to resume training from a checkpoint if needed (after a computer crash, or after the model going off the rails, etc.).\n\nPython Code:\ntraining_args = TrainingArguments(\n    output_dir=\"./bert-dna\",\n    overwrite_output_dir=True,\n    logging_steps=50,  # Log loss every step\n    save_steps=100,\n    save_total_limit=2,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    max_steps=4000,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    push_to_hub=False,\n    report_to=\"none\",  # Disables wandb logging, can enable if you have a wandb account so you can track your training\n)\nWhile I have enabled it here, I can recommend tracking your training runs on wandb. Go to wandb.ai (w and b meaning weights and biases, the core parameters in AI models) to make a free account. Now to some extent, this is like Strava but for AI, and there is a risk of obsessing over the training metadata. But if you find yourself with a limited amount of compute, or expensive compute paid per minute, it makes a lot of sense to track big training runs in real time so you can intervene. If the run crashes, you can restart, or abort the node so you aren’t paying for an expensive GPU node that’s no longer training.\n\n\n2.5.7 Preparing for Masked Language Modeling (MLM)\nSince we are training our DNA BERT model using masked language modeling (MLM), we need to handle introducing masked tokens properly. This is done using the DataCollatorForLanguageModeling, which:\n\nRandomly masks nucleotides in the training sequences.\nCreates labels automatically, meaning the model learns by trying to predict these labeled masked tokens.\nUses a masking probability of 5%-15%, ensuring that a small but meaningful portion of the sequence is masked during training.\n\nBy applying MLM, we allow the model to generalize nucleotide relationships and capture sequence dependencies, just like how BERT learns relationships between words in text.\nPython Code:\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=hf_tokenizer,\n    mlm=True,\n    mlm_probability=0.05\n)\n\n\n2.5.8 Training the DNA BERT Model\nWith our configuration and data collator in place, we now train the model. We use the Hugging Face Trainer API, which simplifies the training process by handling:\n\nDataset iteration: Automatically loads and batches training sequences.\nGradient updates: Adjusts model weights based on training loss.\nLogging & saving: Tracks training progress and stores checkpoints.\n\nOnce training begins, the model will gradually learn nucleotide dependencies and improve its ability to predict missing DNA bases. Python Code:\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=hf_tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()\nIf you have set up a free wandb account, you can track your training runs, wherever they are running, on a central dashboard. You then get a dashboard full of pretty loss vs progress plots like the one below in figure 2, which I screencapped about ± 30 minutes into training a tiny version of the model on my MacBook.\n\n\n\nFigure 2: a training loss curve screen capped from wandb.com during training\n\n\nI’t valuable to consider the meaning of the loss. In this case the cross entropy loss can be transformed into a probability: \\(e^{-loss} = p\\) the predicted masked token is the true token in the training data. So as you can see in DNA data that is absolutely dominated by the tokens for G,C,T and A (the training data has no, or very very few, ambiguous nucleotide tokens) the loss almost immediately drops to below ~ -1.4, which is very close too random guess in of those four nocleotides: \\(e^{-1.4}  = 0.25\\). In other words it takes the model 1 iteration to learn almost all bases are G,C,T or A in the FASTA. after half an hour the loss is near 1.2, which corresponds to a probability of predicting the masked token correctly of: \\(e^{-1.2}  = 0.30\\). We’ll dig deeper into the meaning of the loss in Chapter 3.\n\n\n2.5.9 Saving the Trained Model\nAfter training completes, we save both the model and tokenizer so they can be used for future predictions or fine-tuning.\n\nThe model weights are stored in ./bert-dna, allowing us to reload the trained model.\nThe tokenizer is also saved, ensuring that input sequences can be processed the same way during inference.\n\nFinally, a success message is printed, confirming that the training process has been completed.\nPython Code:\n# Save the final model and tokenizer\ntrainer.save_model(\"./bert-dna\")\nhf_tokenizer.save_pretrained(\"./bert-dna\")\n\nprint(\" Training complete! Model saved to ./bert-dna\")\nIf you intend to use, and re-use your model repeatedly, on different machines, or share it with others, it’s very convenient to save it to Hugging Face. If you have an account, you can do so for free using their internal tools. You’ll need to include an API token, I have omitted mine, and so should you when sharing code, because the API token lets people write to your account!\nPython Code:\nmodel.push_to_hub(repo_id=\"MichelNivard/DNABert-CDS-13Species-v0.1\",use_auth_token=\"\")\nhf_tokenizer.push_to_hub(repo_id=\"MichelNivard/DNABert-CDS-13Species-v0.1\",use_auth_token=\"\")\nBecause we used a standard BERT model (BertModern), it’s super easy for others to pull the model weights from the hub into a model configured for use on their machine, using the Hugging Face Transformers library. If you want to train a state of the art model you’d obviously need to scale training well beyond what I did here. We’ll cover how to in the chapter on scaling.\n\n\n2.5.10 Summary\nIn this section, we:\n\nDefined training hyperparameters such as batch size, learning rate, and training steps.\nUsed masked language modeling (MLM) to train the model to predict gaps in DNA sequences.\nLeveraged the Hugging Face Trainer API to automate model training.\nSaved the final trained model and tokenizer for future use.\n\nWith this trained model, we can now fine-tune or apply it to various genomic tasks, such as predicting genetic variations or classifying functional DNA sequences. In the next chapter, we will explore how to fine-tune our DNA BERT model for specific applications.\n\n\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. “DNA Language Models Are Powerful Predictors of Genome-Wide Variant Effects.” Proceedings of the National Academy of Sciences 120 (44). https://doi.org/10.1073/pnas.2311219120.\n\n\nBenegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun S. Song. 2025. “Genomic Language Models: Opportunities and Challenges.” Trends in Genetics, January. https://doi.org/10.1016/j.tig.2024.11.013.\n\n\nNguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva Katrekar, David B. Li, Liam J. Bartie, et al. 2024. “Sequence Modeling and Design from Molecular to Genome Scale with Evo.” Science 386 (6723). https://doi.org/10.1126/science.ado9336.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Training our first DNA Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html",
    "href": "Chapter3_DNA.html",
    "title": "3  Evaluating DNA Language Models",
    "section": "",
    "text": "3.1 Introduction\nIn Chapters 1 and 2, we introduced the process of preparing DNA sequences and training a BERT language model for genomic data. In this chapter, we will turn our attention to how single nucleotide mutations can be systematically generated and evaluated using the trained DNA language model.\nThese synthetic mutations can form the basis for an evaluation of our DNA language model.\nThis chapter has two goals:",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#introduction",
    "href": "Chapter3_DNA.html#introduction",
    "title": "3  Evaluating DNA Language Models",
    "section": "",
    "text": "To explain to machine learning readers how to enumerate synonymous and missense mutations based on the standard genetic code, and why this is biologically meaningful. Consider the Khan Academy “AP Bio” course if this chapter doesn’t really offer enough for you https://www.khanacademy.org/science/ap-biology\nTo explain to bioinformatics and genetics readers how the Masked Language Modeling (MLM) objective provides a way to compute the pseudo-log-likelihood (PLL) of entire sequences and to score mutations in terms of their “naturalness” under the trained model.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#biological-background-the-genetic-code-and-mutation-types",
    "href": "Chapter3_DNA.html#biological-background-the-genetic-code-and-mutation-types",
    "title": "3  Evaluating DNA Language Models",
    "section": "3.2 Biological Background: The Genetic Code and Mutation Types",
    "text": "3.2 Biological Background: The Genetic Code and Mutation Types\nBefore diving into code, it’s useful to recall the basics of how DNA encodes proteins. DNA is transcribed into RNA, and RNA is translated into proteins using codons, groups of three nucleotides. Each codon corresponds to a specific amino acid, this mapping is called the genetic code.\n\n\n\nFigure 3.1 DNA is transcribed to RNA then translated to amino acids which form proteins. Source: https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/intro-to-gene-expression-central-dogma\n\n\nCrucially, some amino acids can be encoded by multiple codons, a property called degeneracy. This degeneracy is why synonymous mutations exist — changes in the DNA sequence that do not alter the resulting amino acid. In contrast, missense mutations alter the encoded amino acid, which may change protein function.\nThis distinction between synonymous and missense mutations will allow us to systematically categorize the impact of each possible single nucleotide substitution. Below is the standard genetic code table; it contains a full translation from DNA to protein.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodon\nAmino Acid\nCodon\nAmino Acid\nCodon\nAmino Acid\nCodon\nAmino Acid\n\n\n\n\nTTT\nF\nTTC\nF\nTTA\nL\nTTG\nL\n\n\nTCT\nS\nTCC\nS\nTCA\nS\nTCG\nS\n\n\nTAT\nY\nTAC\nY\nTAA\nStop\nTAG\nStop\n\n\nTGT\nC\nTGC\nC\nTGA\nStop\nTGG\nW\n\n\nCTT\nL\nCTC\nL\nCTA\nL\nCTG\nL\n\n\nCCT\nP\nCCC\nP\nCCA\nP\nCCG\nP\n\n\nCAT\nH\nCAC\nH\nCAA\nQ\nCAG\nQ\n\n\nCGT\nR\nCGC\nR\nCGA\nR\nCGG\nR\n\n\nATT\nI\nATC\nI\nATA\nI\nATG\nM\n\n\nACT\nT\nACC\nT\nACA\nT\nACG\nT\n\n\nAAT\nN\nAAC\nN\nAAA\nK\nAAG\nK\n\n\nAGT\nS\nAGC\nS\nAGA\nR\nAGG\nR\n\n\nGTT\nV\nGTC\nV\nGTA\nV\nGTG\nV\n\n\nGCT\nA\nGCC\nA\nGCA\nA\nGCG\nA\n\n\nGAT\nD\nGAC\nD\nGAA\nE\nGAG\nE\n\n\nGGT\nG\nGGC\nG\nGGA\nG\nGGG\nG\n\n\n\nA single nucleotide mutation can cause:\n\nA synonymous mutation: The amino acid does not change, meaning the mutation is “silent” in terms of protein sequence. For example, in row 1 of the table, we see that if we mutate the codon TTT to TTC, both before and after the mutation the amino acid F (phe) is produced. While it’s not guaranteed by any means that a synonymous mutation is entirely harmless, they’re very likely to be harmless.\nA missense mutation: The amino acid changes, potentially altering protein structure and function. For example, in row 1 of the table, we see that if we mutate the codon TTT to TTA, the amino acid F (phe) is replaced by L (leu) in the protein, potentially changing the function. While missense mutations aren’t always damaging, they are far more likely to be damaging.\n\nEarlier, we trained a DNA language model on coding sequences for humans, and I actually expanded that to a training run of 2 epochs (the data was all used twice) on 500k sequences from 13 vertebrate species. This model should, with probabilities slightly above chance,",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#creating-all-single-nucleotide-mutants",
    "href": "Chapter3_DNA.html#creating-all-single-nucleotide-mutants",
    "title": "3  Evaluating DNA Language Models",
    "section": "3.3 Creating All Single-Nucleotide Mutants",
    "text": "3.3 Creating All Single-Nucleotide Mutants\nThe code in this section systematically generates every possible single nucleotide substitution across the input sequence. Since each codon consists of three nucleotide’s, and each nucleotide can mutate into three alternatives, there are up to 9 potential codon variants for each original codon.\n\n\n\n\n\n\nTip\n\n\n\nThe data generated by applying this “mutator” to the DRD2 (Dopamine receptor D2) gene is on Hugging Face: https://huggingface.co/datasets/MichelNivard/DRD2-mutations\n\n\nFor each mutation, we check the original amino acid and the new amino acid using the standard genetic code table. This allows us to classify each mutation as either:\n\nSynonymous: Same amino acid, no apparent change to the protein.\nMissense: Different amino acid, potential change to protein function.\n\nThis step is crucial in genomics, where we often want to prioritize functional variants — mutations that actually change protein products, rather than silent changes that do not.\nI have a preference for R myself, so I wrote this specific job in R. We provide the gene sequence, starting at the start codon; I use the dopamine receptor gene DRD2. Based on the genetic code, which translates DNA to the amino acids that eventually are produced, we then write code to mutate each codon in a gene.\nFOr the demo we’ll manually imput the gene:\nDRD2 &lt;- \"ATGGATCCACTGAATCTGTCCTGGTATGATGATGATCTGGAGAGGCAGAACTGGAGCCGGCCCTTCAACGGGTCAGACGGGAAGGCGGACAGACCCCACTACAACTACTATGCCACACTGCTCACCCTGCTCATCGCTGTCATCGTCTTCGGCAACGTGCTGGTGTGCATGGCTGTGTCCCGCGAGAAGGCGCTGCAGACCACCACCAACTACCTGATCGTCAGCCTCGCAGTGGCCGACCTCCTCGTCGCCACACTGGTCATGCCCTGGGTTGTCTACCTGGAGGTGGTAGGTGAGTGGAAATTCAGCAGGATTCACTGTGACATCTTCGTCACTCTGGACGTCATGATGTGCACGGCGAGCATCCTGAACTTGTGTGCCATCAGCATCGACAGGTACACAGCTGTGGCCATGCCCATGCTGTACAATACGCGCTACAGCTCCAAGCGCCGGGTCACCGTCATGATCTCCATCGTCTGGGTCCTGTCCTTCACCATCTCCTGCCCACTCCTCTTCGGACTCAATAACGCAGACCAGAACGAGTGCATCATTGCCAACCCGGCCTTCGTGGTCTACTCCTCCATCGTCTCCTTCTACGTGCCCTTCATTGTCACCCTGCTGGTCTACATCAAGATCTACATTGTCCTCCGCAGACGCCGCAAGCGAGTCAACACCAAACGCAGCAGCCGAGCTTTCAGGGCCCACCTGAGGGCTCCACTAAAGGAGGCTGCCCGGCGAGCCCAGGAGCTGGAGATGGAGATGCTCTCCAGCACCAGCCCACCCGAGAGGACCCGGTACAGCCCCATCCCACCCAGCCACCACCAGCTGACTCTCCCCGACCCGTCCCACCATGGTCTCCACAGCACTCCCGACAGCCCCGCCAAACCAGAGAAGAATGGGCATGCCAAAGACCACCCCAAGATTGCCAAGATCTTTGAGATCCAGACCATGCCCAATGGCAAAACCCGGACCTCCCTCAAGACCATGAGCCGTAGGAAGCTCTCCCAGCAGAAGGAGAAGAAAGCCACTCAGATGCTCGCCATTGTTCTCGGCGTGTTCATCATCTGCTGGCTGCCCTTCTTCATCACACACATCCTGAACATACACTGTGACTGCAACATCCCGCCTGTCCTGTACAGCGCCTTCACGTGGCTGGGCTATGTCAACAGCGCCGTGAACCCCATCATCTACACCACCTTCAACATTGAGTTCCGCAAGGCCTTCCTGAAGATCCTCCACTGCTGA\"\nnchar(DRD2)/3\nThewn we’ll need to encode the genertic code, to translate between nucleotides and amino-acids:\n# Genetic code table (Standard Code)\ngenetic_code &lt;- c(\n  \"TTT\"=\"F\", \"TTC\"=\"F\", \"TTA\"=\"L\", \"TTG\"=\"L\",\n  \"TCT\"=\"S\", \"TCC\"=\"S\", \"TCA\"=\"S\", \"TCG\"=\"S\",\n  \"TAT\"=\"Y\", \"TAC\"=\"Y\", \"TAA\"=\"Stop\", \"TAG\"=\"Stop\",\n  \"TGT\"=\"C\", \"TGC\"=\"C\", \"TGA\"=\"Stop\", \"TGG\"=\"W\",\n  \"CTT\"=\"L\", \"CTC\"=\"L\", \"CTA\"=\"L\", \"CTG\"=\"L\",\n  \"CCT\"=\"P\", \"CCC\"=\"P\", \"CCA\"=\"P\", \"CCG\"=\"P\",\n  \"CAT\"=\"H\", \"CAC\"=\"H\", \"CAA\"=\"Q\", \"CAG\"=\"Q\",\n  \"CGT\"=\"R\", \"CGC\"=\"R\", \"CGA\"=\"R\", \"CGG\"=\"R\",\n  \"ATT\"=\"I\", \"ATC\"=\"I\", \"ATA\"=\"I\", \"ATG\"=\"M\",\n  \"ACT\"=\"T\", \"ACC\"=\"T\", \"ACA\"=\"T\", \"ACG\"=\"T\",\n  \"AAT\"=\"N\", \"AAC\"=\"N\", \"AAA\"=\"K\", \"AAG\"=\"K\",\n  \"AGT\"=\"S\", \"AGC\"=\"S\", \"AGA\"=\"R\", \"AGG\"=\"R\",\n  \"GTT\"=\"V\", \"GTC\"=\"V\", \"GTA\"=\"V\", \"GTG\"=\"V\",\n  \"GCT\"=\"A\", \"GCC\"=\"A\", \"GCA\"=\"A\", \"GCG\"=\"A\",\n  \"GAT\"=\"D\", \"GAC\"=\"D\", \"GAA\"=\"E\", \"GAG\"=\"E\",\n  \"GGT\"=\"G\", \"GGC\"=\"G\", \"GGA\"=\"G\", \"GGG\"=\"G\"\n)\nThen we define a function, which for a codon, in a gene, applied all mutations (e.g. each of the 3 based is mutated to G,C,T and A). It stores a whole bunch of info on the mutation, essential is that it also encodes whether the mutation is synonymous, or missense, but also which amino-acid is changed to which other amino-acid. This could allow you to later go back\n# Function to get all mutations for a codon\nmutate_codon &lt;- function(codon, codon_index, full_sequence) {\n  nucleotides &lt;- c(\"A\", \"T\", \"C\", \"G\")\n  mutations &lt;- data.frame()\n  \n  original_aa &lt;- genetic_code[[codon]]\n  \n  for (pos in 1:3) {\n      original_base &lt;- substr(codon, pos, pos)\n      for (nuc in nucleotides) {\n          if (nuc != original_base) {\n              # Mutate the codon at this position\n              mutated_codon &lt;- codon\n              substr(mutated_codon, pos, pos) &lt;- nuc\n              mutated_aa &lt;- genetic_code[[mutated_codon]]\n              \n              # Create the mutated sequence\n              mutated_sequence &lt;- full_sequence\n              start &lt;- (codon_index - 1) * 3 + 1\n              substr(mutated_sequence, start, start+2) &lt;- mutated_codon\n              \n              mutation_type &lt;- if (mutated_aa == original_aa) \"synonymous\" else \"missense\"\n              \n              mutations &lt;- rbind(mutations, data.frame(\n                  codon_index = codon_index,\n                  position = pos,\n                  original_codon = codon,\n                  mutated_codon = mutated_codon,\n                  original_aa = original_aa,\n                  mutated_aa = mutated_aa,\n                  mutation_position = (codon_index -1)*3 + pos,\n                  mutation_type = mutation_type,\n                  sequence = mutated_sequence\n              ))\n          }\n      }\n  }\n  return(mutations)\n}\nThen we define the main function, which applies the mutations to the whole sequence, storing all possible single base mutations:\n# Main function to process the whole sequence\nmutate_sequence &lt;- function(dna_sequence) {\n  codons &lt;- strsplit(dna_sequence, \"\")[[1]]\n  codons &lt;- sapply(seq(1, length(codons), by=3), function(i) paste(codons[i:(i+2)], collapse=\"\"))\n  all_mutations &lt;- data.frame()\n  \n  for (i in seq_along(codons)) {\n      codon &lt;- codons[i]\n      mutations &lt;- mutate_codon(codon, i, dna_sequence)\n      all_mutations &lt;- rbind(all_mutations, mutations)\n  }\n  return(all_mutations)\n}\nWe apply these functionsto the DRD2 sequence, and we store all mutated sequences and a bunch of meta data.\n# Example usage\nsequence &lt;- DRD2\nmutations &lt;- mutate_sequence(sequence)\n\n# Filter synonymous and missense if needed\nsynonymous_mutations &lt;- subset(mutations, mutation_type == \"synonymous\")\nmissense_mutations &lt;- subset(mutations, mutation_type == \"missense\")\n\nsource &lt;- c(NA,\"wildtype\",DRD2)\n\noutput &lt;- rbind(source,mutations[,7:9])\n\nwrite.csv(file=\"DRD2_mutations.csv\",output)",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#evaluating-base-position-likelihoods-with-a-bert-model",
    "href": "Chapter3_DNA.html#evaluating-base-position-likelihoods-with-a-bert-model",
    "title": "3  Evaluating DNA Language Models",
    "section": "3.4 Evaluating base position likelihoods with a BERT Model",
    "text": "3.4 Evaluating base position likelihoods with a BERT Model\nIn machine learning terms, the MLM loss is the negative log likelihood (NLL) of the correct nucleotide. For example, if the correct nucleotide is “A” at a given position, and the model assigns “A” a probability of 0.8, then the contribution to the loss is:\n\\[loss = −ln(0.8) = 0.22\\]\nThe lower this value, the better the model’s confidence matches reality — indicating that the nucleotide was expected. Near the end of training our models loss hovered around 1.09, meaning that the average true base had a predicted probability of ±34%. The loss is highly dependent on the tokenizer, for example if we would have used a more complex tokenizer with say 100 options for each next token (encoding for example all 3-mer combinations of bases: A,C,T,G,AA,AC,AT,AG etc etc until GGA,GGG) the the probability of geting the one correct token is way lower as the base rate is way lower!\nWhen we compute the pseudo-log-likelihood (PLL) for an entire sequence, we mask and score each position, adding up these log probabilities:\n\\[log⁡P(nucleotide_1)+log⁡P(nucleotide_2)+ ⋯ +log⁡P(nucleotide_n)​\\]\nThis sum is the total log likelihood of the sequence under the model — it quantifies how natural the model thinks the sequence is.\nFirst we load the model I trained in Chapter 2, if you trained your own on more data, or for longer, or want to evaluate a different model you can load those yourself easily.\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport torch\nimport pandas as pd\n\n# Load model & tokenizer\nmodel_name = \"MichelNivard/DNABert-CDS-13Species-v0.1\"  # Replace if needed\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\nmodel.eval()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Maximum context length — BERT's trained context window\nMAX_CONTEXT_LENGTH = 512\n\n3.4.1 Estimating the effects of all mutations\nThen we define two functions (we’re back to python here) to compute: 1) the pseudo-likelihood of the whole sequence up to 512 bases, as that is the sequence length we trained DNABert for (in full-scale applications, you’d use a longer sequence length), and 2) the log-likelihood ratio of the mutation vs. the wildtype (original DRD2 sequence).\ndef compute_log_likelihood(sequence, tokenizer, model):\n    \"\"\"Compute pseudo-log-likelihood (PLL) for the first 512 bases.\"\"\"\n    tokens = tokenizer(sequence, return_tensors='pt', add_special_tokens=True)\n    input_ids = tokens['input_ids'].to(device)\n    attention_mask = tokens['attention_mask'].to(device)\n\n    log_likelihood = 0.0\n    seq_len = input_ids.shape[1] - 2  # Exclude [CLS] and [SEP]\n\n    with torch.no_grad():\n        for i in range(1, seq_len + 1):\n            masked_input = input_ids.clone()\n            masked_input[0, i] = tokenizer.mask_token_id\n\n            outputs = model(masked_input, attention_mask=attention_mask)\n            logits = outputs.logits\n\n            true_token_id = input_ids[0, i]\n            log_probs = torch.log_softmax(logits[0, i], dim=-1)\n            log_likelihood += log_probs[true_token_id].item()\n\n    return log_likelihood\n\n\ndef compute_mutant_log_likelihood_ratio(wild_type, mutant, position, tokenizer, model):\n    \"\"\"Compare wild type and mutant likelihood at a single position (within 512 bases).\"\"\"\n    assert len(wild_type) == len(mutant), \"Wild type and mutant must have the same length\"\n    assert wild_type[position] != mutant[position], f\"No mutation detected at position {position + 1}\"\n\n    tokens = tokenizer(wild_type[:MAX_CONTEXT_LENGTH], return_tensors='pt', add_special_tokens=True)\n    input_ids = tokens['input_ids'].to(device)\n    attention_mask = tokens['attention_mask'].to(device)\n\n    mask_position = position + 1  # Shift for [CLS] token\n\n    masked_input = input_ids.clone()\n    masked_input[0, mask_position] = tokenizer.mask_token_id\n\n    with torch.no_grad():\n        outputs = model(masked_input, attention_mask=attention_mask)\n        logits = outputs.logits\n\n        log_probs = torch.log_softmax(logits[0, mask_position], dim=-1)\n\n    wild_base_id = tokenizer.convert_tokens_to_ids(wild_type[position])\n    mutant_base_id = tokenizer.convert_tokens_to_ids(mutant[position])\n\n    log_prob_wild = log_probs[wild_base_id].item()\n    log_prob_mutant = log_probs[mutant_base_id].item()\n\n    return log_prob_wild - log_prob_mutant\n\n\n3.4.2 The Likelihood Ratio to evaluate mutations\nThe log-likelihood ratio (LLR) compares how much more (or less) likely the wild-type sequence is compared to a mutant sequence, given the DNA language model. Specifically, we compare the log-likelihood of the correct wild-type nucleotide to the log likelihood of the mutant nucleotide at the mutated position only.\n\\[LLR = log ⁡ P ( wild-type nucleotide ∣ context ) − log ⁡ P ( mutant nucleotide ∣ context )\\]\nThis metric is widely used in bioinformatics because it focuses on the exact site of the mutation, instead of comparing entire sequences. A positive LLR indicates the wild-type is favored by the model (the mutation is unlikely and therefore possibly deleterious), while a negative LLR means the mutant is more likely (the mutation is neutral or maybe even protective).\nWe then apply these functions to all the synthetic DRD2 mutations we generated (in the first 512 bases) to evaluate whether the DNABert we trained thinks the missense mutations are generally less likely, and therefore possibly damaging, given the model.\n# Load dataset directly from Hugging Face dataset repo\ndataset_url = \"https://huggingface.co/datasets/MichelNivard/DRD2-mutations/raw/main/DRD2_mutations.csv\"\ndf = pd.read_csv(dataset_url)\n\n# Find wild-type sequence\nwild_type_row = df[df['mutation_type'] == 'wildtype'].iloc[0]\nwild_type_sequence = wild_type_row['sequence'][:MAX_CONTEXT_LENGTH]\n\nresults = []\n\n# Process all sequences\nfor idx, row in df.iterrows():\n    sequence = row['sequence'][:MAX_CONTEXT_LENGTH]\n    mutation_type = row['mutation_type']\n    mutation_position = row['mutation_position'] - 1  # Convert 1-based to 0-based\n\n    # Skip mutants where the mutation position is beyond 512 bases\n    if mutation_type != 'wildtype' and mutation_position &gt;= MAX_CONTEXT_LENGTH:\n        continue\n\n    print(idx)\n\n    llr = None\n    log_prob_wild = None\n    prob_wild = None\n\n    if mutation_type != 'wildtype':\n        llr, log_prob_wild, prob_wild = compute_mutant_log_likelihood_ratio(\n            wild_type_sequence, sequence, int(mutation_position), tokenizer, model\n        )\n\n    # append results for each mutation:\n    results.append({\n        'sequence': sequence,\n        'mutation_type': mutation_type,\n        'pll': 0,\n        'llr': llr,\n        'wildtype_log_prob': log_prob_wild,\n        'wildtype_prob': prob_wild,\n        'mutation_position': mutation_position + 1\n    })\n\n\n# Convert to DataFrame for saving or inspection\nresults_df = pd.DataFrame(results)\n\n# Save or print results\nprint(results_df)\n\n# Optionally, save to CSV\nresults_df.to_csv(\"sequence_log_likelihoods.csv\", index=False)\n\n\n3.4.3 Language Models Provide Biological Insight through the likelihood ratio\nWhy do we care about these log likelihoods and log likelihood ratios? Because they provide a direct, data-driven estimate of how plausible or “natural” each mutated sequence looks to the model compared to the wild type sequence. Since the model was trained on real DNA sequences, sequences with high likelihoods resemble biological reality, while sequences with low likelihoods deviate from patterns the model has learned. A high “mutation log likelihood ratio” corresponds to the model strongly favoring the reference sequences over the mutation(Meier et al. 2021). This test lets us flag potentially deleterious mutations (those with sharp likelihood drops), prioritize candidate variants for functional follow-up, or even explore adaptive evolution by identifying mutations that DNA BERT “likes” more than the wild-type\nTo explore our result here, we can plot the LLR versus the position within the DRD2 gene, this can give us insight into the location within the coding sequence where we find unlikely (and therefore potentially damaging) mutations. in the plot below a LOW LLR means the variant is unlikely. Most variants cluster around a neutral LLR, consistent with some statistical noise.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter to only mutations (skip wildtype which has no llr)\nplot_df = results_df[results_df['mutation_type'].isin(['synonymous', 'missense'])].copy()\n\n\n# Optional: Clip LLR to avoid excessive sizes\nplot_df['size'] = plot_df['llr'].clip(-5, 5)  # LLRs smaller than -5 get maximum size\n\n# Scatter plot with enhanced size scaling\nplt.figure(figsize=(14, 5))\nsns.scatterplot(\n    x='mutation_position', \n    y='llr', \n    hue='mutation_type', \n    size='size',  # Use clipped size column\n    sizes=(20, 200),  # Bigger range for better visibility\n    alpha=0.7, \n    palette={'synonymous': 'green', 'missense': 'orange'},\n    data=plot_df\n)\nplt.axhline(0, color='gray', linestyle='--', label='Neutral LLR')\nplt.title('Mutation Log Likelihood Ratio (LLR) Along DRD2 Gene')\nplt.xlabel('Position in Gene')\nplt.ylabel('Log Likelihood Ratio (LLR)')\nplt.legend(title='Mutation Type', bbox_to_anchor=(1.02, 1), loc='upper left')\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\nFigure 3.2 The LLR for the mutation (y-axis) give the position in the DRD2 gene (x-axis). High values indicate the reference, or wild, type sequence is far more likely than the mutation. Very high LLR values are almost exclusively missense mutations, consistent with a DNA model able to pick up deleterious variants based on its training\n\n\nIt’s obvious from Figure 3.2 that 1. really almost all very unlikely mutations (positive LLR) are missense mutations and 2. There are potentially certain locations within this particular coding sequence where there is an abundance of unlikely mutations packed closely together, these could be regions that are intolerant to deleterious mutations.\nIt’s important to not get overconfident in our predictions! Remember this is a relatively tiny DNA sequence model (±5m parameters) we trained on sequences for 13 fairly randomly picked vertebrate species. Let’s look at the likelihood of the true base in the references (wild-type) sequence given the model. The mean probability is 40% (Figure 3.3), given the model essentially is trying to pick between 4 tokens (G,C,T & A) 40% is considerably better than chance! It’s also clear the probability is not even across the gene, the first few bases are almost certain (almost all coding sequences in these species start with the start codon ATG, the model obviously learned this). After that, there is quite a spread, which is logical I think, in many places across the sequence the specific base might be very obvious, as all three alternates might be missense mutations, but in other spots one, two or even all three alternate tokens might be synonymous, and perhaps even present in the analog gene in the other 12 species we trained our model on! This would make the model FAR less certain about the true base at that location.\n\n\n\nFigure 3.3 The probability of the base in the reference, or wild type, sequence given the DNABert model we trained. The model clearly performed above random (random guessing would be 1 in 4, or 25%).",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#self-study-exercises",
    "href": "Chapter3_DNA.html#self-study-exercises",
    "title": "3  Evaluating DNA Language Models",
    "section": "3.5 Self-Study Exercises",
    "text": "3.5 Self-Study Exercises\nYou have reached a key point where you now are a “consumer” of DNA language models. If you have followed along you should be able to run them on a sequence of our choosing. But to become a savvy or discerning consumer (let that cabernet breath, roll it around the glass…) of DNA language models, you’ll have to evaluate them on a benchmark that matches your application. The exercises below are meant to give you a taste of evaluating a few models. The hard exercises also set us up for the next few chapters by pulling in external evolutionary salient data. There are people who think long and hard about model evaluation (Patel et al. 2024; Tang et al. 2024) and if you are to do rigorous evaluation for an academic project/paper it’ll pay off to catch up on the literature.\n\n\n\n\n\n\nTip\n\n\n\nIf you want a good Masters/PhD term project with limited compute demands, or optimal self-study try some of the following excersizes:\n1. (easy) Evaluate about 100 genes split the variants into synonymous, missense, start-lost (the start coding is changed by the mutation), stop-gained (a specific amino-acid is mutated to a stop coding) and reproduce those 4 rows in (Benegas, Batra, and Song 2023) figure 4. Think critically whether you want to consider alternate metrics for “fit” we used LLR in this chapter, but others used cosine distance, or dot-product between wild-type and mutated sequence embedding as a measure of deleteriousness. Do the other metrics more clearly separate stop-gained from missense from synonymous?\n2. (medium) Evaluate about 100 genes split the synthetic variants into synonymous, missense, start-lost, stop-gained and reproduce those 4 rows in (Benegas, Batra, and Song 2023) figure 4. Do so for 3 DNA language models, the one we trained in Chapter 2 (use your own, or if you didn’t train one the one I made available: https://huggingface.co/MichelNivard/DNABert-CDS-13Species-v0.1 ) a medium sized (500m parameters) nucleotide transformer (Dalla-Torre et al. 2024) (https://huggingface.co/InstaDeepAI/nucleotide-transformer-v2-500m-multi-species) and DNABERT-2 (Zhou et al. 2023) (https://huggingface.co/zhihan1996/DNABERT-2-117M).\n3. (hard) Evaluate about 100 genes split the synthetic variants into synonymous, missense, start-lost , stop-gained and reproduce those 4 rows in (Benegas, Batra, and Song 2023) figure 4. Do so for 3 DNA language models, the one we trained in Chapter2 ( use your own, or if you didn’t train one the one I made available: https://huggingface.co/MichelNivard/DNABert-CDS-13Species-v0.1 ) a medium sized (500m parameters) nucleotide transformer (Dalla-Torre et al. 2024) (https://huggingface.co/InstaDeepAI/nucleotide-transformer-v2-500m-multi-species) and DNABERT-2 (Zhou et al. 2023) (https://huggingface.co/zhihan1996/DNABERT-2-117M). Allign the DNA sequences with at least one external track form the UCSC genome browser, for example the allele frequencies in 1000 genomes, or the 100 way alignment across species (https://hgdownload.soe.ucsc.edu/goldenPath/hg38/multiz100way/). Establish whether mutations to bases that are rarely variant(MAF &lt; 5%), or variants that are never observed (freq = 0%), or those to bases that are less conserved are less likely according to the model.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#further-reading",
    "href": "Chapter3_DNA.html#further-reading",
    "title": "3  Evaluating DNA Language Models",
    "section": "3.6 Further reading",
    "text": "3.6 Further reading\n\nA paper diving deeper into the the nature of proteins for which the likelihood ratio is able to detect the deleteriousness of mutations. It turns out the LLR is most informative for proteins with a certain perplexity given the model, which puts boundaries on the set of proteins for which the LLR is useful.\nA blog post implementing the LLR, and other scoring metrics form a recent paper(Meier et al. 2021), for protein language models (specifically ESM2).",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter3_DNA.html#summary",
    "href": "Chapter3_DNA.html#summary",
    "title": "3  Evaluating DNA Language Models",
    "section": "3.7 Summary",
    "text": "3.7 Summary\nThis chapter introduced key pieces that are essential for those who want to train, or just understand, DNA Language models.\n\nWe explored how to systematically generate all synonymous and missense mutations in a gene, these simulated mutations then form an important part in initial evaluation of our model.\nWe discussed how to compute the log-likelihood of sequences, and log-likelihood ratio of a single mutation using DNA BERT. These metrics are a proxy for how natural the model considers each sequence.\nWe finally used these simulated mutations and some knowledge of biology (whether the mutations are synonymous or missense) to validate that our language model actually did do some learning.\nWe reviewed possible exercises that could help you become a better consumer of genomic language models by critically evaluating them.\n\nThe analyses outlined in this chapter form the foundation for variant effect prediction using genomic language models.\n\n\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. “DNA Language Models Are Powerful Predictors of Genome-Wide Variant Effects.” Proceedings of the National Academy of Sciences 120 (44). https://doi.org/10.1073/pnas.2311219120.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2024. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. “Language Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.” http://dx.doi.org/10.1101/2021.07.09.450648.\n\n\nPatel, Aman, Arpita Singhal, Austin Wang, Anusri Pampari, Maya Kasowski, and Anshul Kundaje. 2024. “DART-Eval: A Comprehensive DNA Language Model Evaluation Benchmark on Regulatory DNA.” https://doi.org/10.48550/ARXIV.2412.05430.\n\n\nTang, Ziqi, Nirali Somia, Yiyang Yu, and Peter K Koo. 2024. “Evaluating the Representational Power of Pre-Trained DNA Language Models for Regulatory Genomics.” http://dx.doi.org/10.1101/2024.02.29.582810.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2023. “DNABERT-2: Efficient Foundation Model and Benchmark for Multi-Species Genome.” https://doi.org/10.48550/ARXIV.2306.15006.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluating DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html",
    "href": "Chapter4_DNA.html",
    "title": "4  Evolution-Aware Encoders",
    "section": "",
    "text": "4.1 Introduction\nIn previous chapters, we introduced the basic principles of BERT for DNA sequences. We took inspiration from natural language processing (NLP), treating DNA as a language, where sequences of nucleotides (A, T, C, G, -) could be processed using transformers. This approach, while powerful, carries over several assumptions from natural language that do not perfectly align with biological sequences. In this chapter, we will re-examine how we encode genomic data and introduce a new design paradigm, evolutionary-aware encoding, inspired by the recently proposed GPN (Genomic Pre-trained Network).",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#tokenization-and-embedding-in-language-models",
    "href": "Chapter4_DNA.html#tokenization-and-embedding-in-language-models",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.2 Tokenization and Embedding in Language Models",
    "text": "4.2 Tokenization and Embedding in Language Models\nModern language models, whether BERT, GPT, or similar architectures, rely heavily on how input sequences are tokenized and encoded before they ever reach the attention layers. This initial step — often overlooked — plays a profound role in shaping how the model learns.\n\n4.2.1 Tokenization in Natural Language\nIn human languages like English or French, the vocabulary is large, often comprising tens of thousands of tokens. These tokens could be:\n\nWhole words (“cat”, “sat”).\nSubwords (“cat” might break into “c”, “at”).\nEven characters (in rare cases).\n\nSince the number of tokens is so large, each token is assigned a unique vector embedding, a dense, learnable representation of its “meaning”. These embeddings are gradually refined during training as the model learns how tokens behave in different contexts. The model learns, based on the massive amounts of training data, what the word means, what other words have similar or related meanings. This is essential because linguists and those who study language have vast knowledge of word meaning, numerically encoding that knowledge so that a computational model could process isn’t currently a feasible task. Therefore, in a natural (as opposed to biological) large language model, word embeddings are learned from the data, the data being all the text on the internet.\n\n\n4.2.2 The Embedding Process (NLP BERT)\nInput Sentence:  \"The cat sat on the mat\"\n\nStep 1 - Tokenization:\n    [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n\nStep 2 - Lookup:\n    Each token gets a fixed vector from an embedding table.\n\n    \"The\" -&gt; [0.25, 0.13, -0.11, ..., 0.04]\n    \"cat\" -&gt; [0.88, -0.23, 0.45, ..., -0.67]\n\nStep 3 - Transformer Layers:\n    These embeddings are updated based on surrounding words (context).\n\n\n4.2.3 Language Evolution is Decayed\nThe design of these token embeddings reflects a key fact about human languages: the evolutionary history of words might be relevant to understanding their meaning today, but the words’ context in text is way more informative. While linguistic etymology exists, the meaning of “cat” today does not rely on whether the word originated from Latin or Proto-Indo-European. Context (the words around “cat”) matters far more than distant etymology. Even if I am unfairly discounting the importance of etymology in linguistics (I am no linguist, don’t take my word for it), the quantity of older texts, relative to the quantity of modern texts, the lack of an obvious coding scheme for embedding a word in its etymological history are problematic and would have to be very effective given how effective “word in textual context” embeddings are. However, biology, and DNA in particular, is different.\n\n\n4.2.4 Biological Sequences are Fundamentally Different\nThe DNA encoding we have been working with (A, T, G, C, -) has 5 tokens, perhaps 20 if we encode all the codes used in genetics to code for ambiguous or missing bases. Protein language models we’ll cover later have ±20 amino-acids commonly found in proteins. If we use longer vocabularies, like k-mer or BPE tokenizer vocabularies, it’s not clear the longer sequences we obtain really are comparable or interchangeable. The point of embedding is to cluster similar and dissimilarities, in order to predict the next or a masked token if the presence of up to 128,000 tokens to choose from, some of which have very similar meanings or could fully alter the meaning of a sentence (by negation or omission). In biology, we have a small vocabulary, 5 or 20, or if you wish up to a few hundred tokens. We do, however, have an incredible understanding of the evolutionary history (Figure 4.1) of each base in the genome, we know its place in the genome of other species and can align those to each other!\n\n\n\nFigure 4.1 The Evogeneao Tree of Life diagram, all rights reserved Leonard Eisenberg (2008 & 2017). Get posters and relevant teaching materials here: https://www.evogeneao.com/en/learn/tree-of-life\n\n\n\n\n4.2.5 Evolutionary Context as an Embedding\nThe evolutionary history of a genomic position, how conserved it is, how it varies across species, directly influences our estimation of its importance and its tolerance to mutation. A nucleotide in a highly conserved enhancer region requires different levels of attention (from the model or us scientists) than a nucleotide in a rapidly evolving spacer.\n\n\n4.2.6 \n\nTable 1 Key Differences Between Language and DNA\n\n\n\n\n\n\n\nAspect\nNatural Language\nGenomics\n\n\n\n\nNumber of Tokens\nTens of thousands\n~5 (A, T, G, C, -)\n\n\nMeaning\nFlexible, evolves over time\nBiochemically fixed\n\n\nEvolutionary Context\nMostly irrelevant to meaning\nOften crucial (conservation, divergence)\n\n\nToken Embedding\nFully learned\nNo unique encoding for each token, but predefined based on token-specific evolutionary history\n\n\nNeighboring Context\nDefines meaning\nDefines local motifs, but evolutionary context adds extra layer\n\n\n\nTo capture this cross-species evolutionary context, we need an embedding strategy that combines:\n\nThe identity of the nucleotide itself (A, T, G, C, -).\nThe state of this position in aligned species (what bases appear at the same position in other species).\n\nThis evolutionary-aware modelling is at the heart of the Genomic Pre-trained Network (GPN) architecture and various famous protein language models like AlphaFold(Benegas et al. 2023; Lupo, Sgarbossa, and Bitbol 2022; Jumper et al. 2021). In the specific DNA language model we’ll discuss in this chapter, the encoding is computed for each base given its history. So while the model has 5 tokens (G, C, T, A, and -), these tokens do not map to a fixed embedding; rather, the base “A” maps to an encoding (one-hot encoding) for A, but then also for the same base in aligned sequences of 99 non-human species. This fundamentally changes the model architecture, changing it from a language model applied to DNA as we did in Chapter 3 to a DNA language model, or maybe even just a DNA model.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#introducing-gpn-msa-bert",
    "href": "Chapter4_DNA.html#introducing-gpn-msa-bert",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.3 4. Introducing GPN-MSA-BERT",
    "text": "4.3 4. Introducing GPN-MSA-BERT\nGPN-MSA-BERT (inspired by Benegas et al. (2023)) adapts BERT-style masked language modeling (MLM) to DNA sequences, but incorporates multispecies alignment (MSA) data directly into the model’s input.\n\n\n\nFigure 2 An example multiple sequence alignment (MSA) across 7 sequences (usually species). Source: https://www.biorender.com/template/multiple-sequence-alignment-dna author: Eunice Huang\n\n\n\n4.3.1 Key Idea: Dynamic Position Embeddings\nFor each position in the human genome, the model receives:\n\nThe human base (A, T, G, C, -) — this is the usual input.\nThe aligned bases from other species — these are additional features.\nThese aligned bases are one-hot encoded and concatenated to the human base’s embedding.\n\nThis turns a simple nucleotide embedding, for any given nucleotide, into a dynamic, position-specific vector that depends on its evolutionary context across species.\n\n\n4.3.2 Visualization\nHuman Position:     A\nAligned Species:    A  G  A  (species 1, species 2, species 3)\n\nEmbedding:\n    [ OneHot_A | OneHot_A | OneHot_G | OneHot_A ]\nThis combined vector captures:\n\nWhat the human base is.\nHow conserved the site is.\nWhich substitutions are tolerated across species.\n\n\n\n4.3.3 Practical Implementation - Replacing the BERT Encoder\nTo implement this in practice, we can directly modify a Hugging Face model class (like ModernBertForMaskedLM) to use our custom GPNEmbedding layer in place of the standard token embedding layer.\nThis requires:\n\nDefining a tokenizer that tokenizes each base and aligned bases in other species into the structure expected by the embedding.\nDefining a GPNEmbedding class that can handle one-hot human base with species features and builds the embedding for each base.\nReplacing ModernBertForMaskedLM with a custom GPNBERTMaskedLM class.\nEnsuring all forward methods accept both input_ids and aux_features, which are passed into the embedding layer.\nWe additionally define our own tokenizer and data collator (not shown here but available in the full script).\n\nThe code below takes a human sequence, encodes it in a one-hot encoding (so A: 10000, T: 01000, G: 00100, C: 00010, -: 00001 for example), does the same for any auxiliary aligned sequences from other species. Then the embedding function combines both into one.\n# --------------------------------\n# 5. Encode Human and Auxiliary Species Sequences in \n# --------------------------------\n\ndef one_hot_encode_base(base):\n    \"\"\"One-hot encodes A, T, G, C, - (5 bases total).\"\"\"\n    base_to_idx = {\"A\": 0, \"T\": 1, \"C\": 2, \"G\": 3, \"-\": 4}\n    one_hot = np.zeros(5, dtype=np.float32)\n    if base in base_to_idx:\n        one_hot[base_to_idx[base]] = 1.0\n    return one_hot\n\ndef tokenize_with_aux(examples):\n    human_seq = clean_sequence(examples[\"human_sequence\"])\n\n    # Drop first 10 species (closest relatives)\n    species_seqs = [clean_sequence(seq) for seq in examples[\"species_sequences\"]]\n    species_seqs = species_seqs[10:]  # &lt;-- This line omits the first 10 species\n\n    # Tokenize human sequence\n    tokens = hf_tokenizer(human_seq, truncation=True, padding=\"max_length\", max_length=512)\n    input_ids = tokens[\"input_ids\"]\n\n    # Process species sequences into concatenated one-hot vectors (aux features)\n    seq_len = len(input_ids)\n    num_species = len(species_seqs)\n\n    aux_features = np.zeros((seq_len, num_species * 5), dtype=np.float32)\n\n    for pos in range(seq_len):\n        if pos &gt;= len(human_seq):  # Handle padding case\n            break\n        for species_idx, species_seq in enumerate(species_seqs):\n            if pos &lt; len(species_seq):\n                aux_features[pos, species_idx * 5:(species_idx + 1) * 5] = one_hot_encode_base(species_seq[pos])\n\n    tokens[\"aux_features\"] = aux_features.tolist()\n    return tokens\n\n\n# --------------------------------\n# 8. Define GPNEmbedding \n# --------------------------------\nclass GPNEmbedding(nn.Module):\n    def __init__(self, config, n_species):\n        super().__init__()\n        self.config = config\n        self.n_species = n_species\n        self.vocab_size = 5  # A, T, G, C, -\n        self.species_feature_size = n_species * self.vocab_size\n\n    def forward(self, input_ids, aux_features):\n        one_hot = F.one_hot(input_ids, num_classes=self.config.vocab_size).float()\n\n        # Combine human one-hot with species aux_features\n        combined = torch.cat([one_hot, aux_features], dim=-1)\n\n        if combined.shape[-1] &lt; self.config.hidden_size:\n            pad = self.config.hidden_size - combined.shape[-1]\n            combined = F.pad(combined, (0, pad))\n\n        return combined\nFrom here on out, things are fairly standard. A transformer model (here BERT but could be anything really) is initialized to learn the relationship between adjacent tokens using a masked language model training regime. In the code below, the custom embeddings are introduced into the masked language model (ModernBert in this case) while the encoder part of the model (the core part of the model that learns the relation between adjacent tokens) remains unchanged.\n# --------------------------------\n# 6. GPNBERTMaskedLM\n# --------------------------------\n\nclass GPNBERTMaskedLM(nn.Module):\n    def __init__(self, config, n_species):\n        super().__init__()\n        self.config = config\n        self.n_species = n_species\n        self.vocab_size = 5  # A, T, G, C, -\n        self.species_feature_size = n_species * self.vocab_size\n\n        self.encoder = ModernBertModel(config)  # Directly initialize the transformer backbone\n        self.cls = nn.Linear(config.hidden_size, config.vocab_size)\n        self.embedding = GPNEmbedding(config, n_species)\n\n    def forward(self, input_ids=None, aux_features=None, labels=None, **kwargs):\n        embeddings = self.embedding(input_ids, aux_features)\n\n        # Only pass valid args to the encoder\n        encoder_kwargs = {k: v for k, v in kwargs.items() if k in {\"attention_mask\", \"position_ids\", \"head_mask\"}}\n\n        outputs = self.encoder(inputs_embeds=embeddings, **encoder_kwargs)\n\n        sequence_output = outputs.last_hidden_state\n        prediction_scores = self.cls(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n            loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        return MaskedLMOutput(\n            loss=loss,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#what-are-we-masking",
    "href": "Chapter4_DNA.html#what-are-we-masking",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.4 What are we masking?",
    "text": "4.4 What are we masking?\nI promised we’d have to deeply consider what we count as prediction, and that’s going to have to happen right now. In Chapter 2, we trained a DNA language model, and in Chapter 3, we saw how well it did and did not predict specific features. In Figure 3, you saw the model predicts the true bases in the DRD2 gene with about 40%. What if I told you I can predict the bases in the human reference genome with &gt; 95% probability with a “model” based on a supervised model? To do so, I’d just pick the consensus base across other species! Human DNA and chimpanzee DNA are &gt; 90% identical, and human and mouse genomes are remarkably similar (85%, I think). In Figure 4 (from (“Initial Sequencing and Comparative Analysis of the Mouse Genome” 2002)), we see the human sequences that are preserved well in the mouse genome, and they cover a staggering portion of it. This means we can “beat” base predictions made by our previous model by a mile, simply by picking the base that is most frequent across evolutionary history.\n\n\n\nFigure 4 The mouse genome, with sections of the human genome (color-coded) that are largely preserved across evolution (Figure 3 in (“Initial Sequencing and Comparative Analysis of the Mouse Genome” 2002))\n\n\nIn training our GPNBert model with auxiliary sequences, we train by masking the human base only (following (Benegas et al. 2023)). This very obviously and dramatically improves base prediction, and does so very quickly. After a few hundred iterations (trained on about 5,000 genes), the model learns that it should just assign the base most often found in other species. But as the evaluations in the original GPN-MSA paper make clear, eventually the model learns more than that. The model outperforms just picking the consensus base across species. They are able to show fairly convincingly that the predicted probabilities are a better predictor of the allele frequency in humans than just picking the allele frequency across species (Figure 2B in Benegas et al. (2023)) as an estimate of the allele frequency within humans. Furthermore, their model is able to identify deleterious mutations better than CADD scores, based on supervised machine learning ((Rentzsch et al. 2021)), or than sophisticated evolutionary constraint scores ((Sullivan et al. 2023)).",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#recap",
    "href": "Chapter4_DNA.html#recap",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.5 Recap",
    "text": "4.5 Recap\nIn Chapter 2, we trained a vanilla BERT on DNA sequences alone — treating DNA as just another language. That model only had access to the human sequence, with no evolutionary context.\nIn this chapter, we’ve re-imagined that process. Instead of treating A, T, G, C, - as abstract symbols, leaving the model entirely unsupervised when picking embeddings, we inject evolutionary history directly into the embedding. This allows our model to:\n\nUse the aligned species data as a rich evolutionary prior.\nStill leverage transformers for learning sequence motifs.\nPredict masked human bases using both local sequence and cross-species evolutionary patterns.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter4_DNA.html#preview-of-chapter-5",
    "href": "Chapter4_DNA.html#preview-of-chapter-5",
    "title": "4  Evolution-Aware Encoders",
    "section": "4.6 Preview of Chapter 5",
    "text": "4.6 Preview of Chapter 5\nIn Chapter 5, we will put these two models — Vanilla BERT and GPN-BERT — to the test. We will evaluate their performance on:\n\nPredicting masked bases (MLM accuracy).\nPredicting the functional impact of mutations.\n\nThis head-to-head comparison will highlight the strengths and weaknesses of each approach and show the value of embedding evolutionary context directly into genomic language models.\n\n\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2023. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” http://dx.doi.org/10.1101/2023.10.10.561776.\n\n\n“Initial Sequencing and Comparative Analysis of the Mouse Genome.” 2002. Nature 420 (6915): 520–62. https://doi.org/10.1038/nature01262.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nLupo, Umberto, Damiano Sgarbossa, and Anne-Florence Bitbol. 2022. “Protein Language Models Trained on Multiple Sequence Alignments Learn Phylogenetic Relationships.” Nature Communications 13 (1). https://doi.org/10.1038/s41467-022-34032-y.\n\n\nRentzsch, Philipp, Max Schubach, Jay Shendure, and Martin Kircher. 2021. “CADD-Spliceimproving Genome-Wide Variant Effect Prediction Using Deep Learning-Derived Splice Scores.” Genome Medicine 13 (1). https://doi.org/10.1186/s13073-021-00835-9.\n\n\nSullivan, Patrick F., Jennifer R. S. Meadows, Steven Gazal, BaDoi N. Phan, Xue Li, Diane P. Genereux, Michael X. Dong, et al. 2023. “Leveraging Base-Pair Mammalian Constraint to Understand Genetic Variation and Human Disease.” Science 380 (6643). https://doi.org/10.1126/science.abn2937.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evolution-Aware Encoders</span>"
    ]
  },
  {
    "objectID": "Chapter5_DNA.html",
    "href": "Chapter5_DNA.html",
    "title": "5  Comparing Models",
    "section": "",
    "text": "5.1 Evaluating Model accuracy (code)\nWe have trained two models with very different architectures and goals. We can now compare these models (though it remains to be seen whether we can do so fairly). The first comparison we’ll make is whether the two models accurately predict the human base. This is an entirely unfair comparison—the deck is stacked massively towards the GPN-BERT model we trained in Chapter 4, as for that model we hard-code evolutionary history in the embedding, and when masking, we only mask the human base. So we’ll do a few things to level the playing field. First, we trained the GPN model with access to only 12 out of 100 auxiliary species. As you might recall, the DNABERT model developed and trained in Chapter 2, a version of which is available on Hugging Face (MichelNivard/DNABert-CDS-13Species-v0.1), was trained on human coding sequences and those of 12 further species. At a minimum, the two models saw approximately the same amount of genomic content during training (though the content is used in different ways). Then we also evaluate the GPN model while masking the auxiliary sequences (by setting all auxiliary species bases for the focal base to “-”). This scenario is comparable to inferences on a patient’s genome where the patient has a genomic feature (a small inversion, insertion, deletion, or duplication, etc.) that doesn’t align to other species since it is novel, but we still want to infer its potential deleteriousness. Our first hypothesis is that without the help of the auxiliary sequences, the GPN model’s accuracy takes a dive. Our second hypothesis is that for bases where the human base differs from the most frequently observed ancestral base, the GPN model accuracy will take a dive, but DNABERT might not. One of these hypotheses will prove true…\nBelow is a minimal code example where we 1. load the two models and write a helper function (get_predictions) that evaluates the likelihood of a given base in both models. This function can then be repeatedly applied to generate all kinds of comparisons. The full (and somewhat verbose) code for those evaluations is found in the script Chapter5_competative_eval.py available on GitHub: https://github.com/MichelNivard/Biological-language-models/tree/main/scripts/DNA/Chapter_5\nThe important skill we learn here is to load two models, both trained on slightly different datasets, with slightly different DNA tokenizers, and apply them both to a single dataset, so we can make a direct comparison.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "Chapter5_DNA.html#evaluating-model-accuracy-code",
    "href": "Chapter5_DNA.html#evaluating-model-accuracy-code",
    "title": "5  Comparing Models",
    "section": "",
    "text": "# --------------------------------\n# 1. Competative evaluations!!\n# --------------------------------\n\n# Load GPN-enhanced ModernBERT (your custom model)\ndevice = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n\nmodel_gpn = torch.load(\"./bert-dna-gpn/gpn_bert_model.pt\") # Assuming it's already trained and loaded\ntokenizer_gpn = AutoTokenizer.from_pretrained(\"./bert-dna-gpn\")\n# Load the full model\nmodel_gpn.eval()\n\n# Load DNABert-CDS-13Species\nmodel_name_dnabert = \"MichelNivard/DNABert-CDS-13Species-v0.1\"\ntokenizer_dnabert = AutoTokenizer.from_pretrained(model_name_dnabert)\nmodel_dnabert = AutoModelForMaskedLM.from_pretrained(model_name_dnabert).to(device)\nmodel_dnabert.eval()\n\n# Helper to get vocab mapping\nid_to_token_gpn = {v: k for k, v in tokenizer_gpn.get_vocab().items()}\nid_to_token_dnabert = {v: k for k, v in tokenizer_dnabert.get_vocab().items()}\n\n\n# Helper function to get predictions of the same base for both models it requires all the model-specific elements, the models themselves, attention masks, tokenizers, tokenized inputs, for both models.\n\ndef get_predictions(pos, input_ids_gpn, attention_mask_gpn, aux_features,\n                   input_ids_dnabert, attention_mask_dnabert,\n                   model_gpn, model_dnabert, tokenizer_gpn, tokenizer_dnabert,\n                   device):\n    \"\"\"Helper function to get predictions from both models for a specific position\"\"\"\n    \n    # Mask the position in both models\n    masked_input_ids_gpn = input_ids_gpn.clone()\n    masked_input_ids_gpn[0, pos] = tokenizer_gpn.mask_token_id\n    \n    masked_input_ids_dnabert = input_ids_dnabert.clone()\n    masked_input_ids_dnabert[0, pos] = tokenizer_dnabert.mask_token_id\n    \n    # Get predictions from GPN\n    with torch.no_grad():\n        output_gpn = model_gpn(\n            input_ids=masked_input_ids_gpn,\n            attention_mask=attention_mask_gpn,\n            aux_features=aux_features\n        )\n        logits_gpn = output_gpn.logits\n        log_probs_gpn = torch.log_softmax(logits_gpn[0, pos], dim=-1)\n        \n        # Get predictions from DNABERT\n        output_dnabert = model_dnabert(\n            masked_input_ids_dnabert,\n            attention_mask=attention_mask_dnabert\n        )\n        logits_dnabert = output_dnabert.logits\n        log_probs_dnabert = torch.log_softmax(logits_dnabert[0, pos], dim=-1)\n    \n    # Get top predictions\n    top_preds_gpn = torch.topk(log_probs_gpn, k=4)\n    top_preds_dnabert = torch.topk(log_probs_dnabert, k=4)\n    \n    return {\n        'gpn_probs': top_preds_gpn,\n        'dnabert_probs': top_preds_dnabert\n    }",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "Chapter5_DNA.html#evaluating-model-accuracy-results",
    "href": "Chapter5_DNA.html#evaluating-model-accuracy-results",
    "title": "5  Comparing Models",
    "section": "5.2 Evaluating Model accuracy (results)",
    "text": "5.2 Evaluating Model accuracy (results)\nWhen evaluating up to 500 bases (first 500) for the first 30 genes, and all bases where the human base doesn’t match the most frequently observed ancestral base across 100 species we can generate model accuracy stats. Here we use the mean number of bases correctly called by the model (where called means the model assigns the true human references base the highest probability when predicting it.). As expected the GPN based Bert model we trained, which has access to the evolutionary history of the base, blows vanilla DNA Bert out of the water. This isn’t unexpected though, very large portions of the coding sequences are conserved across species, and training on human species and 12 aligned species, where those other species are available at inference time just makes prediction VERY easy. However as per our first hypothesis the GPN model doesn’t do well if we mask the bases evolutionary history. In those cases, it’s bested by DNA Bert, which is trained to predict sequences without explicit evolutionary history and so does a reasonable job at it.\n# 1. Overall Accuracy Bar Plot\nplt.figure(figsize=(10, 6))\naccuracies = {\n    'GPN (normal)': df['gpn_normal_correct'].mean(),\n    'DNABERT': df['dnabert_normal_correct'].mean(),\n    'GPN (no species)': df['gpn_missing_correct'].mean()\n}\nplt.bar(accuracies.keys(), accuracies.values(), color=['blue', 'green', 'red'])\nplt.title('Model Accuracy Comparison')\nplt.ylabel('Accuracy')\nplt.ylim(0, 1)\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\nFigure 1: Model prediction accuracy compared across a common set of coding sequences of 30 genes (up to ±15k bases). GPN (no species) are predictions by the GPN model but with other species base masked (changed to “-” , the missing/sequences gap token) during prediction.\n\n\nI had expected that if considering bases where the human references base differs from the majority of the other species bases, and the base is highly conserved (&gt; 75%) then GPN models might be thrown off, and perhaps DNA Bert would not? It appears though (See Figure 2) that all models drop in accuracy, but DNA Bert drops more than the GPN model! So our second hypothesis wasn’t confirmed. Now this is a textbook-like document, these models are under-trained, we only evaluated the first 500 bases in 30 genes, within them there are only a few hundred bases where the human and dominant ancestral base differ, so please don’t generalize any of these conclusions!\n\n\n\nFigure 2: The model prediction accuracy for bases where the human base differs from the base most frequently observed in other species.\n\n\nFinally, we can plot model accuracy as a function of “conservation score” which is an ad-hoc measure of the % of valid bases (so A, T, C, G) across species that is the dominant most frequent) base. You could argue a more comprehensive conservation score might account for the number of species where the part of the sequences could not be aligned (e.g. the percentage of “-” bases), and that might be a great exercise for you!\n# 4. Conservation Score vs Accuracy\nplt.figure(figsize=(12, 6))\nconservation_bins = np.linspace(0, 1, 11)\ndf['conservation_bin'] = pd.cut(df['conservation_score'], bins=conservation_bins)\n\nfor model, color in zip(['gpn_normal', 'dnabert_normal', 'gpn_missing'], ['blue', 'green', 'red']):\n    accuracy_by_conservation = df.groupby('conservation_bin')[f'{model}_correct'].mean()\n    plt.plot(conservation_bins[:-1] + 0.05, accuracy_by_conservation, \n            marker='o', label=model.replace('_', ' ').title(), color=color)\n\nplt.title('Model Accuracy vs Conservation Score')\nplt.xlabel('Conservation Score')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "Chapter5_DNA.html#discussion-of-results",
    "href": "Chapter5_DNA.html#discussion-of-results",
    "title": "5  Comparing Models",
    "section": "5.3 Discussion of Results",
    "text": "5.3 Discussion of Results\nOur comparative analysis of the GPN-BERT and DNABERT models reveals several key insights:\n\nBase Prediction Performance:\n\nGPN-BERT with evolutionary context significantly outperforms DNABERT in standard conditions\nWhen evolutionary information is masked, GPN-BERT’s performance drops below DNABERT\nBoth models show reduced accuracy when predicting bases that differ from the ancestral consensus\n\nConservation Score Impact:\n\nHigher conservation scores correlate with better prediction accuracy across all models\nThe relationship between conservation and accuracy appears to be non-linear\nGPN-BERT maintains a performance advantage even at lower conservation levels\n\nModel Architecture Trade-offs:\n\nGPN-BERT’s superior performance comes at the cost of requiring cross-species alignment data\nDNABERT shows more robust performance when evolutionary context is unavailable\nThe results suggest potential benefits in combining both approaches\n\n\nThese findings align with our project’s focus on comparing these two DNA language models, particularly in their ability to handle positions where human sequences differ from ancestral sequences. The analysis of prediction ranks and base-by-base accuracy provides valuable insights into each model’s strengths and limitations.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "Chapter5_DNA.html#next-steps",
    "href": "Chapter5_DNA.html#next-steps",
    "title": "5  Comparing Models",
    "section": "5.4 Next Steps",
    "text": "5.4 Next Steps\nIn the upcoming chapters, we will: 1. Explore hybrid architectures that combine the strengths of both models 2. Evaluate performance on specific mutation types 3. Investigate the relationship between conservation patterns and prediction accuracy 4. Consider practical applications in genomic research and clinical settings\nThe code and detailed analysis for all experiments are available in the project repository, allowing for reproduction and extension of these results.\n\n\n\n\nMarin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen, Dennis Pultz, Ole Winther, and Wouter Boomsma. 2023. “BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks.” https://doi.org/10.48550/ARXIV.2311.12570.\n\n\nPatel, Aman, Arpita Singhal, Austin Wang, Anusri Pampari, Maya Kasowski, and Anshul Kundaje. 2024. “DART-Eval: A Comprehensive DNA Language Model Evaluation Benchmark on Regulatory DNA.” https://doi.org/10.48550/ARXIV.2412.05430.\n\n\nTang, Ziqi, Nirali Somia, Yiyang Yu, and Peter K Koo. 2024. “Evaluating the Representational Power of Pre-Trained DNA Language Models for Regulatory Genomics.” http://dx.doi.org/10.1101/2024.02.29.582810.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "Chapter6_DNA.html",
    "href": "Chapter6_DNA.html",
    "title": "6  A Review of Current DNA Language Models",
    "section": "",
    "text": "6.1 Modeling Paradigms\nAs we learned in the first 5 chapters, gLMs (genomic Language Models) borrow from natural language processing by treating DNA sequences as “text” composed of four characters (A, C, G, T). Early models used k-mer tokenization (e.g., DNABERT), but newer approaches experiment with both nucleotide‐level and subword tokenizations (such as 3-mer or 6-mer) to better capture biological semantics.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Review of Current DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter6_DNA.html#architectural-innovations",
    "href": "Chapter6_DNA.html#architectural-innovations",
    "title": "6  A Review of Current DNA Language Models",
    "section": "6.2 Architectural Innovations",
    "text": "6.2 Architectural Innovations\nThere are two specific architectural innovations in DNA language models worth discussing. The first is GPA-MSA, the core idea behind a model we discussed in Chapter 4. In this model, the trainable embedding layer is replaced with a biologically informed deterministic embedding that reflects the evolutionary history of the genome at a given base.\nA second innovation was necessitated by the need to model long-range dependence in DNA (changes to DNA can have effects over thousands of bases “downstream”). While transformer-based models initially dominated the field, their quadratic scaling with sequence length has prompted the development of more efficient architectures. Models such as HyenaDNA extend context lengths up to 1 million tokens at single-nucleotide resolution, and hybrid architectures like HybriDNA combine transformers with selective state-space models (Mamba2) to process sequences up to 131 kilobases. Omni-DNA and GENERator further illustrate the trend toward unified, cross-modal genomic foundation models capable of multitask learning.\nBelow is a summary table of several prominent DNA language models with innovative architectures, along with links to their corresponding paper and GitHub (or related resource) repositories:\n\n\n\nModel\nDescription\nPaper Link\nGitHub / Resource Link\n\n\n\n\nDNABERT\nA transformer-based model that learns bidirectional representations from DNA sequences using k-mer tokenization.\nDNABERT Paper\nGitHub\n\n\nNucleotide Transformer (NT‑v2)\nA large transformer pretrained on massive human genomic data to learn robust DNA representations for various downstream tasks.\nNucleotide Transformer v2\nGitHub/HuggingFace\n\n\nGPN\nThe Genomic Pre-trained Network that leverages unsupervised DNA language modeling to predict genome-wide variant effects.\nGPN Paper (PNAS 2023)\nGitHub\n\n\nGPN-MSA\nThe Genomic Pre-trained Network that leverage multiple sequence alignment across species to develop specialized evolution-aware token embedding.\nGPN-MSA preprint\nGitHub\n\n\nHyenaDNA\nA long-range genomic language model operating at single-nucleotide resolution using Hyena’s implicit convolutional approach to overcome transformer scaling issues.\nHyenaDNA (arXiv)\nGitHub\n\n\nHybriDNA\nA hybrid model combining Transformer and Mamba2 (state-space) architectures for efficient long-range DNA modeling.\nHybriDNA (arXiv)\n\n\n\nOmni‑DNA\nA unified genomic foundation model that supports cross-modal and multi-task learning across a wide range of genomic applications.\nOmni‑DNA (arXiv)\nHugging Face Collection\n\n\nGENERator\nA long-context generative genomic foundation model designed for sequence generation and optimization tasks with a context length of up to 98k bp.\nGENERator (arXiv)\nGitHub\n\n\nEvo 2\nEvo 2 is a state of the art DNA language model (available in 1B, 7B and 40B sizes) for long context modeling and design. Evo 2 models DNA sequences at single-nucleotide resolution at up to 1 million base pair context length using the StripedHyena 2architecture. Evo 2 was pretrained using Savanna. Evo 2 was trained autoregressively on OpenGenome2, a dataset containing 8.8 trillion tokens from all domains of life.\nPreprint\nGithub Hugging Face collection\n\n\n\nThis table highlights each model’s core features and provides direct access to the publication and code repository (or resource page) where available.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Review of Current DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter6_DNA.html#applications",
    "href": "Chapter6_DNA.html#applications",
    "title": "6  A Review of Current DNA Language Models",
    "section": "6.3 Applications",
    "text": "6.3 Applications\nThese models have demonstrated state-of-the-art performance across multiple downstream tasks including: Variant Effect Prediction: Unsupervised approaches can predict deleterious mutations by modeling the “grammar” of the genome. Regulatory Element Identification: By learning long-range interactions, gLMs help detect promoters, enhancers, and other regulatory motifs. Sequence Generation and Protein Tasks: Some models generate synthetic regulatory sequences or transform coding DNA into meaningful protein representations, bridging genomics and proteomics.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Review of Current DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Chapter6_DNA.html#challenges-and-future-directions",
    "href": "Chapter6_DNA.html#challenges-and-future-directions",
    "title": "6  A Review of Current DNA Language Models",
    "section": "6.4 Challenges and Future Directions",
    "text": "6.4 Challenges and Future Directions\nThere are some interesting immediate challenges that become apparent form the literature. One very obvious one is a better grasp of the relation between training data (multi-species sequences vs intra-human variation) and model performance. The nucleotide transformer paper (Dalla-Torre et al. 2024) highlights how as things stand training on human sequences does not improve the model over multi-species training. Its obviously true that most bases are identical for most people, and so the human training data has low variability, which might adversely impact the model. It could also be that for the specific validation tests evolutionary constraint convey’s more information than the limited human variation in 1000-genomes.\none specific avenue for exploration could therefore be to deeply consider the order in which training data is presented (first train across species, then train within humans) and the effect of learning rate on specific segments of training. YOu could imagine training at a high learning rate using the high variance intra-species data and then train at a lower learning rate with the low variance sequences, only modestly updating the model in that phase. Alternatively you could consider “low rank training”, where the model is first trained on multi-species data and then only fine-tuned on human data, restricting the degrees of freedom in that second phase trough low rank matrix approximation (LoRa)(Hu et al. 2021) which learns less, but also forgets less from previous training epochs(Biderman et al. 2024).\nIn any field that receives outsized attention (and I feel we can conclude AI is currently such a field) its always critical to evaluate innovations. There is a growing literature around DNA language model evaluation you should familiarize yourself with if you are going to evaluate these models for academic or industry use (Tang et al. 2024; Patel et al. 2024; Marin et al. 2023). For specific tasks its good to continually evaluate whether DNA language models are overkill. Does your model outperform alpha-missense, for which the scores are already available? How does it fair against older, and computationally likely lighter, supervised models like CADD(Schubach et al. 2024)? Don’t just trust the original authors, they are biased (we all are) consider independent evaluations, like for example: (Ljungdahl et al. 2023).\n\n6.4.1 Key challenge: Do these models memorize sequences, or learn biology?\nGenomic language models have shown remarkable ability to capture statistical patterns in DNA sequences, yet recent analyses underscore a fundamental limitation: these models often rely on memorizing recurring motifs rather than internalizing the complex regulatory “grammar” of the genome.Some critique is focused on the fact that simple evaluation might not distinguish between memorization and understanding or learning(Consens et al. 2025). In Chapter 4 we discussed language models that lean on Multiuple Sequence alignment, and while these arguably do very well at some task, these clearly memorize the value of the MSA. A recent critical analytical review tries to empirically illustrate that DNA language models more broadly just memorizing (Hassan et al. 2025) by assessing the models relative performance on species represented well or poorly inside the training datasets.\nThis tendency toward memorization becomes especially apparent when examining early gLM architectures such as DNABERT and GROVER. Research on DNABERT revealed that performance gains often stem from recalling frequent k-mer patterns in the training corpus, rather than modeling the long-range dependencies essential for regulatory element function(Consens et al. 2025). Similarly, (Consens et al. 2025) note, benchmarks relying on distinguishing real from randomly generated sequences predominantly are tests of motif frequency and fail to probe higher-order sequence logic.\nTaken together, these critiques argue that merely increasing parameter counts or context windows will not suffice to endow DNA language models with a genuine understanding of genomic “laws.” Instead, both works call for integrating evolutionary constraints, structural information, and explicit functional representations into model design. (Hassan et al. 2025) emphasizes that overcoming inherent architectural limitations might require paradigms beyond autoregressive next-token prediction (The authors do not go into how CLM and MLM might differ in this respect). While (Consens et al. 2025) urges the development of biologically meaningful benchmarks that can distinguish true comprehension from pattern memorization Without such advances, DNA language models risk remaining powerful pattern matchers—valuable for certain tasks but ultimately limited in their ability to unveil the deep regulatory principles critical for biomedical discovery.\n\n\n\n\nBenegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun S. Song. 2025. “Genomic Language Models: Opportunities and Challenges.” Trends in Genetics, January. https://doi.org/10.1016/j.tig.2024.11.013.\n\n\nBiderman, Dan, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, et al. 2024. “LoRA Learns Less and Forgets Less.” https://doi.org/10.48550/ARXIV.2405.09673.\n\n\nConsens, Micaela Elisa, Ben Li, Anna R. Poetsch, and Stephen Gilbert. 2025. “Genomic Language Models Could Transform Medicine but Not Yet.” Npj Digital Medicine 8 (1). https://doi.org/10.1038/s41746-025-01603-4.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2024. “Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.” Nature Methods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nHassan, Hassan, Kyle Puhger, Ali Saadat, Johannes Mayer, and Maximilian Sprang. 2025. “Life as a Function: Why Transformer Architectures Struggle to Gain Genome-Level Foundational Capabilities.” http://dx.doi.org/10.1101/2025.01.13.632745.\n\n\nHu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. “LoRA: Low-Rank Adaptation of Large Language Models.” https://doi.org/10.48550/ARXIV.2106.09685.\n\n\nLjungdahl, Alicia, Sayeh Kohani, Nicholas F. Page, Eloise S. Wells, Emilie M. Wigdor, Shan Dong, and Stephan J. Sanders. 2023. “AlphaMissense Is Better Correlated with Functional Assays of Missense Impact Than Earlier Prediction Algorithms.” http://dx.doi.org/10.1101/2023.10.24.562294.\n\n\nMarin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen, Dennis Pultz, Ole Winther, and Wouter Boomsma. 2023. “BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks.” https://doi.org/10.48550/ARXIV.2311.12570.\n\n\nPatel, Aman, Arpita Singhal, Austin Wang, Anusri Pampari, Maya Kasowski, and Anshul Kundaje. 2024. “DART-Eval: A Comprehensive DNA Language Model Evaluation Benchmark on Regulatory DNA.” https://doi.org/10.48550/ARXIV.2412.05430.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. “CADD V1.7: Using Protein Language Models, Regulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.” Nucleic Acids Research 52 (D1): D1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nTang, Ziqi, Nirali Somia, Yiyang Yu, and Peter K Koo. 2024. “Evaluating the Representational Power of Pre-Trained DNA Language Models for Regulatory Genomics.” http://dx.doi.org/10.1101/2024.02.29.582810.",
    "crumbs": [
      "DNA models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Review of Current DNA Language Models</span>"
    ]
  },
  {
    "objectID": "Scaling_training.html",
    "href": "Scaling_training.html",
    "title": "Scale up Training",
    "section": "",
    "text": "Don’t Try to Win the Compute Race\nThis chapter isn’t a part of the “DNA” section of the book, because the lessons are really quite general, but it comes after because we needed a little bit of experience with language model training before even considering training a serious model. This is also a somewhat awkward chapter for me to write, especially for the part of the readership that has a background in ML. See, I am a psychologist by training (though I have worked in genetic epidemiology for years and years), and while a lot of my academic work is fairly computational, I am not an expert in language model scaling by any means! Remember, the preamble to the book explains that this book is an account of me learning about biological language models and taking others along for the ride, not an authoritative text!\nAmong the DNA models I could find on Hugging Face are 7B parameter models like https://huggingface.co/genbio-ai/AIDO.DNA-7B. AIDO is trained on “256 H100 GPUs” in “8 days”. The training data consisted of 10.6 billion bases. That’s not even a particularly large model in the grand scheme of things, but if you consider a cost of approximately $2 per hour per H100, you are going to spend $100k. Obviously, there are academic compute resources you can get access to by appointment or based on fair use at your institute, university, or through collaborative national infrastructure, but even those are finite.\nYou have to consider feasibility. Today (March 2025), the Dutch national computer cluster for research (Snellius at SURF Sara) has 88 nodes with 4 H100 GPUs and 72 nodes with 4 A100 GPUs. TACC, the University of Texas at Austin compute provider, has approximately 80 A100 nodes (each with 3 GPUs). Those are two examples of reasonably well-funded HPC providers in academia. In my experience, you could get time reserved for your research at your local academic HPC provider at steep discounts, and these systems are likely large enough to train that 7B model I linked to. However, note how on either TACC or Snellius, 256 GPUs for 8 days would block the entire system for over a week. Perhaps you could apply for access to larger national research clusters, like Isambard-AI in the UK (being built in Bristol right now, a motivation for me to write this) which has 5,000 H100 GPUs. However, in general, it is likely you are going to be relatively limited by compute resources. Don’t be discouraged though—most breakthroughs are not going to be compute-based, and there are immense efficiency gains to be made that will level the playing field.",
    "crumbs": [
      "Scale up Training"
    ]
  },
  {
    "objectID": "Scaling_training.html#focus-on-a-specific-question",
    "href": "Scaling_training.html#focus-on-a-specific-question",
    "title": "Scale up Training",
    "section": "Focus on a specific question",
    "text": "Focus on a specific question\nIt is tempting to train a model on all DNA in the known universe, but honestly, there is actually more DNA than people have even started training on. The models discussed so far often train on the reference sequence/genome, a sort of modal or consensus genome, but individual people’s genomes are different from that reference. You could consider thousands of species, or (tens/hundreds of) thousands of individual human genomes. That would require a lot of bioinformatics, which if your background is in bio might set you apart from other ML/AI researchers. You’d have to phase the genomes to untangle the maternal and paternal strand, you’d have to decide whether you want to get rid of the reference entirely and build a specific reference/genome for each individual, you might require some reference, or a graph genome? It’s also worth considering whether your task really requires the whole genome. Are you performing gene-centric tasks (mutation consequence prediction, gene expression prediction, alternative splice modeling)? If your specific tasks don’t require the whole genome, why not consider training on coding sequences only or the sequences of genes and a few thousand bases around them?",
    "crumbs": [
      "Scale up Training"
    ]
  },
  {
    "objectID": "Scaling_training.html#smart-architectures",
    "href": "Scaling_training.html#smart-architectures",
    "title": "Scale up Training",
    "section": "Smart Architectures",
    "text": "Smart Architectures\nIn Chapter 4, we studied smarter, DNA-specific model architectures. The GPN model inspired by Benegas et al. (2023) that we introduced can outperform a standard BERT in an hour of training on my 2022 MacBook Air (the BERT we trained and compared to our GPN-BERT trained for approximately 8 hours on a strong GPU). The massive efficiency gain may mean you can beat the 7B BERT-like model we took as an example of compute costs with a fraction of the compute! As briefly remarked on in Chapter 6, researchers have designed alternatives for the transformer module in order to expand its context window up to 1 million bases, with far less compute requirement than the transformer (Nguyen et al. 2023). If you are to design and run your own model, it will likely pay off if you implement some of these architectural innovations.",
    "crumbs": [
      "Scale up Training"
    ]
  },
  {
    "objectID": "Scaling_training.html#get-most-out-of-your-gpu",
    "href": "Scaling_training.html#get-most-out-of-your-gpu",
    "title": "Scale up Training",
    "section": "Get most out of your GPU",
    "text": "Get most out of your GPU\nThere is a healthy culture of extreme optimization. A good early example of this is the paper “Craamming: training a language model on a Single GPU in a Day” (Geiping and Goldstein 2022). Other neat examples are this GitHub repo of repeated attempts at training a GPT-2 equivalent (the OG OpenAI model that sort of set the LLM hype cycle in motion) as fast as possible (now in under 3 minutes on 8 H100 GPUs) (https://github.com/KellerJordan/modded-nanogpt). Some of the innovation people made cramming these models won’t generalize to your model, but consider giving their Muon optimizer a go (for GPT-2 it’s a serious efficiency gain), spend time finding optimal learning rates, or consider spending a few extra days/weeks cleaning data. If you do all these things before your big run, it’ll save some serious compute, which means you can push more data through the model in the same compute budget.\n\nOptimisation steps anyone should take\nOptimization doesn’t have to be a full-time job though, there are some easy steps anyone can take to get more training out of the same hardware. Full writeups on simple optimizations are found here, here and here. But I’ll cover the low-hanging fruit right here. Follow these steps and you’ll likely get nearly the same results in half the compute.\n\n\nBatch processing\nTransformers are designed with batch processing in mind. All the weight matrices have an extra (second or third, depending on which matrix) dimension to hold multiple sequences in parallel and apply the training step over all of them. We can very easily change the batch size in the training arguments:\ntraining_args = TrainingArguments(..., \n                                  per_device_train_batch_size=16,\n                                  ...,)\nIf you increase the batch size too much, you’ll have a crash and an out-of-memory warning:\nRuntimeError: CUDA out of memory. Tried to allocate X MiB ...\nWe do so because many elements of a GPU, tensor core, shader units, CUDA cores come in powers of 2, and if your batch is 17, and you happen to have 16 tensor cores (or whatever element in the stack), that means processing 16, then 1, then 16 etc. You can go all the way until you run out of memory, but I wouldn’t. Training is bound by the limits of compute (as of writing, perhaps NVIDIA or AMD innovates rapidly in 2025, and this might change). So, once you find a batch size that hits 100% GPU use during training (you can check with the nvidia-smi command line tool or rocm-smi for AMD GPUs).\n\n\nLower numerical precision (quantize)\nLower numerical precision. Numbers are stored in 32-bit, which effectively means you have 6-9 significant digits, and the number can be zero or can range from \\(-3.4*10^{38}\\) to \\(-1.2*10^{-38}\\), or from \\(1.2*10^{-38}\\) to \\(3.4*10^{38}\\). It’s not entirely uncommon for scientific computations to run into numbers that can’t be represented well in 32-bits, but in order to speed up large models, people have actually gone down to 16-bit numbers.\nIn transformers training arguments you can specify 16-bit training, but with 32-bit parameter accumulation (storage) of results where higher precision is needed, so-called mixed precision training with a simple command:\ntraining_args = TrainingArguments(..., \n                                  fp16=True,\n                                  ...,)\nOr if you have a GPU capable of it (and most are) you can use the more efficient bf16 mixed precision, which has worse precision than fp16 but more dynamic range (can represent a larger range of values).\ntraining_args = TrainingArguments(..., \n                                  bf16=True,\n                                  ...,)\nFinally, on NVIDIA GPUs, you can use some serious dark magic: tf32, which is actually a 19-bit number (it drops precision but keeps dynamic range), which for most purposes is as precise as fp32. In many versions of PyTorch and transformers, tf32 is automatically enabled. But, if you work on a cluster with older versions of PyTorch pre-compiled, you can manually activate tf32, and you can combine it with bf16 for mixed precision training:\nimport torch\ntorch.backends.cuda.matmul.allow_tf32 = True\n\ntraining_args = TrainingArguments(..., \n                                  bf16=True,\n                                  tf32=True,\n                                  ...,)\nThe combination of bf16 and tf32 can result in 4x to 12x speedups, though as tf32 is often already activated as a default the advantage is baked in. There are ways to take this further, use 8-bit numerical representations on modern hardware. The advantage of 8-bit models is greatest for really big models, if you have the means and skills to train those kinds of models, you have no need for this book.\nFinal recommendation: use bf16 with tf32 unless you are training models that are so large as to require 8-bit data formats, unheard of in biological deep learning so far.\n\n\nOptimizer choice\nWe haven’t really spoken about the optimizer itself, the default optimizers used these days are almost always variations of adam (Kingma and Ba 2014). Adam, and a modern implementation of it, like adamW are workhorses. Because they store a rolling average of recent gradients, they are robust if the gradients are very noisy. Because they don’t rely on 2nd order derivatives they are able to deal with very large models, and they are relatively efficient (when optimizing a model with adamW we store 8 bits per parameter). Papers that promise to beat adamW, to then fail, are a bit of a running gag in machine learning. However, you can in fact do better, both in terms of speed and in terms of memory utilization.\nadafactor: adafactor is slower, but far more memory efficient. Where adam stores rolling averages of the gradients of weight matrices, adafactor stores row and column averages of those, meaning it only requires 4 bytes per parameter, significantly reducing the memory usage. This optimizer is a drop-in replacement and while slightly less efficient (takes longer to get to the same minimum) a great option if you are short on memory.\ntraining_args = TrainingArguments(..., \n                                  optim=\"adafactor\", \n                                  ...)\npaged_adam_8bit: Bits and Bytes is a library that deeply compresses certain optimizer states to 8-bits during training and actually even further during fine-tuning or inferences. It’s less of a direct drop-in replacement but it’s fast, almost as fast as adamw. If you are really memory-bound (say you have a 12Gb or 24Gb GPU but bigger ambitions) then this can be an option. The integration section of the manual is found here.\nmuon: You can refer to this writeupfor details of the optimizer. It’s a bit of a weird optimizer, as it’s meant ONLY for the inner layers of a transformer, the first and the last layer are still optimized with adamW. This also means you’ll need to write your own code, this certainly isn’t a simple case of dropping an argument into Trainer. The advantage of muon is that it’s both faster, and the loss drops more steeply. In other words, it learns more per token than other optimizers. This is actually very relevant from protein and DNA language models. As we’ll learn in the next few chapters on protein language models, there is way less usable data in the biological sphere than there is for natural language models. Facebook LLama is trained on 15 Trillion tokens, the largest protein language models on 780 billion tokens, which required including massive amounts of meta-genomic, and synthetic protein data. We are running out of data, and considering an optimizer that squeezes a little more out of each amino-acid might be worth your time! To implement moun you’d need to apply it to all 2D layers, and apply a separate optimizer to all other (1D) layers, from the GitHub:\n# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.90, 0.95), weight_decay=0.01)\nfrom muon import Muon\n\n\n# Find ≥2D parameters in the body of the network -- these should be optimized by Muon\nmuon_params = [p for p in model.body.parameters() if p.ndim &gt;= 2]\n# Find everything else -- these should be optimized by AdamW\nadamw_params = ([p for p in model.body.parameters() if p.ndim &lt; 2]\n              + [*model.head.parameters(), *model.embed.parameters()])\n\n\n# Create the optimizer\noptimizers = [Muon(muon_params, lr=0.02, momentum=0.95, rank=0, world_size=1),\n              torch.optim.AdamW(adamw_params, lr=3e-4, betas=(0.90, 0.95), weight_decay=0.01)]\n...\n\n# in the training step\nfor opt in optimizers:\n    opt.step()\nThe authors mention that muon isn’t tested for fine-tuning, and they don’t think it’ll work well with small batches.\nFinal recommendation: Stick with adamW unless you are training a mid-size (&gt; 300m parameters) foundational model and risk running out of training data.",
    "crumbs": [
      "Scale up Training"
    ]
  },
  {
    "objectID": "Scaling_training.html#parallel-training",
    "href": "Scaling_training.html#parallel-training",
    "title": "Scale up Training",
    "section": "Parallel Training",
    "text": "Parallel Training\nAfter you have squeezed every drop out of your GPUs, the next performance step is training a model on multiple GPUs. Huggingface has a great read-me on training across multiple GPUs you should check out, below I cover the basics. If you stick with Huggingface transformers library it’s actually quite simple! Their accelerate library enables us to launch a single script across multiple GPUs in one machine, or even multiple GPUs in multiple machines. It’s truly as simple as writing a training script (say train.py) and launching it with the accelerate command line tool from the terminal:\naccelerate launch --multi-gpu train.py\nIf your model is a model that comes with the Huggingface transformers library this command should just find the GPUs, and get going. Though you’ll very likely want a bit more control. In my case, for example, this command kept trying to start a job on my 2 GPUs, AND on the internal GPU in the computer’s CPU package… That built-in GPU is of a different nature altogether and would only slow things down, so I had to specify the specific two GPUs I wanted to use:\nCUDA_VISIBLE_DEVICES=\"0,1\" accelerate launch {script_name.py} \nThis level of control can also come in handy if you want separate GPUs to run separate experiments. It is remarkable how plug-and-play accelerate really is. If you have one machine with multiple GPUs, and your model fits in the GPUs’ memory. In a few test runs on a cloud machine with 8 A100 GPUs, or 8 H100 GPUs I got my Protein language model running across 8 GPUs, munging through 500+ proteins a second (so ±500,000 tokens a second) without any script modifications.\n\nDifferent kinds of parallel\n\ndata parallelism\nThere are a few different kinds of parallelism. There are data parallel (DP) and distributed data parallel (DDP), where each GPU has a copy of the model, and a unique batch of data (this is why the trainer argument is: per_device_train_batch_size each GPU gets a batch) and the loss, and gradients are averaged or pooled across GPUs after each training step.\nData parallelism requires your model, and a reasonable batch of data to fit on each GPU. If they do, then DDP and DP are the most straightforward option. If you model doesn’t fit your GPU (and large models frequently won’t!) you’ll need a different strategy. If you need to pick between DP and DDP consider that DP is less communication-heavy but slightly less optimal. So if you have 4 GPUs on a slow motherboard PCIe connection, I’d go with DP, but if you have the GPUs linked via NVlink or similar high-speed card-to-card connections, then DDP might be faster. You could inquire which might be best for you with your HPC team, but I find it easier to just try DDP and DP and see which is fastest over a 100-500 step trial run.\nThe most advanced version of data parallelism is “Fully sharded data parallelism” where a copy of the model isn’t kept on each GPU but the model is distributed across all GPUs, minimizing memory use, but increasing communication overhead. It’s a great option for large models on modern GPUs with fast (nvlink) interconnects) you can read more about its implementation in accelerate here.\n\n\nmodel/tensor parallelism\nOnce a model exceeds the size of the GPU, you have no choice but to distribute layers of the some across multiple GPUs. you could do so naively, say layer 1-4 GPU1, 5-8 GPU2 etc etc. The GPUs would have to wait for the GPU before it to finish, meaning you’d have a log of GPUs idling waiting for layers to be called on. But usually, there are smarter ways to pack things, companies like Google pioneered Gpipe. There are Pytorch and Huggingface tutorials on model parallel training.\nFinal recommendation: If you are in the audience for this book, then you might be ready for training, or fine-tuning a model in the 200m to 600m parameter range. You should be able to do that on a single machine with 4 or ideally 8 powerful GPUs. Stick with data parallel strategies that are easy to implement while you figure out the thousands of other choices you need to make to arrive at an optimal model. The reason to work your way up from smaller to larger models. The reason to start with smaller models is that there are always new surprises behind each corner. I just trained a protein language model where the data become progressively richer for mammalian proteins, and considered increasingly similar proteins. I thought that would enhance some application on human proteins, but it didn’t in a 45 million parameter and then 70 million parameter model. Had I trained a 300m parameter model on commercial hardware as I intended to without testing, I would have been out $1500 and would have ended up with a “meh” model at best. I did the math on training a 150M or 300M model on 8Xa6000 GPUs, or 8XA100 GPUs. In ~40 hours, and using the suggestions outlined above, you could train a near cutting-edge DNA/protein language model on one of these machines, and more powerful 8xH100 machines are coming online everywhere. Distributed data parallel training across 2,4 or 8 GPUs in one machine should be sufficient for the kind of learning you need to do at this stage, once you are ready to scale further there are serious resources.",
    "crumbs": [
      "Scale up Training"
    ]
  },
  {
    "objectID": "Scaling_training.html#aim-for-the-stars..",
    "href": "Scaling_training.html#aim-for-the-stars..",
    "title": "Scale up Training",
    "section": "Aim for the stars..",
    "text": "Aim for the stars..\nShould you get to the point where you exceed what’s feasible on a single very powerful machine with 8 GPUs, then you can move to a cluster. Tools like accelerate will work across multiple machines, and that is likely sufficient compute for almost anyone. But, if you ever get to this stage it’s good to read up on “ultra-scale’ training there are two great manuals you should flick through.\nGoogle deep mind wrote a guide to scaling large language models on TPU/Jax, but it covers all the math and concepts you’ll need in any framework in great detail.\nHuggingface wrote an interactive playbook on ultra-scale training which is more practical.\n\n\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2023. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” http://dx.doi.org/10.1101/2023.10.10.561776.\n\n\nGeiping, Jonas, and Tom Goldstein. 2022. “Cramming: Training a Language Model on a Single GPU in One Day.” https://doi.org/10.48550/ARXIV.2212.14034.\n\n\nKingma, Diederik P., and Jimmy Ba. 2014. “Adam: A Method for Stochastic Optimization.” https://doi.org/10.48550/ARXIV.1412.6980.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” https://doi.org/10.48550/ARXIV.2306.15794.",
    "crumbs": [
      "Scale up Training"
    ]
  },
  {
    "objectID": "Chapter1_Proteins.html",
    "href": "Chapter1_Proteins.html",
    "title": "7  Proteins: from sequence to structure",
    "section": "",
    "text": "7.1 Traditional Approaches to Protein Structure Prediction\nProteins are the fundamental building blocks of biological systems, and understanding their structure is crucial for deciphering their function. The journey from a one-dimensional (1D) amino acid sequence to a two-dimensional (2D) contact map/distance map and finally to a three-dimensional (3D) structure has been one of the grand challenges in computational biology. In this chapter, we explore how protein models based on attention/transformer-like modules, like AlphaFold and ESMfold(2)(Lin et al. 2023), revolutionized this space. Though we should not from the start, that models which are aware of multiple sequences alignment (MSA) during training, that are supervised to understand evolutionary constraint (i.e learn what can and cannot easily change in a protein) are VERY hard to beat for pure Protein language Models. Those MSA aware models, specifically Alphafold and Alphafold2(Jumper et al. 2021) won their developers a Nobel Price in medicine. Before you dive into these chapters on protein models, its worth watching this vertasium video titled “The most useful thing AI has ever done” as a way of easing into the topic. one elephant in the room is that protein language models, which do not rely on MSA, have struggled greatly to beat much smaller protein models that directly incorporate MSA, so always evaluate the pro’s/con’s of different architectures for your specific research questions & needs!\nBefore the advent of language models and sophisticated deep learning architectures, protein structure prediction relied heavily on physics-based models and evolutionary information encoded in multiple sequence alignments (MSAs). Homology modeling, one of the earliest techniques, used the structures of similar, evolutionarily related proteins to infer the structure of a target protein. Threading methods aligned sequences against known structures to find the best possible fold. Ab initio modeling, in contrast, attempted to predict protein structure from first principles, using physical energy functions to simulate the folding process. These methods often struggled with accuracy and required extensive computational resources, making them impractical for many real-world applications.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Proteins: from sequence to structure</span>"
    ]
  },
  {
    "objectID": "Chapter1_Proteins.html#the-critical-breakthrough-alphafold",
    "href": "Chapter1_Proteins.html#the-critical-breakthrough-alphafold",
    "title": "7  Proteins: from sequence to structure",
    "section": "7.2 The critical breakthrough: AlphaFold",
    "text": "7.2 The critical breakthrough: AlphaFold\nMSA-based models, like those used in the early iterations of AlphaFold and Rosetta, made significant strides by leveraging evolutionary couplings between residues. These statistical relationships, inferred from aligned sequences across species, provided powerful constraints on the possible 3D structures. Coupling information was used to construct 2D contact maps — matrices indicating which amino acid pairs were likely to be spatially close in the folded protein.\n\n\n\nFigure 1: The progress in protein structure prediction across iterations of CASP, notice the clear breakthroughs AlphaFold and AlphaFold-2 represent.\n\n\nAlphaFold(Senior et al. 2020) in CASP13, and AlphaFold 2(Jumper et al. 2021) in CASP 14 were revolutionary. The GDT scores, which are the percentage of atoms within 1, 2, or 8 angstroms (\\(10^{-10}\\) m) of the directly measured protein structure, reached ±90, meaning 90% of the atoms in the prediction were extremely close to the measured protein. The score was referred to as “near experimental accuracy,” though that claim made in the Nature paper was unreferenced. I would love a table with an empirical estimate of the experimental accuracy (from repeated independent measurements of the protein, for example) and the AF2 predictions’ GDT/RMSD side by side (I have not been able to find anything like it, could be my inexperience with the particular field!).\n\n7.2.1 Good old MSA\nIn our chapters on DNA language models, we came across GPN-MSA (Benegas et al. 2023), which relied on multiple sequence alignment (MSA) between species. We even trained a model that was a lot like it in Chapter 4! As you’ll recall, the model did amazing, but mostly I feel because while masking, we only masked the human base and let the model use all ancestral bases during prediction. The multiple sequence alignments play a big role in the recent success in protein folding.\nThe logic being that if two amino acids “correlate” or co-occur across species, like column 3 and 9 highlighted in red in Figure 2, that indicates their co-evolution. Their co-evolution at distance is then viewed as an indicator of their physical proximity in the 3D molecule. In the toy example, we only observe G and H OR T and V in positions 3 and 9, which means there is likely a fitness penalty against other combinations. If these co-evolution signals are robust (reliably found across many aligned sequences) and at a distance, they likely reflect that the two amino acids are physically close, and the substitution of one has an effect on the binding/function of the other.\n\n\n\n\n\nFigure 2: Example MSA with a co-evolving pair of amino acids (pos 3 and 9) highlighted in red.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Proteins: from sequence to structure</span>"
    ]
  },
  {
    "objectID": "Chapter1_Proteins.html#the-leap-to-protein-language-models",
    "href": "Chapter1_Proteins.html#the-leap-to-protein-language-models",
    "title": "7  Proteins: from sequence to structure",
    "section": "7.3 The Leap to Protein Language Models",
    "text": "7.3 The Leap to Protein Language Models\nThe success of AlphaFold 2 marked a watershed moment for protein structure prediction. By integrating attention-based neural networks and using MSAs to predict 3D structures directly from sequence information, AF2 achieved unprecedented accuracy. Yet, the reliance on MSAs introduced limitations — the need for evolutionary data and the computational cost of alignment. It even appears that as the datasets, and the models grow explicitly conditioning on MSA becomes less important. AlphaFold3 for example already reduces the importance of MSA related information in the model. that being said, Alphafold2 is a &lt; 100 million parameter model and it handily beats Protein language models ten to twenty times it size at times!\nProtein language models (PLMs) now slowly emerge as an alternative that is way cheaer to run, and doesnt require MSA allignment. By pretraining on vast protein databases, PLMs capture contextual information about amino acids and their interactions without needing MSAs. Models like ESM (Evolutionary Scale Modeling)(Rives et al. 2021), ESM2(Lin et al. 2023) and ProtTrans(Elnaggar et al. 2022) demonstrated the potential of PLMs to predict secondary and tertiary structures directly from sequence data. By encoding the relationships between residues through attention mechanisms, these models implicitly learn structural and functional properties, generating accurate 2D contact maps and even 3D coordinates in some cases.\nProtein language models are frequently augmented with geometric neural networks or other additional model “adapters” when used for protein structure prediction. These augmentations ensure the 1D sequences, 2D contact maps, and 3D protein shapes that follow adhere to basic geometric rules. A language model or another sequence model isn’t inherently aware of 3D space. So while we’ll experiment with the latent 2D representations (distances between amino acids) that are implicitly hiding in language models, those representations are noisy and might imply 3D structures that cannot exist (e.g., the distance between a and b, and b and c, might not be consistent with the distance between b and c, etc.). Adding specific neural networks that are designed to produce results that can exist in Euclidean (3D) space greatly improves protein structure prediction. More recent experimentation has linked protein language models directly to diffusion image models to predict protein structure in 3D space with minimal supervision.\nIts worth considering that while pure protein language models have partly closed the gap to models rthata re aware of MSA and structure, they still lag those models in various applicaiton as becoems apprent from extensicve protein model evaluations. For a recent overview of the relative performance of differnet protein model architectures check out this 2025 blog post by Pascal Notin.\n\n7.3.1 Protein language models as biologically tokenized DNA language models\nWe previously considered DNA language models at nucleotide level, and discussed in Chapter 2 how tokenization is a key step in designing any language model. A protein language model tokenized at amino-acid model is like a naturally tokenized DNA language model, at least for human coding sequences within the DNA. Its important to realize that while DNA coding sequences faithfully translated to amino-acid sequences, amino-acids cannot be perfectly translated back to DNA sequences given multiple triplet DNA/RNA codon sequences (e.g. GAC and GAU) code for the same amino-acid (in this case Asp). That being said, t a later point in this book (Chapter to be written) we’ll consider “multi-modal” models that can represent an input sequence as DNA, RNA or amino-acids with allowances for the asymmetry of the translation.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Proteins: from sequence to structure</span>"
    ]
  },
  {
    "objectID": "Chapter1_Proteins.html#step-by-step-1d-sequences-to-3d-structures",
    "href": "Chapter1_Proteins.html#step-by-step-1d-sequences-to-3d-structures",
    "title": "7  Proteins: from sequence to structure",
    "section": "7.4 Step by Step: 1D Sequences to 3D Structures",
    "text": "7.4 Step by Step: 1D Sequences to 3D Structures\nThe process of moving from a linear amino acid sequence to a folded protein structure involves multiple steps, and in the following chapters, we’ll trace those steps:\n\nData sources: As always, science begins with good data. In Chapter 8, we’ll talk about data sources for protein (language) models.\n1D Sequence Representation: The raw sequence of amino acids, analogous to a string of text in natural language. In Chapter 9, we’ll train a protein language model. Since we can’t compete with the likes of Google DeepMind and Facebook, or even startups, and I want it to be feasible for you to run all of the steps at home, we’ll train a small language model on an ad-hoc selection of G-protein coupled receptor (GPCR) (like/related) proteins across hundreds of species. This large protein family (4% of human protein-coding genome, for example) shares evolutionary origins and therefore is probably a ripe target for a small model with limited training budget.\n2D Contact Map Prediction: For any protein, you can conceive a matrix representation showing which pairs of residues are likely to be spatially close, providing key insights into the folding topology. In Chapter 10, we’ll discuss ways in which protein language models (latently!) actually already learn the 2D contact map for each protein, and we’ll extract these from the protein model we trained and compare them to the true contact map for a few proteins. because we’ll dig into the internals of a language model we’ll also have to study self-attention, the core “correlational” mechanism in a language model in this chapter.\n3D Structure Construction: In Chapter 11, we’ll look at a few different ways in which we can train models that output a full 3D protein by post-processing the 2D information learned in protein language models. We’ll conceive of 2D structures as images, and 3D structures as simply 3D representations. Deep-learning models for image data have advanced incredibly, and those lessons have been applied to proteins structure modeling. One highly successful guided image modeling technique is guided-diffusion, a model where trough adding noise to images in 1000s of small steps, and then training a model to denoise the image one step at a time people have been able to train “generative” models. These models can be augmented with textual (or other) context, so while denoising a golden retriever with a funny hat, the model is provided the embedding, or reduction of the text “Golden retriever with a funny hat”. Alphafold3(Abramson et al. 2024), one of the most performant models to date, is essentially a protein language model (slightly augmented with MSA information) where the final information form the language model is used to guide a diffusion model. Because Alphafold3 is a fairly complex model, we’ll take a stab at training something slightly more abstracted. We;’ll use our own protein model (or facebook/esm2_t33_650M_UR50D) to produce embedding, and by using cross attention between the language model embedding, and the diffusion image model matrices “guide” the construction of 2D contact maps from noise using fairly standard image modeling techniques(Saharia et al. 2022). Finally, we’ll have code to do the same for 3D protein structure, though training that model will likely exceed the computational facilities I or either of you can easily access. in Chapter 12 we expand our toy model abstraction of AF3, by conditioning the diffusion model on both 1D sequence embeddings and 2D attention maps.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Proteins: from sequence to structure</span>"
    ]
  },
  {
    "objectID": "Chapter1_Proteins.html#weird-and-wonderful-protein-model-variants",
    "href": "Chapter1_Proteins.html#weird-and-wonderful-protein-model-variants",
    "title": "7  Proteins: from sequence to structure",
    "section": "7.5 Weird and wonderful protein model variants",
    "text": "7.5 Weird and wonderful protein model variants\nOn the back of the success of Alphfold have come some truly creative models.\nAmino-acids to structure and then back to tokens: In Chapter 13, we’ll discuss an interesting class of models that actually mimic nature in a weird way, these models take molecules as input, and condense those into categorical tokens, not amino-acid, but learned tokens trained to represent a hyper efficient geometric alphabet. Foldseek (Kempen et al. 2023) and models like it(Gao, Tan, and Li 2024) take a 3D structure, and encode that into categorical tokens, at the same resolution of the amino acids. Then at each base we know amino-acid, its location in 3D space, and a new token that compresses that location in space. This technique, the first of which was known as might sound a little redundant but it gets at an important issue: many very different proteins, in terms of structure, might fold in similar ways, and even serve similar or the same biological function! Furthermore having encoded structure into tokens allows for ultra fast structure similarity search (orders of magnitudes faster then search based on the full 3d molecule). One of the exciting applications these models might have is to train GPT like models on these spatial tokens in order to generate candidate proteins which are likely to fold into a specific shape(Gaujac et al. 2024).\n\n\n\n\nAbramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, et al. 2024. “Accurate Structure Prediction of Biomolecular Interactions with AlphaFold 3.” Nature 630 (8016): 493–500. https://doi.org/10.1038/s41586-024-07487-w.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S. Song. 2023. “GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction.” http://dx.doi.org/10.1101/2023.10.10.561776.\n\n\nElnaggar, Ahmed, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, et al. 2022. “ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning.” IEEE Transactions on Pattern Analysis and Machine Intelligence 44 (10): 7112–27. https://doi.org/10.1109/tpami.2021.3095381.\n\n\nGao, Zhangyang, Cheng Tan, and Stan Z. Li. 2024. “FoldToken4: Consistent & Hierarchical Fold Language.” http://dx.doi.org/10.1101/2024.08.04.606514.\n\n\nGaujac, Benoit, Jérémie Donà, Liviu Copoiu, Timothy Atkinson, Thomas Pierrot, and Thomas D. Barrett. 2024. “Learning the Language of Protein Structure.” https://doi.org/10.48550/ARXIV.2405.15840.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nKempen, Michel van, Stephanie S. Kim, Charlotte Tumescheit, Milot Mirdita, Jeongjae Lee, Cameron L. M. Gilchrist, Johannes Söding, and Martin Steinegger. 2023. “Fast and Accurate Protein Structure Search with Foldseek.” Nature Biotechnology 42 (2): 243–46. https://doi.org/10.1038/s41587-023-01773-0.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, et al. 2023. “Evolutionary-Scale Prediction of Atomic-Level Protein Structure with a Language Model.” Science 379 (6637): 1123–30. https://doi.org/10.1126/science.ade2574.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences 118 (15). https://doi.org/10.1073/pnas.2016239118.\n\n\nSaharia, Chitwan, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, et al. 2022. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” https://doi.org/10.48550/ARXIV.2205.11487.\n\n\nSenior, Andrew W., Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, et al. 2020. “Improved Protein Structure Prediction Using Potentials from Deep Learning.” Nature 577 (7792): 706–10. https://doi.org/10.1038/s41586-019-1923-7.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Proteins: from sequence to structure</span>"
    ]
  },
  {
    "objectID": "Chapter2_Proteins.html",
    "href": "Chapter2_Proteins.html",
    "title": "8  Selecting and curating protein sequences",
    "section": "",
    "text": "8.1 Protein Sequence Data\nProtein data comes from multiple sources, each providing different types of information useful for training machine learning models and protein language models. Here’s the revised version with links added at the top of each paragraph. Given we can “convert” DNA to proteins using the genetic code, it also makes sense to consider sources of DNA data we discussed in Chapter 1 for training protein models.\nUniProt | NCBI RefSeq | MGnify\nThe most widely used source of protein sequences is UniProt, which contains curated (Swiss-Prot) and unreviewed (TrEMBL) protein sequences from various organisms. Other databases, like NCBI RefSeq, provide reference protein sequences linked to genomic data. Large-scale sequencing projects, such as MGnify, focus on metagenomic protein sequences, offering an even broader diversity. These sequence datasets serve as the foundation for protein language models (PLMs).",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Selecting and curating protein sequences</span>"
    ]
  },
  {
    "objectID": "Chapter2_Proteins.html#protein-sequence-data",
    "href": "Chapter2_Proteins.html#protein-sequence-data",
    "title": "8  Selecting and curating protein sequences",
    "section": "",
    "text": "8.1.1 UniRef50 and UniRef90\nUniRef\nUniRef50 and UniRef90 are clustered versions of UniProt protein sequences designed to reduce redundancy while maintaining sequence diversity. UniRef90 groups sequences that share at least 90% identity and have 80% overlap, merging nearly identical sequences into single representative entries. UniRef50 applies a looser 50% identity threshold, further condensing the dataset while preserving diverse functional and structural information. These clustered datasets are particularly useful for training protein language models and other ML applications, as they help mitigate biases from highly redundant sequences while still providing broad coverage across species. By using UniRef datasets, models can learn more generalizable representations of protein sequence space while improving computational efficiency.\n\n\n8.1.2 Protein Structure Data\nProtein Data Bank (PDB) | AlphaFold DB | SCOP\nFor structural information, the Protein Data Bank (PDB) is the most comprehensive resource, containing experimentally determined 3D structures from X-ray crystallography, NMR, and cryo-electron microscopy. The recent breakthroughs in AlphaFold DB provide high-quality computational structure predictions for almost all known proteins, greatly expanding the available structural information. Another key resource is SCOP (Structural Classification of Proteins), which organizes protein structures into hierarchical families based on evolutionary relationships, helping models understand structure-function relationships.\n\n\n8.1.3 Evolutionary and Functional Data\nPfam | InterPro | KEGG\nEvolutionary conservation is captured in Pfam, a database of protein families built from multiple sequence alignments, which is useful for understanding functional motifs. InterPro integrates multiple databases to annotate protein sequences with conserved domains, GO (Gene Ontology) terms, and other functional descriptors. KEGG (Kyoto Encyclopedia of Genes and Genomes) links proteins to biochemical pathways, aiding in training models to understand biological context.\n\n\n8.1.4 Protein-Protein Interactions and Kinetics\nSTRING | BioGRID | BindingDB | PDBBind\nDatabases like STRING and BioGRID compile large-scale protein-protein interaction networks, valuable for models predicting binding affinity or molecular interactions. Meanwhile, BindingDB and PDBBind offer datasets of protein-ligand interactions with binding affinities, supporting drug discovery applications.\n\n\n8.1.5 Specialized Datasets for ML and PLMs\nTAPE Dataset | SwissProt50/TrEMBL50 | Proteinnet\nSeveral datasets have been specifically designed for training ML models and protein LMs. TAPE (Tasks Assessing Protein Embeddings) provides benchmarks for PLMs on tasks like secondary structure prediction, fluorescence, and stability prediction. SwissProt50/TrEMBL50 datasets are commonly used for training PLMs by balancing redundancy in sequence data. Proteinnet contains the sequence, functional annotations, and atomic structure of all CASP proteins. The cool thing is that they are ordered chronologically, so you can train on CASP 7/8/9/10/11 and use CASP 12 or 13 as holdouts for validation.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Selecting and curating protein sequences</span>"
    ]
  },
  {
    "objectID": "Chapter2_Proteins.html#sequence-diversity-in-training",
    "href": "Chapter2_Proteins.html#sequence-diversity-in-training",
    "title": "8  Selecting and curating protein sequences",
    "section": "8.2 Sequence diversity in training",
    "text": "8.2 Sequence diversity in training\nWhen training protein sequence models, there is a risk of “overtraining” on abundantly measured, or frequently observed protein sequences. Not all protein families (characterized by a common evolutionary origin) are equally represented, and so uniform sampling from UniProt will skew towards certain types of proteins while undersampling others.\nVarious empirical projects consider sequencing diversity in training protein (language) models. The ESM-1 paper (Rives et al. 2021) evaluated 3 different levels of sequence diversity. The low-diversity dataset (UR100) consists of UniRef100 representative sequences, which are all proteins in UniProt removing identical copies (clustering sequences that share 100% identity). This dataset will have many near-identical sequences. The high-diversity sparse dataset (UR50/S) is derived from UniRef50 representative sequences; it’s going to have many unique sequences, but limited variation in similar sequences as all sequences with &gt; 50% shared identity are collapsed into a single cluster from which 1 representative sequence is retained. The high-diversity dense dataset (UR50/D) achieves broader coverage by uniformly sampling UniRef100 sequences across all UniRef50 clusters, mixing uniform representation and dense coverage. When models were compared using the exponentiated cross-entropy (ECE) metric, the sense diverse data performed best (see Table 1).\n\nTable 1: A single model trained on 3 data mixtures suggests dense diverse sequences lead to better training results (source: (Rives et al. 2021) )\n\n\n\n\n\n\n\n\n\nModel\nVariant\nParams\nTraining data\nECE\n\n\n\n\nTransformer\n34-layer\n669.2 M\nUR100 (low diversity)\n10.32\n\n\nTransformer\n-\n-\nUR50/S (h diversity sparse)\n8.54\n\n\nTransformer\n-\n-\nUR50/D (h diversity dense)\n8.46\n\n\n\nIf you are going to develop your own base protein (language) model, competing with the likes of Facebook and Google, then selecting the optimal training set is critically important. It is unlikely you’ll be able to compete in terms of compute, but with good data selection, you can save a lot of compute by making the model more efficient.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Selecting and curating protein sequences</span>"
    ]
  },
  {
    "objectID": "Chapter2_Proteins.html#are-we-running-out-of-training-data",
    "href": "Chapter2_Proteins.html#are-we-running-out-of-training-data",
    "title": "8  Selecting and curating protein sequences",
    "section": "8.3 Are we running out of training data?",
    "text": "8.3 Are we running out of training data?\nAn insightful study by (Cheng et al. 2024) shows that if you train on UniRef50 (about 15 billion tokens, or amino-acids), then larger models (3 Billion) start to deteriorate on validating data after training for 3 (or more) Epochs. While if you expand training to metagenomic sources (that cover eukaryotic species, viruses, etc.), you can expect continued improvements.\nThe most recent generation of protein deep learning models, like ESM-3(Hayes et al. 2025) and ESM-c (ESM Team 2024), train on a kind of UniRef70 (trying to find a sweet spot between 50 and 70) augmented with millions of metagenomic sequences from EMBL’s MGnify and JGI’s IMG databases. A good open source of that data is OMG_Prot50, a clustered version of the Open MetaGenomic dataset (OMG, which contains MGnify and IMG) clustered at 50% sequence identity to minimize duplication and maximize diversity. Across OMG_prot50 and UniRef50 or 90, there are a few billion relatively unique proteins and several hundreds of billions of amino acids to train on. That sounds like a lot (and it is), but the latest natural language models are trained on tens of trillions of tokens and show no signs of being saturated.\n\n\n\n\nCheng, Xingyi, Bo Chen, Pan Li, Jing Gong, Jie Tang, and Le Song. 2024. “Training Compute-Optimal Protein Language Models.” http://dx.doi.org/10.1101/2024.06.06.597716.\n\n\nESM Team. 2024. “ESM Cambrian: Revealing the Mysteries of Proteins with Unsupervised Learning.” EvolutionaryScale Website. https://evolutionaryscale.ai/blog/esm-cambrian.\n\n\nHayes, Thomas, Roshan Rao, Halil Akin, Nicholas J. Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, et al. 2025. “Simulating 500 Million Years of Evolution with a Language Model.” Science 387 (6736): 850–58. https://doi.org/10.1126/science.ads0018.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. “Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.” Proceedings of the National Academy of Sciences 118 (15). https://doi.org/10.1073/pnas.2016239118.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Selecting and curating protein sequences</span>"
    ]
  },
  {
    "objectID": "Chapter3_Proteins.html",
    "href": "Chapter3_Proteins.html",
    "title": "9  Training our first Protein Language Model",
    "section": "",
    "text": "9.1 Introduction\nNow that we have prepared a dataset of protein sequences, we can proceed to train a protein language model using the (Modern)BERT model architecture. Protein sequences, like DNA, follow structured patterns that can be learned by deep learning models. This chapter introduces the training of GPCR-BERT, a transformer-based masked language model (MLM) designed to understand and generate protein sequences.\nBecause the fundamentals of training a Protein language model aren’t that different from training a DNA language model this chapter is somewhat abbreviated, see Chapter 2 for more context, most of which directly translates to protein language models.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Training our first Protein Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter3_Proteins.html#tokenisation",
    "href": "Chapter3_Proteins.html#tokenisation",
    "title": "9  Training our first Protein Language Model",
    "section": "9.2 Tokenisation",
    "text": "9.2 Tokenisation\nProtein sequences are represented using the FASTA format, which encodes amino acids with standard single-letter codes. For this model, we define a vocabulary that includes:\n\nThe 20 standard amino acids (A, R, N, D, C, Q, E, G, H, I, L, K, M, F, P, S, T, W, Y, V)\nAmbiguity codes: B (N/D), Z (Q/E), X (any)\nA gap character: ‘-’\n\nA custom tokenizer is developed to handle these sequences efficiently. The tokenizer splits sequences using the defined vocabulary and applies a WordLevel tokenization approach for better generalization.\nimport torch\nimport wandb\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.pre_tokenizers import Split\nfrom transformers import PreTrainedTokenizerFast, ModernBertConfig, ModernBertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import load_dataset\n\n# Initialize Weights & Biases\nwandb.init(project=\"bert-protein\", name=\"bert-protein-GPCR-training_v1\")\n\n\n# --------------------------------\n# Protein Tokenizer with Full FASTA Amino Acid Code\n# --------------------------------\n\n# Define vocabulary to include all FASTA amino acids and special symbols\nprotein_vocab = {\n    \"A\": 0, \"R\": 1, \"N\": 2, \"D\": 3, \"C\": 4, \"Q\": 5, \"E\": 6, \"G\": 7,\n    \"H\": 8, \"I\": 9, \"L\": 10, \"K\": 11, \"M\": 12, \"F\": 13, \"P\": 14, \"S\": 15,\n    \"T\": 16, \"W\": 17, \"Y\": 18, \"V\": 19,  # Standard 20 amino acids\n    \"B\": 20, \"Z\": 21, \"X\": 22,           # Ambiguity codes: B (N/D), Z (Q/E), X (any)\n    \"-\": 23,                               # Gap character\n    \"[UNK]\": 24, \"[PAD]\": 25, \"[CLS]\": 26, \"[SEP]\": 27, \"[MASK]\": 28\n}\n\n# Create tokenizer\nprotein_tokenizer = Tokenizer(WordLevel(vocab=protein_vocab, unk_token=\"[UNK]\"))\nprotein_tokenizer.pre_tokenizer = Split(\"\", \"isolated\")  # Character-level splitting\n\n# Convert to Hugging Face-compatible tokenizer\nhf_protein_tokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=protein_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\"\n)\n\nprint(hf_protein_tokenizer.tokenize(\"MEEPQSDPSV\"))  # Test with a short sequence",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Training our first Protein Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter3_Proteins.html#load-the-training-data",
    "href": "Chapter3_Proteins.html#load-the-training-data",
    "title": "9  Training our first Protein Language Model",
    "section": "9.3 Load the training data",
    "text": "9.3 Load the training data\nUnlike large-scale protein language models trained on massive datasets from UniProt or AlphaFold, our approach focuses on a more specialized subset: G-protein coupled receptor (GPCR) genes, sourced from RefProt 90. This choice is driven by computational constraints and the goal of achieving high-quality representations within a biologically meaningful domain. By narrowing our scope, we ensure that the model learns from a well-defined set of sequences, optimizing for accuracy and relevance in GPCR-related research rather than sheer scale.\nyou’ll find the dataset here, but be aware this isn’t a curated dataset, if you would truly want to train a GPCR transformer, you’d go and spend months studying the best resources for validated GPCR proteins, like the GPCR database and related experimental literature.\n# --------------------------------\n# Load and Tokenize the Protein Dataset\n# --------------------------------\ndataset_name = \"MichelNivard/UniRef90-GPCR-Proteins\" # Generic example for now, will clean own data later\ndataset = load_dataset(dataset_name, split=\"train\")\n# Shuffle the dataset\ndataset = dataset.shuffle(seed=42)\n\nprint(\"dataset loaded\")\n\ncolumn_name = \"value\"\n\ndef tokenize_function(examples):\n    return hf_protein_tokenizer(examples[column_name], truncation=True, padding=\"max_length\", max_length=512)\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[column_name])\n\n# Save tokenized dataset\ntokenized_dataset.save_to_disk(\"tokenized_protein_dataset\")\n\nprint(\"dataset tokenized\")",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Training our first Protein Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter3_Proteins.html#model-architecture-training",
    "href": "Chapter3_Proteins.html#model-architecture-training",
    "title": "9  Training our first Protein Language Model",
    "section": "9.4 Model Architecture & Training",
    "text": "9.4 Model Architecture & Training\nAfter tokenizing the dataset, we proceed to train the model,\nWe use the ModernBertForMaskedLM model from the transformers library. The key components of the training setup are:\n\nModel Configuration: ModernBertConfig defines the model’s parameters, including the number of transformer layers and attention heads.\nData Collation: DataCollatorForLanguageModeling is used to prepare masked language modeling inputs.\nTraining Arguments: Configured with learning rate scheduling, batch size, and gradient accumulation settings.\nTrainer: The Trainer class orchestrates training, validation, and logging using wandb (Weights & Biases).\n\n\n# --------------------------------\n# Load and Tokenize the Protein Dataset\n# --------------------------------\ndataset_name = \"MichelNivard/UniRef90-GPCR-Proteins\" # Generic example for now, will clean own data later\ndataset = load_dataset(dataset_name, split=\"train\")\n# Shuffle the dataset\ndataset = dataset.shuffle(seed=42)\n\nprint(\"dataset loaded\")\n\ncolumn_name = \"value\"\n\ndef tokenize_function(examples):\n    return hf_protein_tokenizer(examples[column_name], truncation=True, padding=\"max_length\", max_length=512)\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[column_name])\n\n# Save tokenized dataset\ntokenized_dataset.save_to_disk(\"tokenized_protein_dataset\")\n\nprint(\"dataset tokenized\")\n\n# --------------------------------\n# Define the BERT Model from Scratch\n# --------------------------------\nconfig = ModernBertConfig(\n    vocab_size=len(protein_vocab),\n    hidden_size=512,\n    num_hidden_layers=24,\n    num_attention_heads=24,\n    intermediate_size=1024,\n    max_position_embeddings=512,\n    type_vocab_size=1,\n)\nconfig.pad_token_id = protein_vocab[\"[PAD]\"]\nmodel = ModernBertForMaskedLM(config)\n\n# --------------------------------\n# Training Configuration (Prints Loss)\n# --------------------------------\ntraining_args = TrainingArguments(\n    output_dir=\"./bert-protein\",\n    overwrite_output_dir=True,\n    logging_steps=1,  # Log loss every step\n    save_steps=1000,\n    save_total_limit=2,\n    per_device_train_batch_size=12,\n    gradient_accumulation_steps=1,\n    num_train_epochs=3,\n    learning_rate=5e-4,\n    weight_decay=0.01,\n    push_to_hub=False,\n    report_to=\"wandb\",  # wandb logging\n)\n\n# MLM Data Collator (Automatically Creates `labels`)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=hf_protein_tokenizer, mlm=True, mlm_probability=0.15)\n\n# --------------------------------\n# Train the Model\n# --------------------------------\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=hf_protein_tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()\n\n# Save the final model and tokenizer\ntrainer.save_model(\"./bert-protein\")\nhf_protein_tokenizer.save_pretrained(\"./bert-protein\")\n\nprint(\"🎉 Training complete! Model saved to ./bert-protein\")\n\n\n# Save the final model and tokenizer to Hub\nmodel.push_to_hub(repo_id=\"MichelNivard/GPCRBert-v0.1\",use_auth_token=\"YOUR-TOKEN-HERE\")\nhf_protein_tokenizer.push_to_hub(repo_id=\"MichelNivard/GPCRBert-v0.1\",use_auth_token=\"YOUR-TOKEN-HERE\")",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Training our first Protein Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter3_Proteins.html#result",
    "href": "Chapter3_Proteins.html#result",
    "title": "9  Training our first Protein Language Model",
    "section": "9.5 Result",
    "text": "9.5 Result\nThe model is able to learn the structure of GPCR proteins, thought eh strong variability in loss over batches suggests its not uniformly learning all members of the cluster, perhaps due to data imbalances (see Figure 1).\n\n\n\nFigure 1: GPCR-BERT training loss curve\n\n\nIts interesting to note that the GCPR-Bert training loss is lower then that of much larger protein models trained on larger datasets. This makes sense, the loss (especially on training data!) is highly dependent on the diversity of the data, learning ALL proteins, is harder then learning a subset of proteins. Even at larger scales this phenomena is still evident. In Figure 2 (source (Cheng et al. 2024) Figure A11) we see two love curves for 85m parameter protein language models. Trained on UniRef90 (representative but not extremely diverse) and ColabFoldDB which contains a lot of meta-genomic protein sequences with far greater diversity. Its harder for the model to learn more diverse data, have the higher loss. However, on downstream tasks like atomic contact prediction and fold classification, the models perform nearly identical!\n\n\n\nFigure 2 (Figure A11 from (Cheng et al. 2024)) the loss curves, and downstream task performances of two models trained on different datasets. Clearly we can’t directly compare the loss as the very different losses translate to similar performance!",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Training our first Protein Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter3_Proteins.html#homework",
    "href": "Chapter3_Proteins.html#homework",
    "title": "9  Training our first Protein Language Model",
    "section": "9.6 Homework",
    "text": "9.6 Homework\n\n\n\n\n\n\nTip\n\n\n\nCheng et al. reveal something profound about model loss and scientist’s goalsin their analysis. The loss isn’t a direct measure of the model’s quality for your specific goals. If you are to design a protein language model for a specific tasks (predict the fold of human proteins after mutation for example) then you have to design validation accordingly. As we read in this chapter, and last, that we are running out of animal and plant reference protein data to train protein language models, people have therefore started too include meta-genomic protein data is training. Thets probably a great idea, but its worth considering weather for YOUR downstream tasks it is. ESM3 is better then ESM2, is its trained on way more data, but is it better for your applications?\nAn interesting unsolved issue to work on in protein model training is how to introduce low diversity but critical protein data in training. We saw models do well on diverse data, but what if we want to introduce data from many individual humans, with disease status as meta info for example, that would be very low diversity data. But the diversity that does exists tells us important things about what variation is, and isn’t tolerated. We need ways to fine-tune a pre-trained models on high value but low diversity data without reducing the models performance. would you reduce the learnign rate, would you mix in very diverse sequences? Would you freeze parts of the model in place?",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Training our first Protein Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter3_Proteins.html#conclusion",
    "href": "Chapter3_Proteins.html#conclusion",
    "title": "9  Training our first Protein Language Model",
    "section": "9.7 Conclusion",
    "text": "9.7 Conclusion\nThis chapter introduced GPCR-BERT, a transformer-based language model for protein sequences. In subsequent chapters, we will explore fine-tuning strategies and downstream applications, such as protein structure prediction. In order to do so we’ll have to really unpack the internals of transformer models, the self-attention mechanism. We’ll return to training protein language models in the chapter “putting it all together” in which we putt all lessons together to do one massive training run and training a protein language model that rivals facebook ESM-2 (and cost me ±900$ out of my own pocket in compute).\n\n\n\n\nCheng, Xingyi, Bo Chen, Pan Li, Jing Gong, Jie Tang, and Le Song. 2024. “Training Compute-Optimal Protein Language Models.” http://dx.doi.org/10.1101/2024.06.06.597716.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Training our first Protein Language Model</span>"
    ]
  },
  {
    "objectID": "Chapter4_Proteins.html",
    "href": "Chapter4_Proteins.html",
    "title": "10  Protein contact maps from attention maps",
    "section": "",
    "text": "10.1 What is attention exactly?\nOne way to construct a scaffold for protein 3D structure learning is to first learn the distances (2d) between all amino-acids, or between atoms within those amino acids. Contact maps have been extensively used as a simplified representation of protein structures. Contact maps capture the most important features of a protein’s fold and are preferred by a number of researchers for the description and study of protein structures”(Duarte et al. 2010).\nA protein’s amino acid sequence is one-dimensional (1D) information: the linear order of residues. The functional three-dimensional (3D) structure is how that chain folds in space, bringing certain residues into contact. Contact maps serve as a two-dimensional (2D) bridge between sequence and structure. Specifically, it is a binary \\(N \\times N\\) matrix (for an \\(N\\)-residue protein) where an entry is 1 if two residues are within a distance threshold (e.g., 12 Å) in the 3D structure, and 0 if they are not. This 2D representation captures key structural relationships (who-neighbors-whom) without specifying exact coordinates.\nIn protein structure prediction, one common approach is: 1D sequence ⇒ predicted 2D contact map ⇒ inferred 3D structure. The contact map, derived from sequence-based predictions (such as co-evolution analysis or machine learning), provides constraints that guide how the chain can fold. Note, however, when converting a full 3D structure into a simplified 2D contact map, some information is inevitably lost. The contact map is a binary (or at best, thresholded distance) representation: it tells us whether a pair of residues is within a certain cutoff distance, but not the exact distance beyond that or their relative orientation. A dramatic illustration is that a protein’s mirror-image (reflected) structure yields an identical contact map, since all pairwise distances are the same. The map alone cannot distinguish a right-handed fold from its left-handed enantiomer: chirality information is lost.\nContact maps can act as a powerful but imperfect intermediate blueprint, translating linear sequence information into spatial relationships that approximate the final folded structure. Figure 1 is an example contact map based on a distance threshold of 12 Angstroms between Cα atoms (carbon atoms that are part of the protein backbone) in the AlphaFold 2 model of the protein structure (PDB format) of a human GRPC protein.\nBelow we’ll study how we can estimate contact maps from the self-attention networks, or maps, that are latently encoded in protein language models, based on ideas developed by Roa et al. (Rao et al. 2020) and others (Vig et al. 2020). This is frankly like magic, just by training to predict protein sequences, the model learns the 2D structure of the protein, and because we know 2D structure relates to 3D structure we can prove protein language models learn (elements of) the 3D structure of proteins.\nTo be able to predict contact maps, we need to examine the models we have been training and study what attention is and what attention maps are. Frankly, understanding attention (and I don’t think I really did until I had to dig in deep to write this chapter) is a key step to understanding what transformer-like models are learning.\nAttention can be thought of as a more flexible, learned version of correlation but one that dynamically changes based on context.\nIn a traditional correlation or covariance matrix, each entry tells us how strongly two variables (or two positions in a sequence) are related, on average across an entire dataset. If we were studying DNA, for example, a correlation matrix might tell us which nucleotide positions tend to mutate together across many species. However, correlation is static—it does not adapt to the specific sequence we are analyzing at any given moment.\nAttention, by contrast, is like a learned, dynamic correlation matrix that changes depending on the sequence it is applied to. Instead of capturing a single, global pattern, it can learn to recognize many different ways in which elements of a sequence relate to one another, depending on context.\nFor example: - In DNA analysis, attention can learn to recognize when certain nucleotides influence each other in one evolutionary context but not another. - In proteins, attention can capture long-range interactions between amino acids that affect folding, even if they are far apart in the sequence. - In language, attention allows a model to understand that a pronoun like “it” refers to different things in different sentences, based on context.\nMathematically, attention produces an attention map, which is similar to a covariance matrix but is computed on-the-fly for each sequence. This means that, instead of assuming a fixed set of relationships, the model can learn multiple different interaction patterns and decide which are relevant based on the input it sees.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Protein contact maps from attention maps</span>"
    ]
  },
  {
    "objectID": "Chapter4_Proteins.html#what-is-attention-exactly",
    "href": "Chapter4_Proteins.html#what-is-attention-exactly",
    "title": "10  Protein contact maps from attention maps",
    "section": "",
    "text": "10.1.1 Multi-Head Attention: Learning Many Correlation Patterns\nA key advantage of multi-headed, multi-layer attention is that it does not learn just one correlation pattern—it learns many at the same time. Multiple parallel attention heads in a transformer layer model act like independent correlation matrices, capturing different kinds of dependencies. Some heads might focus on local relationships (like adjacent words in a sentence or nearby mutations in DNA), while others capture long-range dependencies (like structural dependencies in proteins or distant but functionally linked words in a text). Moreover, because attention is stacked in series across layers, each layer refines and builds on the patterns learned by previous layers. This allows transformers to build hierarchical feature representations, capturing complex relationships that simple, shallow models cannot.\n\n\n10.1.2 The core Attention Equation\nThe fundamental equation of attention in transformer models, the core of each attention head in each layer, is based on computing the relationship between different positions in a sequence through the Query (Q), Key (K), and Value (V) matrices. The attention score is calculated as:\n\\[\\text{Attention}(Q, K, V) = \\text{softmax} \\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V \\]\nwhere \\((Q)\\) represents the queries (which ask for relevant information), \\((K)\\) represents the keys (which provide relevance scores), and \\((V)\\) represents the values (the actual content being retrieved). The softmax function ensures that the attention scores sum to 1, effectively acting as a weighting mechanism over the values.\nThe Query (\\(Q\\)), Key (\\(K\\)), and Value (\\(V\\)) matrices are derived from the input embeddings (\\(X\\)), which represent tokens in a continuous vector space. The embeddings dimensions are sequences length k by embedding dimensions d. These embeddings are specific to the given sequence, meaning different input sequences will produce different embeddings. The transformation into \\(Q\\), \\(K\\), and \\(V\\) happens through learned weight matrices (\\(W_Q\\), \\(W_K\\), \\(W_V\\)), which remain constant after training but are applied dynamically to the current sequence. Specifically, these transformations are computed as:\n\\[Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\\]\nSince (\\(W_Q\\), \\(W_K\\), \\(W_V\\)) are learned parameters, they do not change per sequence after training, but the values of \\(Q\\), \\(K\\), and \\(V\\) do change because they depend on the specific embedding (\\(X\\)) of the input sequence. The input-dependence of \\(Q\\), \\(K\\), and \\(V\\) allows the model to flexibly compute attention maps for different sequences while maintaining a consistent learned transformation process.\n\n\n10.1.3 Mathematical Link Between Attention and Correlation\nLet’s expand slightly on the conceptual analogy between attention and “covariance”. The core self-attention mechanism:\n\\[A = \\text{softmax} \\left(\\frac{QK^T}{\\sqrt{d_k}} \\right)\\]\nwhere:\n\n( \\(Q = X W_Q\\) ), ( \\(K = X W_K\\) ), and ( \\(V = X W_V\\) ) are learned transformations of the input ( \\(X\\) ).\n( \\(A\\) ) is the attention map, which determines how much each element in the sequence attends to others.\nThe denominator ( \\(\\sqrt{d_k}\\) ) is a scaling factor to stabilize gradients.\n\nThe term (\\(QK^T = (X W_Q) (X W_K)^T\\)) computes a similarity measure between elements of (\\(X\\)) after transformation. If (\\(W_Q = W_K\\)), this resembles the covariance matrix of a linear transformation of (\\(X\\)), and \\(QK^T\\) can be rewritten as:\n\\[QK^T = X W_Q W_K^T X^T\\]\nThis is structurally similar to the sample covariance matrix, where Z is an n (people) by m (variables) data matrix:\n\\[\\text{Cov}(Z) = Z^T Z\\]\nNote the transpose is on the other “end” of the product, but because we are computing the dependenence between a sequences of d tokens (rows of X), whule for covariance we compute the dependencies between m variables (columns of Z), the apperent reversal of the side of the equations that get transposed actually makes sense. There are obvious difference: attention obviously additional learned transformations ( \\(W_Q\\) ) and ( \\(W_K\\) ) and its asymmetric (\\(W_Q \\neq W_Q\\)). Thus, the attention map is effectively a learned, context-specific, asymmetric version of dependence, which has analogs to a covariance matrix. Its good to remember we have abstracted away the role of the softmax normalization and the \\(V\\) matrix in service of illustrating how attention is a little bit like very dynamic context specific, and asymmetric covariance\nThe global attention map across a model is not a simple sum of all the attention maps from different heads. Instead, h attention heads operate in parallel, each computing a separate attention matrix (\\(A_h\\)), and their outputs are concatenated and linearly projected rather than summed. However, each individual attention map can often be viewed as a specific conditional relation or dependence in the data.\n\n\n10.1.4 How Attention is Stacked Across Layers\nIn a transformer model, attention is not only computed within a single layer but is also stacked sequentially across multiple layers, allowing deeper layers to refine and build upon the representations learned by previous layers.\nHeads within a layer operate in parallel: Each attention head captures a different perspective on the input sequence within the same layer. Layers operate in sequence: The output of one layer (which has already been shaped by multiple heads) becomes the input for the next layer. This layer-wise stacking allows transformers to build hierarchical feature representations, capturing complex relationships that simple, shallow models cannot.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Protein contact maps from attention maps</span>"
    ]
  },
  {
    "objectID": "Chapter4_Proteins.html#attention-encodes-biology",
    "href": "Chapter4_Proteins.html#attention-encodes-biology",
    "title": "10  Protein contact maps from attention maps",
    "section": "10.2 Attention encodes biology",
    "text": "10.2 Attention encodes biology\nKey interpretable AI work by Roa et al. (Rao et al. 2020) reveal that the attention matrices of sequences based (one dimensional) Protein language models encode the two dimensional structure of proteins, while others have show attention encodes amino-acid substitution probabilities (which in turn relate to the potential deliteriousness of mutations)(Vig et al. 2020). They extract all individual Attention maps, for a given sequence from a Protein language model, and using those to predict the estimated (or experimentally observed) contact maps for the same sequence.\nThe intuition here is that the attention maps are amino-acid by amino-acid maps of the relation between the bases in a given sequences, which encode what other bases the model attends to at base \\(j\\), and if the model encodes long range structure (which, spoiler, it does) when if base \\(j\\) and base \\(I\\) are far apart in the sequence, but interact and are co-evolving because their close in the proteins 3D structure, we expect some of the attention maps to tend to that interaction/co-evolution. As the model is an unsupervised black box, we obviously don’t know what attention matrices will matter, so we’ll simply use Logistic regression where the outcome is the “gold standard” contact map (in our case for convenience sake AlphaFold2 predictions of the contact map) and the predictors are the attention maps. By running logistic regression on only 4(!!!) genes and using the average coefficients we can predict a very crude outline of the contact map of the 5th gene (See Figure 2).\n\n10.2.1 Grabbing attention (maps)\nFortunately, the attention maps are generally an element we can export from language models using standard libraries like transfomers so we can write a function to extract all attention maps from a protein language model. These will become the predictors (x) in our logistic regression later.\nSince the model we trained in Chapter 9 is too small, and a larger model is still in the works (I am running all the compute for this book on my MacBook and an old gaming GPU) we’ll work with \"facebook/esm2_t30_150M_UR50D\" an advanced PLM developed by Standford and Facebook. This particular version has 20 attention heads and 33 layers, so that’s \\(20*30 = 660\\) attention maps to use for prediction.\n# Load model and tokenizer\nmodel_name = \"facebook/esm2_t30_150M_UR50D\"\nconfig = AutoConfig.from_pretrained(model_name, output_attentions=True)\nmodel = AutoModelForMaskedLM.from_pretrained(model_name, config=config)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Function to extract attention matrices\ndef extract_attention_matrices(sequence):\n    inputs = tokenizer(sequence, return_tensors='pt')\n    outputs = model(**inputs)\n    attentions = outputs.attentions  # Tuple (num_layers, batch_size, num_heads, seq_len, seq_len)\n    stacked_attentions = torch.cat([attn.squeeze(0) for attn in attentions], dim=0)\n    return stacked_attentions.detach().numpy()\nSimilarly we extract the “true” contact map from a protein sequence structure file (PDB format).\n# Function to generate true contact maps from PDB\ndef generate_contact_map(pdb_filename, chain_id, threshold=12.0):\n    parser = PDBParser()\n    structure = parser.get_structure(\"protein\", pdb_filename)\n    chain = structure[0][chain_id]\n    \n    residues = [res for res in chain if \"CA\" in res]  # Ensure we only use residues with CA atoms\n    seq_len = len(residues)\n    \n    dist_matrix = np.zeros((seq_len, seq_len))\n    for i, res_one in enumerate(residues):\n        for j, res_two in enumerate(residues):\n            diff_vector = res_one[\"CA\"].coord - res_two[\"CA\"].coord\n            dist_matrix[i, j] = np.sqrt(np.sum(diff_vector * diff_vector))\n    \n    contact_map = dist_matrix &lt; threshold\n    return contact_map.astype(int)\nThen we’ll use logistic regression to predict the contact map from the attention maps, to stabilizes the estimates we’ll do so for 4 genes and average the coefficients. To get an optimal estimate you might use up to 20, 30 or 40 genes here but its interesting to see this actually kind of works with as little as four genes.\nfor seq, pdb in zip(sequences[:3], pdb_filenames[:3]):\n    true_contact_map = generate_contact_map(pdb, chain_id)\n    attention_matrices = extract_attention_matrices(seq)\n    \n    # Prepare features\n    seq_len = true_contact_map.shape[0]\n    X = np.zeros((seq_len * (seq_len - 1) // 2, attention_matrices.shape[0]))\n    y = np.zeros((seq_len * (seq_len - 1) // 2,))\n\n    index = 0\n    for i in range(seq_len):\n        for j in range(i + 1, seq_len):\n            feature_vector = attention_matrices[:, i, j]\n            if j - i &gt;= 1:  # Ignore near-diagonal contacts\n                X[index] = feature_vector\n                y[index] = true_contact_map[i, j]\n            index += 1\n    \n    # Train logistic regression model\n    clf = LogisticRegression()\n    clf.fit(X, y)\n\n    # Store learned coefficients\n    all_coefs.append(clf.coef_)\n    all_intercepts.append(clf.intercept_)\n\n# Compute the average coefficients\navg_coefs = np.mean(np.array(all_coefs), axis=0)\navg_intercept = np.mean(np.array(all_intercepts), axis=0)\n\n\n10.2.2 Results\nBased on the very limited regression model we can predict the hazy outline of the “contact” map (Figure 2).\n\n\n\nFigure 2: predicted (top) version “gold standard” contact map based on a very limited “model” trained on 4 genes, predicted into a fifth gene.\n\n\nI cant emphasize enough this is just an illustration and not an actual academic analysis/prediction, obviously a fuller implementation as done in (Rao et al. 2020) is. The full process of this chapter is outlined in Figure 3, parallel attention heads, across sequential layers, ensure proteins structure can be predicted. We then attention maps form the model, and use those to predict the protein contact map. Currently the prediction of the contacts, and the masked sequence elements are seperate processes, but in many deep learning models in general, and protein language models in specific, those prediction tasks are fused into one single model.\n\n\n\nFigure 3. Protein 2-D structure prediction attention, and attention maps.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Protein contact maps from attention maps</span>"
    ]
  },
  {
    "objectID": "Chapter4_Proteins.html#exercises",
    "href": "Chapter4_Proteins.html#exercises",
    "title": "10  Protein contact maps from attention maps",
    "section": "10.3 Exercises",
    "text": "10.3 Exercises\nIf you want a good Masters/PhD term project with limited compute demands, or you prefer to do self-study trough coding 9Icertainly do), try improving/modeling contact maps. There are some obvious pieces of low hanging fruit here, for example why are we predicting threshold distances for example? whouldn’t it be better to predict distance directly? That would retain additional information form the contact maps. Why are we using these ad-hoc data I just copy pasted from UniProt/AF2? We’d probably gain from using a standardized dataset, proteinnet (AlQuraishi 2019) for example contains sequences and 3d structure of CASP7/8/9/1011 and 12 datasets in a format amenable to analysis. The exercises below motivate you to think about optimizing parts of the process outlined in Figure 3, and, if you are up for it, trying to capture the entire process into a single model.\n\n\n\n\n\n\nTip\n\n\n\nIn sequence from beginner to advanced project consider these exercises:\n1. (easy) Build a predictor across 100 genes, use ridge or lasso regression to improve on the effectiveness of the prediction. Evaluate whether predicting continuous distances results in better prediction results.\n2. (medium) Why aren’t we doing ML in the prediction step? Train an actual tabular/regression model or adapter across a few thousand genes with very good experimental structure data to squeeze all the juice out of the attention matrices. You can consider whether sing experimentally validated protein structures improves the model.\n3. (hard) Finetune a medium size PLM on a joint masked language model loss, and contact map prediction loss based on the attention maps, jointly optimizing the model as a PLM and a contact map prediction model. Youd have to grab a pre-trained PLM (say: facebook/esm2_t30_150M_UR50D on huggingface) add code to 1. produce the attention maps during the model’s forward pass, related the attention maps (non-linearly) to the observed distances using MSE loss (of cross entropy if you keep the contact map dichotomous), and finetune the model on the combined loss: \\(\\alpha * MSEloss + \\beta * MLMLoss\\) YOu’d likely have to do some evaluating of optimal values alpha ans beta. You’d be training a model that concurrently is a great masked protein language model and optimizes the attention matrices such that their maps become an optimal predictor of the molecular distances in the protein.\n\n\nIf you make it to the hard exercise, you are essentially training a “adapter” to fit on top of your PLM. This is common practice in machine learning with language models, add parts to an existing model, either freezing the exciting model in place, or letting it optimize along with he adapter, to excel at a specific task. One of the best performing protein folding PLMs, ESMfold(Lin et al. 2023), is such an adapter, though that adapter is more sophisticated then the one you’d train here.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Protein contact maps from attention maps</span>"
    ]
  },
  {
    "objectID": "Chapter4_Proteins.html#the-wonder-of-attention-and-beyond",
    "href": "Chapter4_Proteins.html#the-wonder-of-attention-and-beyond",
    "title": "10  Protein contact maps from attention maps",
    "section": "10.4 The wonder of attention, and beyond",
    "text": "10.4 The wonder of attention, and beyond\nThe best summary of the amazing properties of attention I came across was in Poli et al.(Poli et al. 2023). They, summarize years of mechanistic studies that abstract the essential features in attention that make it so highly effective at token sequence learning.\n\nAttention is highly expressive, responsive to, and controlled by, the input data. It can see a new sequence, and from its training abstract (imperfectly) the interactions and relations between elements in that sequence.\nWhile the model size (in terms of # parameters) scales with sequences length, the parameter size of the attention weight matrices themselves do not! The weight matrices (\\(W_Q\\), \\(W_K\\), \\(W_V\\)) are of dimension embedding size, by key/query/value vector length.\nWhile models have a set (or computationally limited) context size, the attention mechanism itself does not! Regardles of the token lenght dimension of X, the products (\\(Q = X W_Q\\) ), ( \\(K = X W_K\\) ), and ( \\(V = X W_V\\) ) can be compute. Attention weights can attended to any pair of tokens regardless of their (relative) position. Though smart types of positional embedding are required to effectively capitalize on this aspect of attentions (i.e. effectively the degree to which attention scales beyond the sequence length seen during training is modest).\n\nThese amazing features come at a cost, the computational cost of attention scale quadratically with the length of the sequence (as \\(X\\) “grows” so do \\(Q\\),\\(K\\),\\(V\\)). Chasing sub-quadratic alternatives to attention, with the “hyena operator” being a current favorite(Nguyen et al. 2023). These alternatives may not always produce explicit “attention maps” and so peopel have developed alternate strategies to extract contact maps and proteins structure information from PLMs(Zhang et al. 2024).",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Protein contact maps from attention maps</span>"
    ]
  },
  {
    "objectID": "Chapter4_Proteins.html#conclussion",
    "href": "Chapter4_Proteins.html#conclussion",
    "title": "10  Protein contact maps from attention maps",
    "section": "10.5 Conclussion",
    "text": "10.5 Conclussion\nSo what did we learn here? we learned that within the unsupervised PLM, which is trained on 1-D sequences, we uncover “knowledge” of 2D protein structure. What I did here is by no means an optimal protein contact prediction but others have build on this idea and done so successfully(Lin et al. 2023). In the next chapter we’ll discuss how we can append models onto PLMs first to predict 2D contact maps, and then to go above and beyond and predict 3D protein structure. To do so we’ll build a model that has conceptual similarities to alphafold3(Abramson et al. 2024) which means we’ll need to learn about diffusion image models, and about cross-attention between sequences and molecules. With Alphafold3 we’ll also step into the world of model closed off for commercial reasons. It obvious why google deepmind would want to retain an edge, but its less clear why Nature published the Alphafold3 paper and accepted it without code (“AlphaFold3  Why Did Nature Publish It Without Its Code?” 2024), which makes studying it a lot harder.\n\n\n\n\nAbramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, et al. 2024. “Accurate Structure Prediction of Biomolecular Interactions with AlphaFold 3.” Nature 630 (8016): 493–500. https://doi.org/10.1038/s41586-024-07487-w.\n\n\n“AlphaFold3  Why Did Nature Publish It Without Its Code?” 2024. Nature 629 (8013): 728–28. https://doi.org/10.1038/d41586-024-01463-0.\n\n\nAlQuraishi, Mohammed. 2019. “ProteinNet: A Standardized Data Set for Machine Learning of Protein Structure.” BMC Bioinformatics 20 (1). https://doi.org/10.1186/s12859-019-2932-0.\n\n\nDuarte, Jose M, Rajagopal Sathyapriya, Henning Stehr, Ioannis Filippis, and Michael Lappe. 2010. “Optimal Contact Definition for Reconstruction of Contact Maps.” BMC Bioinformatics 11 (1). https://doi.org/10.1186/1471-2105-11-283.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, et al. 2023. “Evolutionary-Scale Prediction of Atomic-Level Protein Structure with a Language Model.” Science 379 (6637): 1123–30. https://doi.org/10.1126/science.ade2574.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. “HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.” https://doi.org/10.48550/ARXIV.2306.15794.\n\n\nPoli, Michael, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. 2023. “Hyena Hierarchy: Towards Larger Convolutional Language Models.” https://doi.org/10.48550/ARXIV.2302.10866.\n\n\nRao, Roshan, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. 2020. “Transformer Protein Language Models Are Unsupervised Structure Learners.” http://dx.doi.org/10.1101/2020.12.15.422761.\n\n\nVig, Jesse, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. 2020. “BERTology Meets Biology: Interpreting Attention in Protein Language Models.” https://doi.org/10.48550/ARXIV.2006.15222.\n\n\nZhang, Zhidian, Hannah K. Wayment-Steele, Garyk Brixi, Haobo Wang, Dorothee Kern, and Sergey Ovchinnikov. 2024. “Protein Language Models Learn Evolutionary Statistics of Interacting Sequence Motifs.” Proceedings of the National Academy of Sciences 121 (45). https://doi.org/10.1073/pnas.2406285121.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Protein contact maps from attention maps</span>"
    ]
  },
  {
    "objectID": "Chapter5_Proteins.html",
    "href": "Chapter5_Proteins.html",
    "title": "11  Integrated protein diffusion language models",
    "section": "",
    "text": "11.1 Alphafold3 & RFdiffusion\nAlphafold3(Abramson et al. 2024) has what initially looks like a very complex architecture (See Figure 1), at least it did to me for the longest time.\nOne important new aspect to the architecture is the “Diffusion module” on the bottom right. It has 3 inputs, the results from three other model elements feed into it (blue paths) and a weird little point cloud. The actual architecture of AF3 is really remarkably complex, I feel in no way competent to teach it. One of the best overviews I saw is a blog post by Elena Simon & Jake Silberg. Like AF2 before it, AF3 is a model that searches and integrates evolutionary sensible (the multiple sequence alignments) and template based (structures of similar proteins) refinements of the input, internally maintains concurrent token and atom resolution representations of the molecule with their own attention mechanisms, and subsequently imposes geometric (triangle rules) constraints, this means the alphafold3 model is significantly more tailored/specific/supervised and slower, but often better than pure protein language models (at the expense of speed and efficiency).\nBecause I can’t do the full complexity of AF3 justice, but I think I can get people up to speed on how you’d begin to think about bridging protein sequence models, and 2D and 3D representation of proteins, we are going to train a protein diffusion model, that is guided by the inputs with a generic protein language model (facebook/esm2_t30_150M_UR50D) and attaches a diffusion model to it. As we are going to focus on diffusiom i’ts important to note AF3 want the dirst protein diffusion model, others like RFdiffusion(Watson et al. 2023) preceded it, and are architecturally a little more like our current toy model.\nNow since I don’t have the compute that Google DeepMind has (and neither do you…), we’ll train a 2D diffusion model that, guided by a pre-existing protein language model, generates protein contact maps. The reason to again work on predicting contact maps is that 1. the image-like illustrations contact maps are help me effectively convey the concept of diffusion and 2. abstracting away the highly effective MSA and physics-based architecture in AF3 lets me focus on something fairly profound (I think): cross-attention. To my simple statistics/numerically minded brain, the fact that through encoding and attention we can capture the correlations between sequences of tokens is already a huge leap. When I think about how to capture two entirely separate (but related) modalities, a sequence of tokenized amino acids, and the 3-D (or 2D) structure of the molecule, my brain breaks a little. Cross-attention is how ChatGPT can relate your written command (“make a picture of a dog strapped to the roof of Mitt Romney’s car, in the style of Studio Ghibli”) and produce an image of just that scene, to the incredible annoyance and in some cases despair of artists who’s work they pirated for that! Fortunately for us, in Genomics, the data is actually in the public domain! and so there are no moral compromises to what we are about to do in that respect!",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integrated protein diffusion language models</span>"
    ]
  },
  {
    "objectID": "Chapter5_Proteins.html#alphafold3-rfdiffusion",
    "href": "Chapter5_Proteins.html#alphafold3-rfdiffusion",
    "title": "11  Integrated protein diffusion language models",
    "section": "",
    "text": "Figure 1: (source: Figure 1d Alphafold paper(Abramson et al. 2024)) which abstracts the model architecture used in Alphafold3. We’ll talk through things step by step below.\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nA critical requirement for 3D protein models is that they are invariant to transformations (rotations, translations, and reflections), rotations of a molecule obviously change the coordinates of all the atoms against an arbitrary axis, but protein models should be immune to those, which is a tall order. By working with contact maps I abstract away the problems that arise from arbitrary scale transformations, as contact , or distance maps, are invariant if rotated 9the distances remain the same). I am restricting myself to the coordinates of the central C-alpha atom, further abstracting away side-chains which makes modeling considerably easier but less faithful. For a review of how different (diffusion based) deep learning model deal with equivariance see this comprehensive review of the literature: https://doi.org/10.1016/j.csbj.2024.06.021 (Soleymani et al. 2024).",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integrated protein diffusion language models</span>"
    ]
  },
  {
    "objectID": "Chapter5_Proteins.html#diffusion-models",
    "href": "Chapter5_Proteins.html#diffusion-models",
    "title": "11  Integrated protein diffusion language models",
    "section": "11.2 Diffusion models",
    "text": "11.2 Diffusion models\nGaussian Denoising Diffusion models are a very flexible, and ingenious, class of deep learning models for images. Architectures derived from diffusion models are the conceptual basis for all kinds of famous image generation models, like the one integrated into ChatGPT or other image generation AI models.\nDiffusion models have two parts, a forward process, which is defined, or fixed, and a reverse process which takes the form of a neural network and which is learned. The forward process takes training images (in our case of protein distance maps) and adds sequentially more noise (See Figure 2).\nThe relation between the image (x) at time t, and t-1 is:\n\\[\nX_{t-1} =  b_0 * X_{t} + b_1 * \\mathcal{N}(\\mu,\\sigma)\n\\]\nWhere \\(b_0\\) and \\(b_1\\) are parameterized in a clever way, such that they 1. depend on t alone and 2. the variance of the total image stays the same, or is controlled. This means that we don’t have to store/create all $X_{t-1} $ images but can reconstruct them from t and the input image \\(X_0\\). For a specific protein contact map ( \\(X_0\\) )the “noising” sequence might look a little like figure 2.\n\n\n\nFigure 2: Denoising diffusion forward process\n\n\nA diffusion model uses the, and the value of \\(t\\) to learn to “denoise” the image. It doesn’t denoise the image all at once though, it trains to denoise from t=500 to t=499, and t=150 to t=149 etc etc. During training, the images are embedded with their specific timestep \\(t\\) such that the reverse diffusion model (generally a U-Net architecture) can learn weights that are able to optimally estimate (and the subtract) the noise for. given image at time step t.\n\n11.2.1 Diffusion reverse model architecture\nEach “down block” in a diffusion U-Net starts with a normalization layer, which helps keep the model stable during training by making sure the numbers flowing through the network stay within a reasonable range. Then comes an activation function, like Swish or GELU, which adds flexibility to the model and helps it learn more complex patterns. The core part of the block is a Conv2D layer, which looks at small squares of pixels (like 3×3 patches) and learns to summarize what’s in them—kind of like learning to detect edges, textures, or other useful features. A special trick used in diffusion models is the time embedding, which tells the model what step of the denoising process it’s on. This time information is turned into numbers and added to the features in the block so the model can behave differently at each step.\nAfter the main part of the down block, there’s a downsampling layer that reduces the size of the image (usually by half) so the next layer can focus on a broader view of the picture. This is often done with a strided convolution, which skips over pixels to shrink the height and width while keeping the most important features. Skip connections pass the feature maps from each downsampling stage directly to the matching upsampling stage on the other side of the U. This helps the model keep important details that might be lost during compression, allowing the decoder to reconstruct sharper and more accurate outputs. The goal of downsampling the image is to tend to the same image at different scales, the fine detail in the early layers, the more global structure in the later layers. In the case of protein contact maps, the early layers tend to the secondary structure: local spatial conformation of the polypeptide backbone, helices, sheets, and loops.\nIn the middle of the U-Net, after the deepest downsampling layer, there’s often a self-attention block. This layer helps the model understand global relationships in the image — for example, connecting far-apart pixels that should be related (like opposite ends of a stripe or outline). Since it operates at the most compressed resolution, it’s efficient but powerful, and it benefits from the time embedding just like the ResBlocks. In the context of protein contact diffusion models, the attention learns aspects of the tertiary protein structure.\n\n\n\n\n\n\nUp blocks in a diffusion U-Net are the mirror of the down blocks—they take the compressed features and gradually rebuild the image to its original size. Each up block typically starts by upsampling the feature map, usually with a transpose convolution or nearest-neighbor interpolation followed by a Conv2D, which increases the height and width (often doubling them). After upsampling, the block combines the upsampled features with the skip connection from the matching down block, so it has both high-level context and fine details. It then passes this combined input through one or more ResBlocks, just like in the encoder, using normalization, activation, convolutions, and time embedding again to refine the reconstruction.\nAt the start, the scalar timestep t is turned into a high-dimensional vector using a sinusoidal position embedding, and then passed through a small MLP (multi-layer perceptron) to create a learned time embedding. This embedding is then injected into nearly every ResBlock in the model — both in the down blocks, up blocks, and middle blocks. Inside each ResBlock, the time embedding is added (broadcasted) to the feature map after a linear layer transforms it to match the number of channels. This allows every part of the network to be conditioned on how much noise it should expect and how aggressively to denoise.\n\n\n11.2.2 Diffusion model for contact maps\nAs a training set, I build 7000 contact maps, based on experimentally validated proteins in the CASP12 set I obtained from the SideChainNet dataset (King and Koes 2021). I do not build strictly binary contact maps, but build images that reflect 9 distances thresholds (see Figure 3 for an example entry from the training set).\nHere are 2 functions I used to make contact/distance maps and then crop those to 128x 128 pixels.\n\n# === FUNCTIONS ===\n\ndef make_contact_map(coords, binary=False, threshold=8.0):\n    \"\"\"\n    Computes a contact or distance map from 3D Cα coordinates.\n\n    Args:\n        coords (np.ndarray): Shape (L, 3) or (L, A, 3), 3D coordinates.\n        binary (bool): Whether to return binary contact map.\n        threshold (float): Å distance cutoff for binary maps.\n        clip_dist (float): Max distance for clipping and scaling.\n\n    Returns:\n        np.ndarray: (L, L) contact or scaled distance map in uint8.\n    \"\"\"\n    # extract only Cα coordinates (atom 0 for each residue)\n    ca_coords = coords[:, 0, :]  # shape (L, 3)\n\n    n_residues = ca_coords.shape[0]\n    n_missing = np.isnan(ca_coords).any(axis=1).sum()\n    frac_missing = n_missing / n_residues\n\n    if frac_missing &gt; 0.10:\n        print(f\"{protein_id} skipped: {n_missing}/{n_residues} Cα coords missing ({frac_missing:.1%})\")\n        return None\n\n    if np.unique(ca_coords, axis=0).shape[0] &lt;= 1:\n        print(f\"{protein_id} skipped: collapsed structure (identical Cα)\")\n        return None\n\n    if n_residues &lt; 10:\n        print(f\"{protein_id} skipped: too short ({n_residues} residues)\")\n        return None\n\n\n    # normalize based on first Cα\n    ca_coords -= ca_coords[0]  # shift so residue 0 is at (0, 0, 0)\n\n    # compute pairwise distance matrix\n    dists = np.linalg.norm(ca_coords[:, None, :] - ca_coords[None, :, :], axis=-1)\n\n    if binary:\n        contact_map = (dists &lt; threshold).astype(np.uint8) * 255\n        return contact_map\n    else:\n        levels = [0,20,40,60,80,100,120,140,160,180]\n        contact_map = np.zeros_like(dists, dtype=np.uint8) + 255\n\n        # Bin 0: d &lt; threshold - 3\n        mask = dists &lt; (threshold - 3)\n        contact_map[mask] = levels[0]\n\n        # Bins 1 to 9: 1Å slices\n        for i in range(1, 9):\n            lower = threshold - 5 + (i) \n            upper = lower + 1\n            mask = (dists &gt;= lower) & (dists &lt; upper)\n            contact_map[mask] = levels[i]\n        return contact_map\n\n\n\n# Or simply crop the first 128 amino-acids (Ca atoms associated with those amino-acids)\ndef crop_contact_map(contact_map, crop_size=128, pad_value=255, min_size=20):\n    \"\"\"\n    Crop the top-left corner of the contact map to (crop_size, crop_size).\n    Pads with `pad_value` if the map is smaller.\n    Skips maps smaller than `min_size`.\n    \"\"\"\n    h, w = contact_map.shape\n\n    if h &lt; min_size or w &lt; min_size:\n        raise ValueError(f\"Contact map too small: {h}x{w} (min required: {min_size})\")\n\n    canvas = np.full((crop_size, crop_size), pad_value, dtype=np.uint8)\n    crop_h = min(h, crop_size)\n    crop_w = min(w, crop_size)\n    canvas[:crop_h, :crop_w] = contact_map[:crop_h, :crop_w]\n\n    return Image.fromarray(canvas)\nThen using the sidechainnet package, I download protein information, save the contact maps as images using the following function:\n# === MAIN ===\n\nprint(\"Loading SideChainNet CASP12 dataset...\")\ndata = scn.load(casp_version=12)\n\nprint(\"Generating contact maps...\")\nfor i, sample in enumerate(data):\n    try:\n        protein_id = sample.id\n        coords = sample.coords  # shape: (L, A, 3)\n\n        if coords is None or coords.shape[0] &lt; 2:\n            continue\n\n        # Compute distance matrix and preprocess\n        distance_map = make_contact_map(coords, binary=False)\n\n        # Skip if it's completely empty\n        if np.all(distance_map == 255):\n            print(f\"Skipping {protein_id}: distance map is all white\")\n            continue\n            \n        # Make the final image\n        img = crop_contact_map(distance_map, crop_size=128,pad_value=255)\n        img.save(os.path.join(output_dir, f\"{protein_id}.jpg\"), format='JPEG')\n\n        if i % 100 == 0:\n            print(f\"[{i}] Saved: {protein_id}.jpg\")\n\n    except Exception as e:\n        print(f\"Skipping {sample.id} due to error: {e}\")\n\nprint(\" Done.\")\nThis generates a folder with thousands of images of contact maps, where the specific shade of gray of the pixel corresponds to &lt;4 (black), 5, 6, 7, 8, 9, 10, 11, 12, or &gt;13 (white) Angstrom distances.\n\n\n\nFigure 3: A protein distance map based on experimental CASP12 data.\n\n\n\n11.2.2.1 Detailed Parameter Breakdown by Module for our model\nThe table below breaks down the different elements of the U-Net architecture of a tiny diffusion model I trained. The table describes how many free parameters each part of the model contains. As you can see, the decoder ups part of the model has almost half of the parameters. This is where the encoded information, and the skip connections are brought together. You’ll also note the most number of parameters in the time_mlp, which encodes the influence of the specific timestep t. It’s important to realize that the ResBlocks (both in the encoder and decoder) also contain parameters that map the time-specific embedding to the image information. These regulate how the time information influences image generation at each specific stage.\n\n\n\n\n\n\n\n\nModule\nParameters\nDescription\n\n\n\n\nups\n19,832,768\nUpsampling path (decoder): progressively reconstructs the denoised image from compressed features. Includes ResBlocks, upsampling (e.g. transposed convolutions), and skip connections from encoder layers.\n\n\ndowns\n5,402,816\nDownsampling path (encoder): extracts hierarchical features from the noisy input image using stacked ResBlocks and downsampling layers.\n\n\nmid_block1\n4,983,808\nFirst bottleneck ResBlock: processes the most compressed latent representation of the input, directly before/after the attention block.\n\n\nmid_block2\n4,983,808\nSecond bottleneck ResBlock: further transforms latent features after attention at the bottleneck. Acts as a transition before decoding.\n\n\nmid_attn\n264,192\nSelf-attention at bottleneck: captures global spatial dependencies in the most compressed feature map, enabling long-range interactions.\n\n\nfinal_res_block\n152,000\nFinal ResBlock before output: fuses decoder output and prepares it for the final convolution. Often used to refine the final image prediction.\n\n\ntime_mlp\n82,432\nTimestep embedding network: converts scalar timestep into a vector that conditions all ResBlocks, allowing the model to denoise appropriately for each diffusion step.\n\n\ninit_conv\n3,200\nInitial input convolution: expands the input image from 1 channel to base feature dimension (dim=64), preparing it for downstream processing.\n\n\nfinal_conv\n65\nFinal output convolution: projects the final hidden features back to 1 channel to match the original image shape. Predicts either noise (ε_t) or clean image (x₀).\n\n\n\nOnce we’ve trained the diffusion model for about 40/60 minutes (though ideally longer and on way more data!) we can sample from it. Sampling from it involves sampling pure Gaussian noise (“t - 999”) and passing the noise through the diffusion model a few hundred times (each time updating t). There are speedups and tricks that mean you won’t have to pass it through the model 1000 times to get a solid result, though more passes (“steps”) generally means a better result. Because protein contact maps are symmetric, but the model doesn’t know that, I generate a protein but then copy over one of the two halves. In Figure 3 I show a particularly convincing-looking sample at 5 steps from noise to final protein contact map.\n\n\n\nFigure 3: interim and final stages of a protein contact map sampled from pure noise, after training for a few epochs on 7000 real proteins.\n\n\nThe protein contact map sampled in Figure 3 is a fiction that, according to the model, fits the distribution of the training data, which while fascinating isn’t very useful yet. To make things useful, we must condition the model on not just timestep, but also on condensed, or embedded sequence information.\nThe next step is to condition the generation of a diffusion model on the amino-acid sequence. This is a very specific multi-modal problem, we want to create an image, based on a sequence of tokens. In order to wrap our head around.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integrated protein diffusion language models</span>"
    ]
  },
  {
    "objectID": "Chapter5_Proteins.html#from-attention-to-cross-attention",
    "href": "Chapter5_Proteins.html#from-attention-to-cross-attention",
    "title": "11  Integrated protein diffusion language models",
    "section": "11.3 From attention, to cross-attention",
    "text": "11.3 From attention, to cross-attention\nIn Chapter 10 we discussed the attention mechanism, where each token in a sequence is embedded in \\(X\\), looks at all other tokens in the same sequence—we can explore something more general: cross-attention. Cross-attention (See Figure below) is what allows models to link different modalities together. It is how an image can learn to attend to words, or how a protein contact map can align with an amino acid sequence.\nIn technical terms, self-attention happens when the Query, Key, and Value matrices—\\(Q\\), \\(K\\), and \\(V\\)—are all derived from the same input. Cross-attention, on the other hand, involves two different sources. One stream generates the queries, \\(Q_{nxd}\\), where n is the number of tokens in the query source (e.g., the contact map image), and \\(d_q\\) is the learned query dimensionality. The other stream produces the keys \\(K_{mxd}\\) and values \\(K_{mxd}\\), where \\(m\\) is the number of tokens in the key/value source (e.g., the sequence). In this setup, the queries are essentially asking, “Where in the other modality is the most relevant information for me?”\nHere’s where it gets interesting: even though the inputs—say, a 2D contact map versus a 1D amino acid sequence—might have vastly different shapes and lengths (n ≠ m), attention still works because we project both inputs into a shared space using learned linear transformations. The only requirement is that the inner dimensions match appropriately: the dot product QKᵗ yields an attention score matrix \\(A_{nxm}\\), which tells each query vector how much to attend to each key. After applying softmax to these scores, the attention weights are used to compute a weighted sum of the values \\(V\\), resulting in the final output \\(Z_{n*d}\\).\nSo, while the original modalities might differ wildly in format and token count, cross-attention does not require the inputs to be the same size—only that their projections align where it matters: in the shared attention dimensions \\(d_q\\) and \\(d_v\\).\nIn our case, when we say the contact map diffusion model attends to the sequence, we mean the image features are used to generate queries, and the sequence provides the keys and values, the final output dimensions are consistent with those of the diffusion model. This lets the contact map image learn where in the sequence there are features that are most relevant to its shape. That might sound abstract, but think of it as one set of data (the image) trying to find the most useful context from another (the sequence).\nThis mechanism is central in multimodal transformers like text to image, speech to text, or many others. It is increasingly being used in biological modeling too. It enables not just learning within a sequence, but learning between representations—and that’s a big leap.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integrated protein diffusion language models</span>"
    ]
  },
  {
    "objectID": "Chapter5_Proteins.html#cross-attention-and-a-diffusion-model.",
    "href": "Chapter5_Proteins.html#cross-attention-and-a-diffusion-model.",
    "title": "11  Integrated protein diffusion language models",
    "section": "11.4 Cross attention and a diffusion model.",
    "text": "11.4 Cross attention and a diffusion model.\nGoogle’s ‘imagen’ architecture implements a diffusion model, which uses cross attention to attend to the final embeddings of a text model. The great news to us is that the ‘imagen’ paper actually showed you don’t really need to train the “text” part of the model with this specific goal in mind, you can just use an existing model! And that is what we are going to do. We are going to use facebook/esm2_t30_150M_UR50D which is an evolution of the Facebook-funded ESM-2 models, to compute embeddings for all CASP 12 proteins, and train a diffusion model conditioned on the image embedding. Recall how U-net diffusion models are already conditioned on the time t1 embedding, we are simply going to also condition specific steps in the U-net on the sequence model embedding. As we know from Chapter 10 sequence models do learn latent representations of contact maps, we are going to leverage that information when running the diffusion process.\nI use the script below to download the amino-acid sequences for proteins in the sidechainnet dataset, run the facebook/esm2_t30_150M_UR50D model on these proteins, and save the per amino-acid embeddings generated by the last layer. These embeddings, unlike the input embeddings, are the contextualized (their passed through attention blocks across many layers) embedding of each amino-acid in its context. The output is a 128 by 640 (embedding dimension for this model) matrix that is a numerical representation of the protein according to the facebook/esm2_t30_150M_UR50D model. This is a quite resource-intensive process, so I added code to make sure it only computes embeddings for proteins for which we are able to generate a solid contact map (one without too many missing data points for example).\n === embedding_extraction.py ===\n\nimport os\nimport torch\nimport numpy as np\nimport sidechainnet as scn\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModel\n\n# === PARAMETERS ===\nmodel_name = \"facebook/esm2_t30_150M_UR50D\"\noutput_dir = \"embeddings_all\"\nvalid_ids_path = \"valid_proteins.txt\"\nos.makedirs(output_dir, exist_ok=True)\n\n# === Load Model ===\ndevice = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name).to(device).eval()\n# === embedding_extraction.py ===\n\nos.makedirs(output_dir, exist_ok=True)\n\n# === Load valid protein IDs from contact map step ===\nwith open(valid_ids_path, \"r\") as f:\n    valid_ids = set(line.strip() for line in f if line.strip())\n\n\ndef get_embeddings(sequence):\n    with torch.no_grad():\n        inputs = tokenizer(sequence, return_tensors=\"pt\", truncation=True, padding=True)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        outputs = model(**inputs)\n        seq_emb = outputs.last_hidden_state[0].cpu().numpy()  # (L, D)\n        seq_len, emb_dim = seq_emb.shape\n\n        # Pad to 128 if needed\n        if seq_len &lt; 128:\n            pad = np.zeros((128 - seq_len, emb_dim), dtype=np.float32)\n            seq_emb = np.vstack([seq_emb, pad])\n        else:\n            seq_emb = seq_emb[:128]\n\n        return seq_emb  # shape: (128, D)\n\ndef main():\n    casp_versions = [12]\n    print(f\" Loading SideChainNet CASP datasets: {casp_versions}...\")\n\n    # Combine all samples\n    all_samples = []\n    for casp_version in casp_versions:\n        data = scn.load(casp_version=casp_version, casp_thinning=70)\n        all_samples.extend(data)\n\n    print(\" Extracting embeddings only for valid proteins...\")\n    for sample in tqdm(all_samples):\n        protein_id = sample.id\n        if protein_id not in valid_ids:\n            continue\n\n        try:\n            sequence = sample.sequence\n            if sequence is None or len(sequence) &lt; 10:\n                continue\n            cropped_sequence = sequence[:128]\n            emb = get_embeddings(cropped_sequence)\n            np.save(os.path.join(output_dir, f\"{protein_id}.npy\"), emb)\n        except Exception as e:\n            print(f\" Skipping {protein_id} due to error: {e}\")\n\n    print(\" Embedding extraction complete.\")\n\nif __name__ == \"__main__\":\n    main()",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integrated protein diffusion language models</span>"
    ]
  },
  {
    "objectID": "Chapter5_Proteins.html#training-the-imagen-model",
    "href": "Chapter5_Proteins.html#training-the-imagen-model",
    "title": "11  Integrated protein diffusion language models",
    "section": "11.5 Training the imagen model",
    "text": "11.5 Training the imagen model\nThe high-level library huggingface provides for image models, called diffusion, isn’t as fully developed as their transformers library. Therefore, we’re going to use a library maintained by GitHub user lucidrains who I guess is a scientist in biomedical science but also someone who builds these amazing implementations of AI models from papers. The repo we’ll use is imagen-pythorch which you can just install using pip: pip install imagen-pytorch.\nWhat we’ll effectively end up doing to predict protein contact maps is run a two-model sequence: 1. run a protein language model to transform sequences into embeddings, encoding a lot of information using a pre-trained 150m parameter language model, and 2. use those embeddings, and the contact maps of the same protein to train a ~31 million parameter Unet image generation model, where every so often there is a cross-attention layer where the image attends to the sequence embedding. You can find the full script here, but below we can have a look at the core components, a U-net diffusion model, and an imagen model which contains it.\n\n# === Define U-Nets for Imagen Cascade ===\nunet1 = Unet(\n    dim=64,\n    cond_dim=embedding_dim,\n    dim_mults=(1, 2, 4),\n    channels=1,\n    num_resnet_blocks=2,\n    layer_attns=(False, False, True),\n    layer_cross_attns=(False, True, True),\n)\n\n\n# === Imagen ===\nimagen = Imagen(    \n    unets=(unet1),\n    image_sizes=(128),\n    timesteps=1000,\n    cond_drop_prob=0.1,\n    text_embed_dim=640,\n    channels=1\n).to(device)\nWe’ll have to do a little model work, because we can use Hugging Face’s Trainer library here. Though the repo we use here is still fairly high-level, so it provides a trainer, etc. the data loading process is abstracted away here, but of course, all scripts are available in full on GitHub.\n# === ImagenTrainer ===\n# Initialize the trainer\ntrainer = ImagenTrainer(imagen,fp16=True).to(device)\n\n\n# FIX: Wrap with cond_images_dataset so (cond, image) is handled correctly\ntrainer.add_train_dataset(train_subset, batch_size=batch_size, shuffle=True, num_workers=0)\ntrainer.add_valid_dataset(eval_subset, batch_size=batch_size, shuffle=True, num_workers=0)\n\n# set up eval...\neval_losses = []  # store losses for plotting later\neval_every = 100  # steps\n\n\nsteps_per_epoch = len(train_subset) // batch_size\n\n\n# === Training Loop ===\nprint(\" Starting training...\")\nfor epoch in range(epochs):\n    total_loss = 0.0\n    for step in tqdm(range(steps_per_epoch)):\n        loss1 = trainer.train_step(unet_number=1)\n        total_loss += loss1\n\n        if step % 50 == 0:\n            print(f\"Step {step} | Loss: {loss1:.4f}\")\n\n        if step % eval_every == 0:\n            eval_loss = trainer.valid_step(unet_number=1)\n            eval_losses.append((epoch, step, eval_loss))\n            print(f\" Eval Loss @ Epoch {epoch}, Step {step}: {eval_loss:.4f}\")\n\n    print(f\"[Epoch {epoch + 1}/{epochs}]  Loss: {total_loss / steps_per_epoch:.4f}\")\n    if (epoch+1) % 5 == 0:\n        trainer.save(f\"imagen_cont/imagen_protein_epoch{epoch + 1}.pt\")\n\nprint(\"Training complete.\")\nA training loop can be fairly simple, especially with great out-of-the-box pre-built models like the one provided by lucidrains. It’s essentially two Python loops, the outer loop loops over epochs, the inner loop over steps. In each inner loop, we’ll take a single training step, computing the derivatives of all parameters wrt the loss over a batch of data, and updating all parameters:\n    for step in tqdm(range(steps_per_epoch)):\n        loss1 = trainer.train_step(unet_number=1)\nThis particular training loop still abstracts a fair bit away from the user, in later chapters we’ll likely have to open up that training loop even further. We train the imagen model on 16,000 contact maps and embedded sequences, for 80 epochs (image models need way more epochs than sequence models). After training, the model is able to fairly accurately reconstruct protein contact maps, see Figure 5.\n\n\n\nFigure 5: true vs generated protein contact maps generated by diffusion conditional on protein language model embeddings.\n\n\nTo understand the pros and cons of this model relative to the attention-based protein contact map prediction we performed in Chapter 10, we can do some visual inspection. While these generations on the right are sharp, crisp, and often very accurate, Generation 10889 has some clear “hallucinations”, or contacts that seem very confident and real, but aren’t in the ground truth at all! While if you go back to Figure 2 of Chapter 10 you’d see far more “noisy” error in predictions. Hallucinations are a very pernicious kind of noise, they look clean, convincing, plausible, because the diffusion model just turned some early noise into a very slick processed convincing-looking result, which noise in statistical predictions in the last chapter are far more recognizable as noise!\nAlphafold3, and AlphaFold2 before it, had a specific feature to guard against convincing-looking hallucinations. Both models had confidence modules, a neural network that predicts the model’s confidence in each predicted structure/residue. But it also used double conditional diffusion, where the diffusion was conditioned on both the sequence embeddings, and on the embedding of the proteins pair representations (similar to the attention maps we discussed in Chapter 10, a 2D representation of which amino-acids attend to each other).",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integrated protein diffusion language models</span>"
    ]
  },
  {
    "objectID": "Chapter5_Proteins.html#conclussion",
    "href": "Chapter5_Proteins.html#conclussion",
    "title": "11  Integrated protein diffusion language models",
    "section": "11.6 Conclussion",
    "text": "11.6 Conclussion\nSo in this chapter we trained, and discussed, a model that is abstraction fo AF3, it is like AF3 int he sense that we use diffusion, conditional on sequence information to estimate a representation of a protein. Obviosluyit is missing all kind of keye part of the AF3 model. in the next chapter we’ll slightly expand our abstraction of AF3. We’ll train diffusion model hat is conditioned on the amino-acid sequence embedding AND on the protein map as predicted based on the 2D presentation of the attention maps.\nTo prepare, you can try the two self-study assignments below, they pull in things we learned in the last two chapters to basically make protein predictions that adhere to the noisy, but mostly accurate outlines we could generate based on attention maps, and then use diffusion modeling to predict the local details, preventing strong hallucinations. If you’re more of a reader, skip to the next chapter, where I’ll take a stab at these two assignments myself.\n\n\n\n\n\n\nTip\n\n\n\nIn order from beginner to advanced project, consider these self-study exercises, all of which share a common goal, to see if we can further condition the diffusion model on the noisy but hallucination-free attention-based predictions we made in the last chapter, the hallucinations reduce.\n1. (easy) Run contact map prediction like in Chapter 10. That is, extract the attention maps when running facebook/esm2_t30_150M_UR50D for ~20 genes, predict their contact maps with those, store and average the prediction weights and use those to predict contact maps for further proteins in the imagen validation set. Then when sampling from your imagen model, give the attention-based prediction the argument init_image, along with the embeddings. This will start the denoising from the attention-based prediction, you can experiment with skip_steps = xxx because as the model now starts from a prediction, not pure noise, you might want to a portion of the 1000 steps for optimal results\n2. (medium) Run contact map prediction like in Chapter 10. That is, extract the attention maps when running facebook/esm2_t30_150M_UR50D for ~20 genes, predict their contact maps with those, store and average the prediction weights and use those to predict contact maps for further proteins in the imagen training set. Then when training your imagen model, give the attention-based prediction the argument cond_images, along with the text embeddings. This will train an imagen model that is conditioned on the faint outlines of the protein structure generated based on the attention maps, which are less likely to be hallucinations.\n\n\n\n\n\n\nAbramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, et al. 2024. “Accurate Structure Prediction of Biomolecular Interactions with AlphaFold 3.” Nature 630 (8016): 493–500. https://doi.org/10.1038/s41586-024-07487-w.\n\n\nKing, Jonathan Edward, and David Ryan Koes. 2021. “SidechainNet: An All-Atom Protein Structure Dataset for Machine Learning.” Proteins: Structure, Function, and Bioinformatics 89 (11): 1489–96. https://doi.org/10.1002/prot.26169.\n\n\nSoleymani, Farzan, Eric Paquet, Herna Lydia Viktor, and Wojtek Michalowski. 2024. “Structure-Based Protein and Small Molecule Generation Using EGNN and Diffusion Models: A Comprehensive Review.” Computational and Structural Biotechnology Journal 23 (December): 2779–97. https://doi.org/10.1016/j.csbj.2024.06.021.\n\n\nWatson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023. “De Novo Design of Protein Structure and Function with RFdiffusion.” Nature 620 (7976): 1089–1100. https://doi.org/10.1038/s41586-023-06415-8.",
    "crumbs": [
      "Protein models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integrated protein diffusion language models</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "13  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green,\nAlexander Pritzel, Olaf Ronneberger, et al. 2024. “Accurate\nStructure Prediction of Biomolecular Interactions with AlphaFold\n3.” Nature 630 (8016): 493–500. https://doi.org/10.1038/s41586-024-07487-w.\n\n\n“AlphaFold3  Why Did Nature Publish It Without Its\nCode?” 2024. Nature 629 (8013): 728–28. https://doi.org/10.1038/d41586-024-01463-0.\n\n\nAlQuraishi, Mohammed. 2019. “ProteinNet: A Standardized Data Set\nfor Machine Learning of Protein Structure.” BMC\nBioinformatics 20 (1). https://doi.org/10.1186/s12859-019-2932-0.\n\n\nBenegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S.\nSong. 2023. “GPN-MSA: An Alignment-Based DNA Language Model for\nGenome-Wide Variant Effect Prediction.” http://dx.doi.org/10.1101/2023.10.10.561776.\n\n\nBenegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. “DNA\nLanguage Models Are Powerful Predictors of Genome-Wide Variant\nEffects.” Proceedings of the National Academy of\nSciences 120 (44). https://doi.org/10.1073/pnas.2311219120.\n\n\nBenegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun\nS. Song. 2025. “Genomic Language Models: Opportunities and\nChallenges.” Trends in Genetics, January. https://doi.org/10.1016/j.tig.2024.11.013.\n\n\nBiderman, Dan, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul,\nPhilip Greengard, Connor Jennings, Daniel King, et al. 2024. “LoRA\nLearns Less and Forgets Less.” https://doi.org/10.48550/ARXIV.2405.09673.\n\n\nCheng, Xingyi, Bo Chen, Pan Li, Jing Gong, Jie Tang, and Le Song. 2024.\n“Training Compute-Optimal Protein Language Models.” http://dx.doi.org/10.1101/2024.06.06.597716.\n\n\nConsens, Micaela Elisa, Ben Li, Anna R. Poetsch, and Stephen Gilbert.\n2025. “Genomic Language Models Could Transform Medicine but Not\nYet.” Npj Digital Medicine 8 (1). https://doi.org/10.1038/s41746-025-01603-4.\n\n\nDalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez\nCarranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago,\net al. 2024. “Nucleotide Transformer: Building and Evaluating\nRobust Foundation Models for Human Genomics.” Nature\nMethods 22 (2): 287–97. https://doi.org/10.1038/s41592-024-02523-z.\n\n\nDuarte, Jose M, Rajagopal Sathyapriya, Henning Stehr, Ioannis Filippis,\nand Michael Lappe. 2010. “Optimal Contact Definition for\nReconstruction of Contact Maps.” BMC Bioinformatics 11\n(1). https://doi.org/10.1186/1471-2105-11-283.\n\n\nElnaggar, Ahmed, Michael Heinzinger, Christian Dallago, Ghalia Rehawi,\nYu Wang, Llion Jones, Tom Gibbs, et al. 2022. “ProtTrans: Toward\nUnderstanding the Language of Life Through Self-Supervised\nLearning.” IEEE Transactions on Pattern Analysis and Machine\nIntelligence 44 (10): 7112–27. https://doi.org/10.1109/tpami.2021.3095381.\n\n\nESM Team. 2024. “ESM Cambrian: Revealing the Mysteries of Proteins\nwith Unsupervised Learning.” EvolutionaryScale Website. https://evolutionaryscale.ai/blog/esm-cambrian.\n\n\nGao, Zhangyang, Cheng Tan, and Stan Z. Li. 2024. “FoldToken4:\nConsistent & Hierarchical Fold Language.” http://dx.doi.org/10.1101/2024.08.04.606514.\n\n\nGaujac, Benoit, Jérémie Donà, Liviu Copoiu, Timothy Atkinson, Thomas\nPierrot, and Thomas D. Barrett. 2024. “Learning the Language of\nProtein Structure.” https://doi.org/10.48550/ARXIV.2405.15840.\n\n\nGeiping, Jonas, and Tom Goldstein. 2022. “Cramming: Training a\nLanguage Model on a Single GPU in One Day.” https://doi.org/10.48550/ARXIV.2212.14034.\n\n\nGriewank, Andreas. 2012. “Who Invented the Reverse Mode of\nDifferentiation?” In, 389–400. EMS Press. https://doi.org/10.4171/dms/6/38.\n\n\nHassan, Hassan, Kyle Puhger, Ali Saadat, Johannes Mayer, and Maximilian\nSprang. 2025. “Life as a Function: Why Transformer Architectures\nStruggle to Gain Genome-Level Foundational Capabilities.” http://dx.doi.org/10.1101/2025.01.13.632745.\n\n\nHayes, Thomas, Roshan Rao, Halil Akin, Nicholas J. Sofroniew, Deniz\nOktay, Zeming Lin, Robert Verkuil, et al. 2025. “Simulating 500\nMillion Years of Evolution with a Language Model.”\nScience 387 (6736): 850–58. https://doi.org/10.1126/science.ads0018.\n\n\nHu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, Lu Wang, and Weizhu Chen. 2021. “LoRA: Low-Rank\nAdaptation of Large Language Models.” https://doi.org/10.48550/ARXIV.2106.09685.\n\n\n“Initial Sequencing and Comparative Analysis of the Mouse\nGenome.” 2002. Nature 420 (6915): 520–62. https://doi.org/10.1038/nature01262.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael\nFigurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021.\n“Highly Accurate Protein Structure Prediction with\nAlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nKempen, Michel van, Stephanie S. Kim, Charlotte Tumescheit, Milot\nMirdita, Jeongjae Lee, Cameron L. M. Gilchrist, Johannes Söding, and\nMartin Steinegger. 2023. “Fast and Accurate Protein Structure\nSearch with Foldseek.” Nature Biotechnology 42 (2):\n243–46. https://doi.org/10.1038/s41587-023-01773-0.\n\n\nKing, Jonathan Edward, and David Ryan Koes. 2021. “SidechainNet:\nAn All-Atom Protein Structure Dataset for Machine\nLearning.” Proteins: Structure, Function, and\nBioinformatics 89 (11): 1489–96. https://doi.org/10.1002/prot.26169.\n\n\nKingma, Diederik P., and Jimmy Ba. 2014. “Adam: A Method for\nStochastic Optimization.” https://doi.org/10.48550/ARXIV.1412.6980.\n\n\nLin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting\nLu, Nikita Smetanin, et al. 2023. “Evolutionary-Scale Prediction\nof Atomic-Level Protein Structure with a Language Model.”\nScience 379 (6637): 1123–30. https://doi.org/10.1126/science.ade2574.\n\n\nLjungdahl, Alicia, Sayeh Kohani, Nicholas F. Page, Eloise S. Wells,\nEmilie M. Wigdor, Shan Dong, and Stephan J. Sanders. 2023.\n“AlphaMissense Is Better Correlated with Functional Assays of\nMissense Impact Than Earlier Prediction Algorithms.” http://dx.doi.org/10.1101/2023.10.24.562294.\n\n\nLupo, Umberto, Damiano Sgarbossa, and Anne-Florence Bitbol. 2022.\n“Protein Language Models Trained on Multiple Sequence Alignments\nLearn Phylogenetic Relationships.” Nature Communications\n13 (1). https://doi.org/10.1038/s41467-022-34032-y.\n\n\nMarin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen,\nDennis Pultz, Ole Winther, and Wouter Boomsma. 2023. “BEND:\nBenchmarking DNA Language Models on Biologically Meaningful\nTasks.” https://doi.org/10.48550/ARXIV.2311.12570.\n\n\nMeier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and\nAlexander Rives. 2021. “Language Models Enable Zero-Shot\nPrediction of the Effects of Mutations on Protein Function.” http://dx.doi.org/10.1101/2021.07.09.450648.\n\n\nNguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva\nKatrekar, David B. Li, Liam J. Bartie, et al. 2024. “Sequence\nModeling and Design from Molecular to Genome Scale with Evo.”\nScience 386 (6723). https://doi.org/10.1126/science.ado9336.\n\n\nNguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum\nBirch-Sykes, Michael Wornow, Aman Patel, et al. 2023a. “HyenaDNA:\nLong-Range Genomic Sequence Modeling at Single Nucleotide\nResolution.” https://doi.org/10.48550/ARXIV.2306.15794.\n\n\n———, et al. 2023b. “HyenaDNA: Long-Range Genomic Sequence Modeling\nat Single Nucleotide Resolution.” https://doi.org/10.48550/ARXIV.2306.15794.\n\n\nPatel, Aman, Arpita Singhal, Austin Wang, Anusri Pampari, Maya Kasowski,\nand Anshul Kundaje. 2024. “DART-Eval: A Comprehensive DNA Language\nModel Evaluation Benchmark on Regulatory DNA.” https://doi.org/10.48550/ARXIV.2412.05430.\n\n\nPoli, Michael, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao,\nStephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. 2023.\n“Hyena Hierarchy: Towards Larger Convolutional Language\nModels.” https://doi.org/10.48550/ARXIV.2302.10866.\n\n\nRao, Roshan, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander\nRives. 2020. “Transformer Protein Language Models Are Unsupervised\nStructure Learners.” http://dx.doi.org/10.1101/2020.12.15.422761.\n\n\nRentzsch, Philipp, Max Schubach, Jay Shendure, and Martin Kircher. 2021.\n“CADD-Spliceimproving Genome-Wide Variant Effect\nPrediction Using Deep Learning-Derived Splice Scores.” Genome\nMedicine 13 (1). https://doi.org/10.1186/s13073-021-00835-9.\n\n\nRives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin,\nJason Liu, Demi Guo, et al. 2021. “Biological Structure and\nFunction Emerge from Scaling Unsupervised Learning to 250 Million\nProtein Sequences.” Proceedings of the National Academy of\nSciences 118 (15). https://doi.org/10.1073/pnas.2016239118.\n\n\nSaharia, Chitwan, William Chan, Saurabh Saxena, Lala Li, Jay Whang,\nEmily Denton, Seyed Kamyar Seyed Ghasemipour, et al. 2022.\n“Photorealistic Text-to-Image Diffusion Models with Deep Language\nUnderstanding.” https://doi.org/10.48550/ARXIV.2205.11487.\n\n\nSchubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and\nMartin Kircher. 2024. “CADD V1.7: Using Protein Language Models,\nRegulatory CNNs and Other Nucleotide-Level Scores to Improve Genome-Wide\nVariant Predictions.” Nucleic Acids Research 52 (D1):\nD1143–54. https://doi.org/10.1093/nar/gkad989.\n\n\nSenior, Andrew W., Richard Evans, John Jumper, James Kirkpatrick,\nLaurent Sifre, Tim Green, Chongli Qin, et al. 2020. “Improved\nProtein Structure Prediction Using Potentials from Deep\nLearning.” Nature 577 (7792): 706–10. https://doi.org/10.1038/s41586-019-1923-7.\n\n\nSoleymani, Farzan, Eric Paquet, Herna Lydia Viktor, and Wojtek\nMichalowski. 2024. “Structure-Based Protein and Small Molecule\nGeneration Using EGNN and Diffusion Models: A Comprehensive\nReview.” Computational and Structural Biotechnology\nJournal 23 (December): 2779–97. https://doi.org/10.1016/j.csbj.2024.06.021.\n\n\nSullivan, Patrick F., Jennifer R. S. Meadows, Steven Gazal, BaDoi N.\nPhan, Xue Li, Diane P. Genereux, Michael X. Dong, et al. 2023.\n“Leveraging Base-Pair Mammalian Constraint to Understand Genetic\nVariation and Human Disease.” Science 380 (6643). https://doi.org/10.1126/science.abn2937.\n\n\nTang, Ziqi, Nirali Somia, Yiyang Yu, and Peter K Koo. 2024b.\n“Evaluating the Representational Power of Pre-Trained DNA Language\nModels for Regulatory Genomics.” http://dx.doi.org/10.1101/2024.02.29.582810.\n\n\n———. 2024a. “Evaluating the Representational Power of Pre-Trained\nDNA Language Models for Regulatory Genomics.” http://dx.doi.org/10.1101/2024.02.29.582810.\n\n\nVig, Jesse, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher,\nand Nazneen Fatema Rajani. 2020. “BERTology Meets Biology:\nInterpreting Attention in Protein Language Models.” https://doi.org/10.48550/ARXIV.2006.15222.\n\n\nWatson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L.\nTrippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023.\n“De Novo Design of Protein Structure and Function with\nRFdiffusion.” Nature 620 (7976): 1089–1100. https://doi.org/10.1038/s41586-023-06415-8.\n\n\nWengert, R. E. 1964. “A Simple Automatic Derivative Evaluation\nProgram.” Communications of the ACM 7 (8): 463–64. https://doi.org/10.1145/355586.364791.\n\n\nZhang, Zhidian, Hannah K. Wayment-Steele, Garyk Brixi, Haobo Wang,\nDorothee Kern, and Sergey Ovchinnikov. 2024. “Protein Language\nModels Learn Evolutionary Statistics of Interacting Sequence\nMotifs.” Proceedings of the National Academy of Sciences\n121 (45). https://doi.org/10.1073/pnas.2406285121.\n\n\nZhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and\nHan Liu. 2023. “DNABERT-2: Efficient Foundation Model and\nBenchmark for Multi-Species Genome.” https://doi.org/10.48550/ARXIV.2306.15006.",
    "crumbs": [
      "References"
    ]
  }
]